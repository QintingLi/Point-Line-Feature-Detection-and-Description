# -*- coding: utf-8 -*-
"""Point/Line Feature Detection and Description.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RhuLilwxv26fJko8pIYaZ70ksl3anJGy
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd

# 数据集路径
v1_1_path = "/content/drive/MyDrive/v1.1"

# 检查目录内容
def explore_directory(directory_path):
    dataset_structure = []
    for root, dirs, files in os.walk(directory_path):
        relative_path = os.path.relpath(root, directory_path)
        if relative_path == ".":
            relative_path = "/"

        # 统计文件类型
        file_types = {}
        for file in files:
            ext = os.path.splitext(file)[1]
            file_types[ext] = file_types.get(ext, 0) + 1

        dataset_structure.append({
            "Path": relative_path,
            "Files Count": len(files),
            "File Types": ", ".join(f"{ext}({count})" for ext, count in file_types.items()),
            "Directories": len(dirs)
        })
    return dataset_structure

# 获取数据集结构
try:
    dataset_structure = explore_directory(v1_1_path)

    # 转换为DataFrame并展示
    df_structure = pd.DataFrame(dataset_structure)
    print("\nv1.1 Dataset Structure:")
    print(df_structure)

    # 显示基本统计信息
    total_files = df_structure["Files Count"].sum()
    total_dirs = df_structure["Directories"].sum()
    print(f"\nTotal Statistics:")
    print(f"Total Files: {total_files}")
    print(f"Total Directories: {total_dirs}")

except Exception as e:
    print(f"Error exploring directory: {e}")

# 如果需要查看具体的pkl文件内容
def explore_pkl_file(file_path):
    import pickle
    try:
        with open(file_path, 'rb') as f:
            data = pickle.load(f)
            print(f"\nPKL File Structure:")
            print("Keys:", list(data.keys()))
            for key in data.keys():
                print(f"{key}: {type(data[key])}")

            # 如果是图像数据，打印形状
            if 'img' in data:
                print(f"Image shape: {data['img'].shape}")
            if 'points' in data:
                print(f"Number of points: {len(data['points'])}")

    except Exception as e:
        print(f"Error reading PKL file: {e}")

# 示例：查看第一个pkl文件的内容
first_pkl = None
for root, _, files in os.walk(v1_1_path):
    for file in files:
        if file.endswith('.pkl'):
            first_pkl = os.path.join(root, file)
            break
    if first_pkl:
        break

if first_pkl:
    explore_pkl_file(first_pkl)

import pickle

# 指定一个 .pkl 文件路径，换成您要检查的具体文件
file_path = '/content/drive/MyDrive/pointlines/00079349.pkl'

with open(file_path, 'rb') as f:
    data = pickle.load(f)
    print("文件内容的键：", data.keys())
    # 打印每个键对应的数据类型
    for key in data.keys():
        print(f"{key}: {type(data[key])}")

"""# A. 图像预处理
- 将BGR转RGB
- 统一图像尺寸到512x512
- 归一化到0-1范围
- 数据增强(随机亮度/对比度、噪声、模糊等)

# B. 特征点处理
- 合并points和junctions作为特征点
- 根据图像缩放比例调整点坐标
- 生成高斯热图作为标签

# C. 批处理
- 处理不同长度的points数组
- 将数据组织成batch格式
"""

import os
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2
import pickle
import matplotlib.pyplot as plt

class WireframeDataset(Dataset):
   def __init__(self, v1_1_path, pointlines_path, split='train', img_size=512):
       self.img_size = img_size
       self.v1_1_path = Path(v1_1_path)
       self.pointlines_path = Path(pointlines_path)
       self.split = split

       # 获取图像列表
       self.img_dir = self.v1_1_path / split
       self.img_paths = list(self.img_dir.glob('*.jpg'))

       # 定义数据增强
       self.transform = A.Compose([
           A.Resize(img_size, img_size),
           A.OneOf([
               A.RandomBrightnessContrast(),
               A.RandomGamma(),
               A.HueSaturationValue(),
           ], p=0.3),
           A.OneOf([
               A.GaussNoise(),
               A.GaussianBlur(),
               A.MotionBlur(),
           ], p=0.2),
           A.Normalize(
               mean=[0.485, 0.456, 0.406],
               std=[0.229, 0.224, 0.225]
           ),
           ToTensorV2()
       ])

       print(f"Found {len(self.img_paths)} images in {split} set")

   def generate_heatmap(self, points, shape):
       """生成高斯热图"""
       heatmap = np.zeros(shape, dtype=np.float32)
       if len(points) == 0:
           return heatmap

       for (x, y) in points:
           if not (0 <= x < shape[1] and 0 <= y < shape[0]):
               continue

           # 生成高斯核
           sigma = 3
           size = 6 * sigma + 1
           x0 = np.arange(0, size, 1, float)
           y0 = x0[:, np.newaxis]
           x0 = x0 - size // 2
           y0 = y0 - size // 2
           gaussian = np.exp(- (x0 ** 2 + y0 ** 2) / (2 * sigma ** 2))

           # 计算高斯核的范围
           x, y = int(x), int(y)
           x1, y1 = max(0, x - size // 2), max(0, y - size // 2)
           x2, y2 = min(shape[1], x + size // 2 + 1), min(shape[0], y + size // 2 + 1)
           gx1, gy1 = max(0, size // 2 - x), max(0, size // 2 - y)
           gx2, gy2 = gx1 + (x2 - x1), gy1 + (y2 - y1)

           # 更新热图
           heatmap[y1:y2, x1:x2] = np.maximum(
               heatmap[y1:y2, x1:x2],
               gaussian[gy1:gy2, gx1:gx2]
           )

       return heatmap

   def __getitem__(self, idx):
       # 读取图像
       img_path = str(self.img_paths[idx])
       image = cv2.imread(img_path)
       image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
       h, w = image.shape[:2]

       # 获取对应的pkl文件
       img_name = self.img_paths[idx].stem
       pkl_path = self.pointlines_path / f"{img_name}.pkl"

       # 读取标注数据
       with open(pkl_path, 'rb') as f:
           data = pickle.load(f)

       # 获取点坐标
       points = np.array(data['points'])
       junctions = np.array(data['junction'])
       all_points = np.vstack([points, junctions])

       # 调整坐标到目标尺寸
       scale_x = self.img_size / w
       scale_y = self.img_size / h
       all_points[:, 0] *= scale_x
       all_points[:, 1] *= scale_y

       # 生成热图
       heatmap = self.generate_heatmap(all_points, (self.img_size, self.img_size))

       # 应用数据增强
       augmented = self.transform(image=image)
       image = augmented['image']

       return {
           'image': image,
           'heatmap': torch.from_numpy(heatmap)[None],  # 添加通道维度
           'points': torch.from_numpy(all_points.astype(np.float32)),
           'path': img_path
       }

   def __len__(self):
       return len(self.img_paths)

def custom_collate_fn(batch):
   """自定义的collate函数，处理不同长度的points"""
   images = torch.stack([item['image'] for item in batch])
   heatmaps = torch.stack([item['heatmap'] for item in batch])

   # 处理不同长度的points
   max_points = max([len(item['points']) for item in batch])
   points_padded = []
   for item in batch:
       points = item['points']
       padding = torch.zeros((max_points - len(points), 2), dtype=points.dtype)
       points_padded.append(torch.cat([points, padding]))
   points = torch.stack(points_padded)

   # 收集路径
   paths = [item['path'] for item in batch]

   return {
       'image': images,
       'heatmap': heatmaps,
       'points': points,
       'path': paths
   }

def create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=2):
   """创建数据加载器"""
   train_dataset = WireframeDataset(
       v1_1_path=v1_1_path,
       pointlines_path=pointlines_path,
       split='train'
   )

   val_dataset = WireframeDataset(
       v1_1_path=v1_1_path,
       pointlines_path=pointlines_path,
       split='test'
   )

   train_loader = DataLoader(
       train_dataset,
       batch_size=batch_size,
       shuffle=True,
       num_workers=num_workers,
       pin_memory=True,
       collate_fn=custom_collate_fn
   )

   val_loader = DataLoader(
       val_dataset,
       batch_size=batch_size,
       shuffle=False,
       num_workers=num_workers,
       pin_memory=True,
       collate_fn=custom_collate_fn
   )

   return train_loader, val_loader

def visualize_batch(batch):
   """可视化一个batch的数据"""
   image = batch['image'][0]
   heatmap = batch['heatmap'][0]
   points = batch['points'][0]
   path = batch['path'][0]

   plt.figure(figsize=(12, 4))

   # 显示原始图像
   plt.subplot(131)
   plt.imshow(image.permute(1,2,0).numpy())
   plt.title('Image')
   plt.axis('off')

   # 显示热图
   plt.subplot(132)
   plt.imshow(heatmap[0], cmap='jet')
   plt.title('Heatmap')
   plt.axis('off')

   # 显示带点的图像
   plt.subplot(133)
   img_with_points = image.permute(1,2,0).numpy().copy()
   valid_points = points[points.sum(dim=1) != 0]  # 只使用非零点
   for point in valid_points:
       x, y = point.int().numpy()
       cv2.circle(img_with_points, (x, y), 2, (1, 0, 0), -1)
   plt.imshow(img_with_points)
   plt.title('Points')
   plt.axis('off')

   plt.tight_layout()
   plt.show()

   print(f"Image path: {path}")
   print(f"Image shape: {image.shape}")
   print(f"Heatmap shape: {heatmap.shape}")
   print(f"Valid points count: {len(valid_points)}")

if __name__ == "__main__":
    # 设置真实路径
    v1_1_path = "/content/drive/MyDrive/v1.1"  # 替换为您的真实图像路径
    pointlines_path = "/content/drive/MyDrive/pointlines"  # 替换为您的真实pkl路径

    try:
        # 创建数据加载器
        train_loader, val_loader = create_dataloader(
            v1_1_path=v1_1_path,
            pointlines_path=pointlines_path,
            batch_size=8,
            num_workers=2
        )

        # 测试数据加载
        for batch in train_loader:
            visualize_batch(batch)
            break  # 可视化一个batch后退出
    except Exception as e:
        print(f"Error: {str(e)}")

"""# **SuperPoint model**"""

# %BANNER_BEGIN%
# ---------------------------------------------------------------------
# %COPYRIGHT_BEGIN%
#
#  Magic Leap, Inc. ("COMPANY") CONFIDENTIAL
#
#  Unpublished Copyright (c) 2020
#  Magic Leap, Inc., All Rights Reserved.
#
# NOTICE:  All information contained herein is, and remains the property
# of COMPANY. The intellectual and technical concepts contained herein
# are proprietary to COMPANY and may be covered by U.S. and Foreign
# Patents, patents in process, and are protected by trade secret or
# copyright law.  Dissemination of this information or reproduction of
# this material is strictly forbidden unless prior written permission is
# obtained from COMPANY.  Access to the source code contained herein is
# hereby forbidden to anyone except current COMPANY employees, managers
# or contractors who have executed Confidentiality and Non-disclosure
# agreements explicitly covering such access.
#
# The copyright notice above does not evidence any actual or intended
# publication or disclosure  of  this source code, which includes
# information that is confidential and/or proprietary, and is a trade
# secret, of  COMPANY.   ANY REPRODUCTION, MODIFICATION, DISTRIBUTION,
# PUBLIC  PERFORMANCE, OR PUBLIC DISPLAY OF OR THROUGH USE  OF THIS
# SOURCE CODE  WITHOUT THE EXPRESS WRITTEN CONSENT OF COMPANY IS
# STRICTLY PROHIBITED, AND IN VIOLATION OF APPLICABLE LAWS AND
# INTERNATIONAL TREATIES.  THE RECEIPT OR POSSESSION OF  THIS SOURCE
# CODE AND/OR RELATED INFORMATION DOES NOT CONVEY OR IMPLY ANY RIGHTS
# TO REPRODUCE, DISCLOSE OR DISTRIBUTE ITS CONTENTS, OR TO MANUFACTURE,
# USE, OR SELL ANYTHING THAT IT  MAY DESCRIBE, IN WHOLE OR IN PART.
#
# %COPYRIGHT_END%
# ----------------------------------------------------------------------
# %AUTHORS_BEGIN%
#
#  Originating Authors: Paul-Edouard Sarlin
#
# %AUTHORS_END%
# --------------------------------------------------------------------*/
# %BANNER_END%

import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2
import pickle
import matplotlib.pyplot as plt
from tqdm import tqdm
import time
import math
import warnings

warnings.filterwarnings("ignore")

# =======================================
# Data Preprocessing Utilities
# =======================================

def compute_gradients(image):
    """
    Compute gradient magnitude and angle using Sobel filters.
    """
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    gray = gray.astype(np.float32) / 255.0

    grad_x = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)
    grad_y = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)
    grad_mag = np.sqrt(grad_x**2 + grad_y**2)
    angle_map = np.arctan2(grad_y, grad_x) * 180 / np.pi
    angle_map = (angle_map + 180) % 180  # Normalize to [0, 180)

    return grad_x, grad_y, grad_mag, angle_map

def region_grow(grad_x, grad_y, angle_map, tolerance=22.5):
    """
    Perform region growing based on gradient orientations.
    """
    h, w = angle_map.shape
    status = np.zeros((h, w), dtype=np.uint8)  # 0: Not Used, 1: Used
    regions = []

    # Sort pixels by gradient magnitude (descending)
    grad_mag = np.sqrt(grad_x**2 + grad_y**2)
    sorted_indices = np.argsort(-grad_mag, axis=None)

    for idx in sorted_indices:
        y, x = divmod(idx, w)
        if status[y, x] != 0 or grad_mag[y, x] < 1e-3:
            continue
        current_angle = angle_map[y, x]
        region = [(x, y)]
        status[y, x] = 1
        queue = [(x, y)]

        while queue:
            cx, cy = queue.pop(0)
            neighbors = [
                (nx, ny)
                for nx in range(cx - 1, cx + 2)
                for ny in range(cy - 1, cy + 2)
                if 0 <= nx < w and 0 <= ny < h and (nx != cx or ny != cy)
            ]
            for nx, ny in neighbors:
                if status[ny, nx] == 0 and abs(angle_map[ny, nx] - current_angle) < tolerance:
                    status[ny, nx] = 1
                    region.append((nx, ny))
                    queue.append((nx, ny))

        regions.append(region)

    return regions

def approximate_rectangles(regions):
    """
    Approximate each region with a rectangle.
    """
    rectangles = []
    for region in regions:
        if len(region) < 2:
            continue
        pts = np.array(region, dtype=np.float32)
        rect = cv2.minAreaRect(pts)
        (center_x, center_y), (width, length), angle = rect

        # Ensure width >= length
        if width < length:
            width, length = length, width
            angle += 90

        rectangles.append({
            'center': (center_x, center_y),
            'angle': angle,
            'length': length,
            'width': width,
            'points': pts
        })

    return rectangles

def compute_nfa(rectangle, image_size, p=0.125, epsilon=1):
    """
    Compute the Number of False Alarms (NFA) for a rectangle using logarithms to prevent overflow.
    Returns log_nfa for comparison.
    """
    num_potential = image_size[0] ** 5

    n = int(rectangle['length'] * rectangle['width'])
    k = len(rectangle['points'])
    if n < k:
        return float('inf')  # Impossible case

    try:
        log_comb = math.lgamma(n + 1) - math.lgamma(k + 1) - math.lgamma(n - k + 1)
    except OverflowError:
        return float('inf')

    try:
        log_prob = k * math.log(p) + (n - k) * math.log(1 - p)
    except ValueError:
        return float('inf')

    # Compute log(NFA)
    log_nfa = math.log(num_potential) + log_comb + log_prob

    return log_nfa

def validate_rectangles(rectangles, image_size, epsilon=1):
    """
    Validate rectangles based on NFA.
    Returns a list of valid rectangles.
    """
    valid_rects = []
    log_epsilon = math.log(epsilon)
    for rect in rectangles:
        log_nfa = compute_nfa(rect, image_size, p=0.125, epsilon=epsilon)
        if log_nfa < log_epsilon:
            valid_rects.append(rect)
    return valid_rects

# =======================================
# Dataset Class
# =======================================

class WireframeDataset(Dataset):
    def __init__(self, v1_1_path, pointlines_path, split='train', img_size=256, nfa_threshold=1):
        self.img_size = img_size
        self.v1_1_path = Path(v1_1_path)
        self.pointlines_path = Path(pointlines_path)
        self.split = split
        self.nfa_threshold = nfa_threshold

        # Get image list
        self.img_dir = self.v1_1_path / split
        self.img_paths = list(self.img_dir.glob('*.jpg'))

        # Define data augmentation
        self.transform = A.Compose([
            A.Resize(img_size, img_size),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            ),
            ToTensorV2()
        ])

        print(f"Found {len(self.img_paths)} images in {split} set")

    def generate_heatmap(self, points, shape):
        """Generate Gaussian heatmap for keypoints."""
        heatmap = np.zeros(shape, dtype=np.float32)
        if len(points) == 0:
            return heatmap

        for (x, y) in points:
            if not (0 <= x < shape[1] and 0 <= y < shape[0]):
                continue

            sigma = 2
            size = int(6 * sigma + 1)
            x0 = np.arange(0, size, 1, float)
            y0 = x0[:, np.newaxis]
            x0 = x0 - size // 2
            y0 = y0 - size // 2
            gaussian = np.exp(- (x0 ** 2 + y0 ** 2) / (2 * sigma ** 2))

            x, y = int(x), int(y)
            x1, y1 = max(0, x - size // 2), max(0, y - size // 2)
            x2, y2 = min(shape[1], x + size // 2 + 1), min(shape[0], y + size // 2 + 1)
            gx1, gy1 = max(0, size // 2 - x), max(0, size // 2 - y)
            gx2, gy2 = gx1 + (x2 - x1), gy1 + (y2 - y1)

            heatmap[y1:y2, x1:x2] = np.maximum(
                heatmap[y1:y2, x1:x2],
                gaussian[gy1:gy2, gx1:gx2]
            )

        return heatmap

    def generate_line_heatmap(self, lines, shape):
        """Generate heatmap for lines."""
        heatmap = np.zeros(shape, dtype=np.float32)
        if len(lines) == 0:
            return heatmap

        for (x1, y1, x2, y2) in lines:
            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])
            cv2.line(heatmap, (x1, y1), (x2, y2), 1, thickness=1)

        return heatmap

    def __getitem__(self, idx):
        img_path = str(self.img_paths[idx])
        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image not found: {img_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w = image.shape[:2]

        img_name = self.img_paths[idx].stem
        pkl_path = self.pointlines_path / f"{img_name}.pkl"

        if not pkl_path.exists():
            raise FileNotFoundError(f"Pkl file not found: {pkl_path}")
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)

        points = np.array(data['points'])  # [N, 2]
        lines = np.array(data['lines'])    # [M, 2] (indices of points)

        # Convert lines from point indices to coordinates
        if lines.size > 0:
            lines_coords = []
            for line in lines:
                idx1, idx2 = line.astype(int)
                if idx1 >= len(points) or idx2 >= len(points):
                    continue
                x1, y1 = points[idx1]
                x2, y2 = points[idx2]
                lines_coords.append([x1, y1, x2, y2])
            lines = np.array(lines_coords)
        else:
            lines = np.zeros((0, 4))

        # Scale coordinates
        scale_x = self.img_size / w
        scale_y = self.img_size / h
        points[:, 0] *= scale_x
        points[:, 1] *= scale_y
        if lines.size > 0:
            lines[:, [0, 2]] *= scale_x
            lines[:, [1, 3]] *= scale_y

        keypoint_heatmap = self.generate_heatmap(points, (self.img_size, self.img_size))
        resized_image = cv2.resize(image, (self.img_size, self.img_size))
        grad_x, grad_y, grad_mag, angle_map = compute_gradients(resized_image)

        regions = region_grow(grad_x, grad_y, angle_map, tolerance=22.5)
        rectangles = approximate_rectangles(regions)
        valid_rects = validate_rectangles(rectangles, (self.img_size, self.img_size), epsilon=self.nfa_threshold)

        line_heatmap_lsd = np.zeros((self.img_size, self.img_size), dtype=np.float32)
        for rect in valid_rects:
            box = cv2.boxPoints((
                (rect['center'][0], rect['center'][1]),
                (rect['length'], rect['width']),
                rect['angle']
            ))
            box = np.int0(box)
            cv2.polylines(line_heatmap_lsd, [box], isClosed=True, color=1, thickness=1)

        # Convert image to grayscale for SuperPoint
        gray_image = cv2.cvtColor(resized_image, cv2.COLOR_RGB2GRAY)
        augmented = self.transform(image=resized_image)
        image_tensor = augmented['image']
        angle_map = angle_map.astype(np.int64)

        # Normalize grayscale image and convert to tensor
        gray_augmented = A.Compose([
            A.Normalize(
                mean=[0.485],
                std=[0.229]
            ),
            ToTensorV2()
        ])(image=gray_image)
        gray_tensor = gray_augmented['image']  # [1, H, W]

        return {
            'image_rgb': image_tensor,  # For potential other uses
            'image_gray': gray_tensor,  # For SuperPoint
            'keypoint_heatmap': torch.from_numpy(keypoint_heatmap)[None],
            'line_heatmap': torch.from_numpy(line_heatmap_lsd)[None],
            'points': torch.from_numpy(points.astype(np.float32)),
            'lines': torch.from_numpy(lines.astype(np.float32)),
            'angle_map': torch.from_numpy(angle_map),
            'path': img_path
        }

    def __len__(self):
        return len(self.img_paths)

def custom_collate_fn(batch):
    images_rgb = torch.stack([item['image_rgb'] for item in batch])
    images_gray = torch.stack([item['image_gray'] for item in batch])
    keypoint_heatmaps = torch.stack([item['keypoint_heatmap'] for item in batch])
    line_heatmaps = torch.stack([item['line_heatmap'] for item in batch])

    max_points = max([len(item['points']) for item in batch])
    max_lines = max([len(item['lines']) for item in batch])
    points_padded = []
    lines_padded = []
    for item in batch:
        points = item['points']
        lines = item['lines']
        points_padding = torch.zeros((max_points - len(points), 2), dtype=points.dtype)
        lines_padding = torch.zeros((max_lines - len(lines), 4), dtype=lines.dtype)
        points_padded.append(torch.cat([points, points_padding]))
        lines_padded.append(torch.cat([lines, lines_padding]))
    points = torch.stack(points_padded)
    lines = torch.stack(lines_padded)

    angle_maps = torch.stack([item['angle_map'] for item in batch])
    paths = [item['path'] for item in batch]

    return {
        'image_rgb': images_rgb,
        'image_gray': images_gray,
        'keypoint_heatmap': keypoint_heatmaps,
        'line_heatmap': line_heatmaps,
        'points': points,
        'lines': lines,
        'angle_map': angle_maps,
        'path': paths
    }

def create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=4):
    train_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='train',
        img_size=256,
        nfa_threshold=1
    )

    val_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='test',
        img_size=256,
        nfa_threshold=1
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    return train_loader, val_loader

# =======================================
# SuperPoint Network
# =======================================

class SuperPoint(nn.Module):
    """SuperPoint Convolutional Detector and Descriptor

    SuperPoint: Self-Supervised Interest Point Detection and
    Description. Daniel DeTone, Tomasz Malisiewicz, and Andrew
    Rabinovich. In CVPRW, 2019. https://arxiv.org/abs/1712.07629

    """
    default_config = {
        'descriptor_dim': 256,
        'nms_radius': 4,
        'keypoint_threshold': 0.005,
        'max_keypoints': -1,
        'remove_borders': 4,
    }

    def __init__(self, config={}):
        super().__init__()
        self.config = {**self.default_config, **config}

        self.relu = nn.ReLU(inplace=True)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        c1, c2, c3, c4, c5 = 64, 64, 128, 128, 256

        self.conv1a = nn.Conv2d(1, c1, kernel_size=3, stride=1, padding=1)
        self.conv1b = nn.Conv2d(c1, c1, kernel_size=3, stride=1, padding=1)
        self.conv2a = nn.Conv2d(c1, c2, kernel_size=3, stride=1, padding=1)
        self.conv2b = nn.Conv2d(c2, c2, kernel_size=3, stride=1, padding=1)
        self.conv3a = nn.Conv2d(c2, c3, kernel_size=3, stride=1, padding=1)
        self.conv3b = nn.Conv2d(c3, c3, kernel_size=3, stride=1, padding=1)
        self.conv4a = nn.Conv2d(c3, c4, kernel_size=3, stride=1, padding=1)
        self.conv4b = nn.Conv2d(c4, c4, kernel_size=3, stride=1, padding=1)

        # Change convPb to output 1 channel for binary keypoint detection
        self.convPa = nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)
        self.convPb = nn.Conv2d(c5, 1, kernel_size=1, stride=1, padding=0)

        self.convDa = nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)
        self.convDb = nn.Conv2d(
            c5, self.config['descriptor_dim'],
            kernel_size=1, stride=1, padding=0)

        # Initialize weights (assuming pretrained weights are not available)
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

        mk = self.config['max_keypoints']
        if mk == 0 or mk < -1:
            raise ValueError('\"max_keypoints\" must be positive or \"-1\"')

        print('Initialized SuperPoint model')

    def forward(self, data):
        """ Compute keypoints, scores, descriptors for image """
        # Shared Encoder
        x = self.relu(self.conv1a(data['image_gray']))
        x = self.relu(self.conv1b(x))
        x = self.pool(x)
        x = self.relu(self.conv2a(x))
        x = self.relu(self.conv2b(x))
        x = self.pool(x)
        x = self.relu(self.conv3a(x))
        x = self.relu(self.conv3b(x))
        x = self.pool(x)
        x = self.relu(self.conv4a(x))
        x = self.relu(self.conv4b(x))

        # Compute the dense keypoint scores (raw logits)
        cPa = self.relu(self.convPa(x))
        raw_scores = self.convPb(cPa)  # [B,1,H,W]

        # Compute keypoints for inference
        # Remove softmax and NMS for training
        scores = torch.sigmoid(raw_scores)  # Apply sigmoid for probability

        if not self.training:
            # NMS and keypoint extraction (only for inference)
            keypoints = []
            for i in range(scores.shape[0]):
                # Detach the tensor before moving to CPU and converting to NumPy
                score_map = scores[i, 0].detach().cpu().numpy()
                keypoints_i = np.argwhere(score_map > self.config['keypoint_threshold'])
                keypoints_i = keypoints_i[:, [1, 0]]  # (x, y)
                # Apply NMS if needed (can be implemented here)
                keypoints_i = torch.tensor(keypoints_i, dtype=torch.float32)
                keypoints.append(keypoints_i)
        else:
            keypoints = None

        # Compute the dense descriptors
        cDa = self.relu(self.convDa(x))
        descriptors = self.convDb(cDa)
        descriptors = torch.nn.functional.normalize(descriptors, p=2, dim=1)

        return {
            'keypoints': keypoints,          # List of keypoints per image in batch, None during training
            'scores': scores,                # [B,1,H,W]
            'raw_scores': raw_scores,        # [B,1,H,W] for loss
            'descriptors': descriptors,      # [B,D,H',W']
        }

# =======================================
# Helper Functions for SuperPoint
# =======================================

def simple_nms(scores, nms_radius: int):
    """ Fast Non-maximum suppression to remove nearby points """
    assert(nms_radius >= 0)

    def max_pool(x):
        return torch.nn.functional.max_pool2d(
            x, kernel_size=nms_radius*2+1, stride=1, padding=nms_radius)

    zeros = torch.zeros_like(scores)
    max_mask = scores == max_pool(scores)
    for _ in range(2):
        supp_mask = max_pool(max_mask.float()) > 0
        supp_scores = torch.where(supp_mask, zeros, scores)
        new_max_mask = supp_scores == max_pool(supp_scores)
        max_mask = max_mask | (new_max_mask & (~supp_mask))
    return torch.where(max_mask, scores, zeros)


def remove_borders(keypoints, scores, border: int, height: int, width: int):
    """ Removes keypoints too close to the border """
    mask_h = (keypoints[:, 0] >= border) & (keypoints[:, 0] < (height - border))
    mask_w = (keypoints[:, 1] >= border) & (keypoints[:, 1] < (width - border))
    mask = mask_h & mask_w
    return keypoints[mask], scores[mask]


def top_k_keypoints(keypoints, scores, k: int):
    if k >= len(keypoints):
        return keypoints, scores
    scores, indices = torch.topk(scores, k, dim=0)
    return keypoints[indices], scores


def sample_descriptors(keypoints, descriptors, s: int = 8):
    """ Interpolate descriptors at keypoint locations """
    b, c, h, w = descriptors.shape
    keypoints = keypoints - s / 2 + 0.5
    keypoints /= torch.tensor([(w*s - s/2 - 0.5), (h*s - s/2 - 0.5)],
                              ).to(keypoints)[None]
    keypoints = keypoints*2 - 1  # normalize to (-1, 1)
    args = {'align_corners': True} if torch.__version__ >= '1.3' else {}
    descriptors = torch.nn.functional.grid_sample(
        descriptors, keypoints.view(b, 1, -1, 2), mode='bilinear', **args)
    descriptors = torch.nn.functional.normalize(
        descriptors.reshape(b, c, -1), p=2, dim=1)
    return descriptors

# =======================================
# Metrics
# =======================================

def calculate_sap(pred_points, gt_points, threshold=3.0):
    if len(pred_points) == 0 or len(gt_points) == 0:
        return 0.0
    distances = torch.cdist(pred_points.float(), gt_points.float())
    min_distances, _ = torch.min(distances, dim=1)
    correct_matches = (min_distances < threshold).float()

    precision = correct_matches.sum() / len(pred_points) if len(pred_points) > 0 else 0
    recall = correct_matches.sum() / len(gt_points) if len(gt_points) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)
    return f1.item()

def calculate_lap(pred_lines, gt_lines, dist_threshold=5.0, angle_threshold=10.0):
    if len(pred_lines) == 0 or len(gt_lines) == 0:
        return 0.0

    def compute_line_similarity(line1, line2):
        line1 = line1.float()
        line2 = line2.float()

        dist1 = torch.sqrt(((line1[:2] - line2[:2]) ** 2).sum())
        dist2 = torch.sqrt(((line1[2:] - line2[:2]) ** 2).sum())

        vec1 = line1[2:] - line1[:2]
        vec2 = line2[2:] - line2[:2]

        norm1 = torch.sqrt((vec1 * vec1).sum())
        norm2 = torch.sqrt((vec2 * vec2).sum())

        cos_angle = (vec1 * vec2).sum() / (norm1 * norm2 + 1e-8)
        angle = torch.acos(torch.clamp(cos_angle, -1.0, 1.0)) * 180 / np.pi
        return dist1.item(), angle.item()

    matches = 0
    matched_gt = set()
    for pred_line in pred_lines:
        best_dist = float('inf')
        best_angle = float('inf')
        best_gt = -1

        for idx, gt_line in enumerate(gt_lines):
            if idx in matched_gt:
                continue
            dist, angle = compute_line_similarity(pred_line, gt_line)
            if dist < best_dist:
                best_dist = dist
                best_angle = angle
                best_gt = idx

        if best_dist < dist_threshold and best_angle < angle_threshold:
            matches += 1
            matched_gt.add(best_gt)

    precision = matches / len(pred_lines) if len(pred_lines) > 0 else 0
    recall = matches / len(gt_lines) if len(gt_lines) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)
    return f1

# =======================================
# Training and Evaluation
# =======================================

def train_model(v1_1_path, pointlines_path, model_save_path="superpoint_model", batch_size=8, num_workers=4, num_epochs=20):
    train_loader, val_loader = create_dataloader(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        batch_size=batch_size,
        num_workers=num_workers
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SuperPoint().to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
    criterion_keypoint = nn.BCEWithLogitsLoss()

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            images_gray = batch['image_gray'].to(device)
            keypoint_targets = batch['keypoint_heatmap'].to(device)  # [B,1,256,256]

            optimizer.zero_grad()
            outputs = model({'image_gray': images_gray})

            pred_keypoint_logits = outputs['raw_scores']  # [B,1,H_out,W_out], e.g. [B,1,32,32]

            # Downsample the target heatmap to match the prediction size
            # Use nearest or bilinear interpolation. Nearest is commonly used for heatmaps.
            keypoint_targets_resized = F.interpolate(keypoint_targets, size=pred_keypoint_logits.shape[-2:], mode='nearest')

            # Compute keypoint detection loss at the downsampled resolution
            loss_keypoint = criterion_keypoint(pred_keypoint_logits, keypoint_targets_resized)

            # Descriptor loss
            # Since we don't have ground truth descriptors, this part is skipped
            # Alternatively, implement a self-supervised descriptor loss if applicable
            loss_descriptor = torch.tensor(0.0).to(device)

            # Total loss
            loss = loss_keypoint + loss_descriptor
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        scheduler.step()
        avg_loss = running_loss / len(train_loader)
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")
        torch.save(model.state_dict(), f"{model_save_path}_epoch_{epoch+1}.pth")

    print("Training completed.")

def evaluate_model(v1_1_path, pointlines_path, model_path="superpoint_model_epoch_20.pth", batch_size=1, num_workers=4):
    _, val_loader = create_dataloader(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        batch_size=batch_size,
        num_workers=num_workers
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SuperPoint().to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    sap_scores = []
    lap_scores = []
    fps_times = []

    print("Starting evaluation...")
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(val_loader, desc="Evaluating")):
            images_gray = batch['image_gray'].to(device)
            gt_points = batch['points'][0]
            gt_lines = batch['lines'][0]
            angle_maps = batch['angle_map'].to(device)

            start_time = time.time()
            outputs = model({'image_gray': images_gray})
            inference_time = time.time() - start_time
            fps = 1.0 / inference_time if inference_time > 0 else 0.0
            fps_times.append(fps)

            # Keypoint detection
            keypoint_output = torch.sigmoid(outputs['raw_scores'])  # [B,1,H,W]
            keypoint_map = keypoint_output.cpu().squeeze().numpy()
            pred_points_y, pred_points_x = np.where(keypoint_map > 0.5)
            pred_points = torch.tensor(np.stack([pred_points_x, pred_points_y], axis=1))

            # Descriptor extraction (if needed)
            # pred_descriptors = outputs['descriptors']  # [B,D,H',W']

            sap = calculate_sap(pred_points, gt_points)
            lap = calculate_lap(torch.zeros((0, 4)), gt_lines)  # Line evaluation not applicable
            sap_scores.append(sap)
            lap_scores.append(lap)

            if batch_idx % 50 == 0:
                image_np = batch['image_rgb'].cpu().squeeze().permute(1, 2, 0).numpy()
                image_np = (image_np * np.array([0.229, 0.224, 0.225]) +
                            np.array([0.485, 0.456, 0.406]))
                image_np = np.clip(image_np, 0, 1)

                plt.figure(figsize=(15, 5))

                plt.subplot(131)
                plt.imshow(image_np)
                plt.title('Input Image')
                plt.axis('off')

                plt.subplot(132)
                plt.imshow(keypoint_map, cmap='jet')
                plt.plot(pred_points[:, 0], pred_points[:, 1], 'r.', markersize=2)
                plt.title(f'Keypoints (num={len(pred_points)})')
                plt.axis('off')

                plt.subplot(133)
                plt.imshow(batch['line_heatmap'].cpu().squeeze().numpy(), cmap='gray')
                plt.title(f'Lines (num={len(gt_lines)})')
                plt.axis('off')

                plt.tight_layout()
                plt.show()

    mean_sap = np.mean(sap_scores) if len(sap_scores) > 0 else 0.0
    mean_lap = np.mean(lap_scores) if len(lap_scores) > 0 else 0.0
    mean_fps = np.mean(fps_times) if len(fps_times) > 0 else 0.0

    print("\nEvaluation Results:")
    print(f"SAP (Structure-Aware Performance - F1 Score): {mean_sap:.4f}")
    print(f"LAP (Line-Aware Performance - F1 Score): {mean_lap:.4f}")
    print(f"Average FPS: {mean_fps:.2f}")

# =======================================
# Main Execution
# =======================================

if __name__ == "__main__":
    # Please modify the dataset paths accordingly
    v1_1_path = "/content/drive/MyDrive/v1.1"
    pointlines_path = "/content/drive/MyDrive/pointlines"

    if not os.path.exists(v1_1_path):
        raise FileNotFoundError(f"v1.1_path not found: {v1_1_path}")
    if not os.path.exists(pointlines_path):
        raise FileNotFoundError(f"pointlines_path not found: {pointlines_path}")

    print("Starting Training...")
    train_model(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        model_save_path="superpoint_model",
        batch_size=8,
        num_workers=4,
        num_epochs=20
    )

    print("\nStarting Evaluation...")
    evaluate_model(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        model_path="superpoint_model_epoch_20.pth",
        batch_size=1,
        num_workers=4
    )









"""# **线特征数据预处理**"""

import os
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2
import pickle
import matplotlib.pyplot as plt

class WireframeDataset(Dataset):
    def __init__(self, v1_1_path, pointlines_path, split='train', img_size=512):
        self.img_size = img_size
        self.v1_1_path = Path(v1_1_path)
        self.pointlines_path = Path(pointlines_path)
        self.split = split

        # 获取图像列表
        self.img_dir = self.v1_1_path / split
        self.img_paths = list(self.img_dir.glob('*.jpg'))

        # 定义数据增强
        self.transform = A.Compose([
            A.Resize(img_size, img_size),
            A.OneOf([
                A.RandomBrightnessContrast(),
                A.RandomGamma(),
                A.HueSaturationValue(),
            ], p=0.3),
            A.OneOf([
                A.GaussNoise(),
                A.GaussianBlur(),
                A.MotionBlur(),
            ], p=0.2),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            ),
            ToTensorV2()
        ])

        print(f"Found {len(self.img_paths)} images in {split} set")

    def generate_heatmap(self, points, shape):
        """生成特征点的高斯热图"""
        heatmap = np.zeros(shape, dtype=np.float32)
        if len(points) == 0:
            return heatmap

        for (x, y) in points:
            if not (0 <= x < shape[1] and 0 <= y < shape[0]):
                continue

            # 生成高斯核
            sigma = 3
            size = 6 * sigma + 1
            x0 = np.arange(0, size, 1, float)
            y0 = x0[:, np.newaxis]
            x0 = x0 - size // 2
            y0 = y0 - size // 2
            gaussian = np.exp(- (x0 ** 2 + y0 ** 2) / (2 * sigma ** 2))

            # 计算高斯核的范围
            x, y = int(x), int(y)
            x1, y1 = max(0, x - size // 2), max(0, y - size // 2)
            x2, y2 = min(shape[1], x + size // 2 + 1), min(shape[0], y + size // 2 + 1)
            gx1, gy1 = max(0, size // 2 - x), max(0, size // 2 - y)
            gx2, gy2 = gx1 + (x2 - x1), gy1 + (y2 - y1)

            # 更新热图
            heatmap[y1:y2, x1:x2] = np.maximum(
                heatmap[y1:y2, x1:x2],
                gaussian[gy1:gy2, gx1:gx2]
            )

        return heatmap

    def generate_line_heatmap(self, lines, shape):
        """生成特征线的热图"""
        heatmap = np.zeros(shape, dtype=np.float32)
        if len(lines) == 0:
            return heatmap

        for (x1, y1, x2, y2) in lines:
            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])
            cv2.line(heatmap, (x1, y1), (x2, y2), 1, thickness=1)

        return heatmap

    def __getitem__(self, idx):
        # 读取图像
        img_path = str(self.img_paths[idx])
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w = image.shape[:2]

        # 获取对应的pkl文件
        img_name = self.img_paths[idx].stem
        pkl_path = self.pointlines_path / f"{img_name}.pkl"

        # 读取标注数据
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)

        # 获取点和线的坐标
        points = np.array(data['points'])  # [N, 2]
        lines = np.array(data['lines'])    # [M, 2]，表示点的索引

        # 将lines从点的索引转换为坐标
        if lines.size > 0:
            lines_coords = []
            for line in lines:
                idx1, idx2 = line.astype(int)
                x1, y1 = points[idx1]
                x2, y2 = points[idx2]
                lines_coords.append([x1, y1, x2, y2])
            lines = np.array(lines_coords)
        else:
            lines = np.zeros((0, 4))  # 没有线的情况

        # 调整坐标到目标尺寸
        scale_x = self.img_size / w
        scale_y = self.img_size / h
        points[:, 0] *= scale_x
        points[:, 1] *= scale_y
        if lines.size > 0:
            lines[:, [0, 2]] *= scale_x
            lines[:, [1, 3]] *= scale_y

        # 生成热图
        keypoint_heatmap = self.generate_heatmap(points, (self.img_size, self.img_size))
        line_heatmap = self.generate_line_heatmap(lines, (self.img_size, self.img_size))

        # 应用数据增强
        augmented = self.transform(image=image)
        image = augmented['image']

        return {
            'image': image,
            'keypoint_heatmap': torch.from_numpy(keypoint_heatmap)[None],
            'line_heatmap': torch.from_numpy(line_heatmap)[None],
            'points': torch.from_numpy(points.astype(np.float32)),
            'lines': torch.from_numpy(lines.astype(np.float32)),
            'path': img_path
        }

    def __len__(self):
        return len(self.img_paths)

def custom_collate_fn(batch):
    """自定义的collate函数，处理不同长度的points和lines"""
    images = torch.stack([item['image'] for item in batch])
    keypoint_heatmaps = torch.stack([item['keypoint_heatmap'] for item in batch])
    line_heatmaps = torch.stack([item['line_heatmap'] for item in batch])

    # 处理不同长度的points和lines
    max_points = max([len(item['points']) for item in batch])
    max_lines = max([len(item['lines']) for item in batch])
    points_padded = []
    lines_padded = []
    for item in batch:
        points = item['points']
        lines = item['lines']
        points_padding = torch.zeros((max_points - len(points), 2), dtype=points.dtype)
        lines_padding = torch.zeros((max_lines - len(lines), 4), dtype=lines.dtype)
        points_padded.append(torch.cat([points, points_padding]))
        lines_padded.append(torch.cat([lines, lines_padding]))
    points = torch.stack(points_padded)
    lines = torch.stack(lines_padded)

    paths = [item['path'] for item in batch]

    return {
        'image': images,
        'keypoint_heatmap': keypoint_heatmaps,
        'line_heatmap': line_heatmaps,
        'points': points,
        'lines': lines,
        'path': paths
    }

def create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=2):
    """创建数据加载器"""
    train_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='train'
    )

    val_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='test'
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    return train_loader, val_loader

def visualize_batch(batch):
    """可视化一个batch的数据"""
    image = batch['image'][0]
    keypoint_heatmap = batch['keypoint_heatmap'][0]
    line_heatmap = batch['line_heatmap'][0]
    points = batch['points'][0]
    lines = batch['lines'][0]
    path = batch['path'][0]

    plt.figure(figsize=(16, 4))

    # 显示原始图像
    plt.subplot(141)
    plt.imshow(image.permute(1, 2, 0).numpy())
    plt.title('Image')
    plt.axis('off')

    # 显示特征点热图
    plt.subplot(142)
    plt.imshow(keypoint_heatmap[0], cmap='jet')
    plt.title('Keypoint Heatmap')
    plt.axis('off')

    # 显示特征线热图
    plt.subplot(143)
    plt.imshow(line_heatmap[0], cmap='jet')
    plt.title('Line Heatmap')
    plt.axis('off')

    # 显示带点和线的图像
    plt.subplot(144)
    img_with_annotations = image.permute(1, 2, 0).numpy().copy()
    img_with_annotations = cv2.cvtColor(img_with_annotations, cv2.COLOR_RGB2BGR)
    valid_points = points[points.sum(dim=1) != 0]
    valid_lines = lines[lines.sum(dim=1) != 0]
    for point in valid_points:
        x, y = point.int().numpy()
        cv2.circle(img_with_annotations, (x, y), 2, (0, 0, 255), -1)
    for line in valid_lines:
        x1, y1, x2, y2 = line.int().numpy()
        cv2.line(img_with_annotations, (x1, y1), (x2, y2), (0, 255, 0), 1)
    img_with_annotations = cv2.cvtColor(img_with_annotations, cv2.COLOR_BGR2RGB)
    plt.imshow(img_with_annotations)
    plt.title('Annotations')
    plt.axis('off')

    plt.tight_layout()
    plt.show()

    print(f"Image path: {path}")
    print(f"Image shape: {image.shape}")
    print(f"Valid points count: {len(valid_points)}")
    print(f"Valid lines count: {len(valid_lines)}")

if __name__ == "__main__":
    # 设置真实路径
    v1_1_path = "/content/drive/MyDrive/v1.1"  # 替换为您的真实图像路径
    pointlines_path = "/content/drive/MyDrive/pointlines"   # 替换为您的真实pkl路径

    try:
        # 创建数据加载器
        train_loader, val_loader = create_dataloader(
            v1_1_path=v1_1_path,
            pointlines_path=pointlines_path,
            batch_size=8,
            num_workers=2
        )

        # 测试数据加载
        for batch in train_loader:
            visualize_batch(batch)
            break  # 可视化一个batch后退出
    except Exception as e:
        print(f"Error: {str(e)}")

"""# **ResNet18作为基线的网络：**"""

import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision.models import resnet18
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2
import pickle
import matplotlib.pyplot as plt
from tqdm import tqdm
import time

# =======================================
# 数据预处理部分
# =======================================

class WireframeDataset(Dataset):
    def __init__(self, v1_1_path, pointlines_path, split='train', img_size=256):
        self.img_size = img_size
        self.v1_1_path = Path(v1_1_path)
        self.pointlines_path = Path(pointlines_path)
        self.split = split

        # 获取图像列表
        self.img_dir = self.v1_1_path / split
        self.img_paths = list(self.img_dir.glob('*.jpg'))

        # 定义数据增强
        self.transform = A.Compose([
            A.Resize(img_size, img_size),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            ),
            ToTensorV2()
        ])

        print(f"Found {len(self.img_paths)} images in {split} set")

    def generate_heatmap(self, points, shape):
        """生成特征点的高斯热图"""
        heatmap = np.zeros(shape, dtype=np.float32)
        if len(points) == 0:
            return heatmap

        for (x, y) in points:
            if not (0 <= x < shape[1] and 0 <= y < shape[0]):
                continue

            # 生成高斯核
            sigma = 2
            size = int(6 * sigma + 1)
            x0 = np.arange(0, size, 1, float)
            y0 = x0[:, np.newaxis]
            x0 = x0 - size // 2
            y0 = y0 - size // 2
            gaussian = np.exp(- (x0 ** 2 + y0 ** 2) / (2 * sigma ** 2))

            # 计算高斯核的范围
            x, y = int(x), int(y)
            x1, y1 = max(0, x - size // 2), max(0, y - size // 2)
            x2, y2 = min(shape[1], x + size // 2 + 1), min(shape[0], y + size // 2 + 1)
            gx1, gy1 = max(0, size // 2 - x), max(0, size // 2 - y)
            gx2, gy2 = gx1 + (x2 - x1), gy1 + (y2 - y1)

            # 更新热图
            heatmap[y1:y2, x1:x2] = np.maximum(
                heatmap[y1:y2, x1:x2],
                gaussian[gy1:gy2, gx1:gx2]
            )

        return heatmap

    def generate_line_heatmap(self, lines, shape):
        """生成特征线的热图"""
        heatmap = np.zeros(shape, dtype=np.float32)
        if len(lines) == 0:
            return heatmap

        for (x1, y1, x2, y2) in lines:
            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])
            cv2.line(heatmap, (x1, y1), (x2, y2), 1, thickness=1)

        return heatmap

    def __getitem__(self, idx):
        # 读取图像
        img_path = str(self.img_paths[idx])
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w = image.shape[:2]

        # 获取对应的pkl文件
        img_name = self.img_paths[idx].stem
        pkl_path = self.pointlines_path / f"{img_name}.pkl"

        # 读取标注数据
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)

        # 获取点和线的坐标
        points = np.array(data['points'])  # [N, 2]
        lines = np.array(data['lines'])    # [M, 2]，表示点的索引

        # 将lines从点的索引转换为坐标
        if lines.size > 0:
            lines_coords = []
            for line in lines:
                idx1, idx2 = line.astype(int)
                x1, y1 = points[idx1]
                x2, y2 = points[idx2]
                lines_coords.append([x1, y1, x2, y2])
            lines = np.array(lines_coords)
        else:
            lines = np.zeros((0, 4))  # 没有线的情况

        # 调整坐标到目标尺寸
        scale_x = self.img_size / w
        scale_y = self.img_size / h
        points[:, 0] *= scale_x
        points[:, 1] *= scale_y
        if lines.size > 0:
            lines[:, [0, 2]] *= scale_x
            lines[:, [1, 3]] *= scale_y

        # 生成热图
        keypoint_heatmap = self.generate_heatmap(points, (self.img_size, self.img_size))
        line_heatmap = self.generate_line_heatmap(lines, (self.img_size, self.img_size))

        # 应用数据增强
        augmented = self.transform(image=image)
        image = augmented['image']

        return {
            'image': image,
            'keypoint_heatmap': torch.from_numpy(keypoint_heatmap)[None],
            'line_heatmap': torch.from_numpy(line_heatmap)[None],
            'points': torch.from_numpy(points.astype(np.float32)),
            'lines': torch.from_numpy(lines.astype(np.float32)),
            'path': img_path
        }

    def __len__(self):
        return len(self.img_paths)

def custom_collate_fn(batch):
    """自定义的collate函数，处理不同长度的points和lines"""
    images = torch.stack([item['image'] for item in batch])
    keypoint_heatmaps = torch.stack([item['keypoint_heatmap'] for item in batch])
    line_heatmaps = torch.stack([item['line_heatmap'] for item in batch])

    # 处理不同长度的points和lines
    max_points = max([len(item['points']) for item in batch])
    max_lines = max([len(item['lines']) for item in batch])
    points_padded = []
    lines_padded = []
    for item in batch:
        points = item['points']
        lines = item['lines']
        points_padding = torch.zeros((max_points - len(points), 2), dtype=points.dtype)
        lines_padding = torch.zeros((max_lines - len(lines), 4), dtype=lines.dtype)
        points_padded.append(torch.cat([points, points_padding]))
        lines_padded.append(torch.cat([lines, lines_padding]))
    points = torch.stack(points_padded)
    lines = torch.stack(lines_padded)

    paths = [item['path'] for item in batch]

    return {
        'image': images,
        'keypoint_heatmap': keypoint_heatmaps,
        'line_heatmap': line_heatmaps,
        'points': points,
        'lines': lines,
        'path': paths
    }

def create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=2):
    """创建数据加载器"""
    train_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='train'
    )

    val_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='test'
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    return train_loader, val_loader

# =======================================
# 网络定义部分（修改后的模型）
# =======================================

# 定义优化的残差块（ORB）
class OptimizedResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(OptimizedResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        # Nesterov加速梯度的实现
        self.momentum = 0.9
        self.prev_dx = None

    def forward(self, x):
        dx = self.conv1(x)
        if self.prev_dx is not None:
            dx = dx + self.momentum * self.prev_dx
        out = self.bn1(dx)
        out = self.relu(out)
        self.prev_dx = dx
        return out

# 点感知模块
# 修复后的点感知模块
class PointAwareModule(nn.Module):
    def __init__(self, in_channels):
        super(PointAwareModule, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)  # 1x1 卷积保持尺寸不变
        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)  # 3x3 卷积，padding=1保持尺寸
        self.conv3 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)  # 3x3 卷积，padding=1保持尺寸
        self.bn = nn.BatchNorm2d(in_channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x1 = self.conv1(x)                # 输出尺寸与输入相同
        x2 = self.conv2(x)                # 输出尺寸与输入相同
        x3 = self.conv3(self.conv1(x))    # 输出尺寸与输入相同

        # 现在所有张量尺寸一致，可以安全相加
        out = x + self.bn(x1) + self.bn(x2) + self.bn(x3)
        out = self.relu(out)
        return out

# 线感知模块
class LineAwareModule(nn.Module):
    def __init__(self, in_channels):
        super(LineAwareModule, self).__init__()
        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.softmax = nn.Softmax(dim=-1)
        self.gamma = nn.Parameter(torch.zeros(1))
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        batch_size, C, width, height = x.size()
        proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)  # [B, N, C']
        proj_key = self.key_conv(x).view(batch_size, -1, width * height)  # [B, C', N]
        energy = torch.bmm(proj_query, proj_key)  # [B, N, N]
        attention = self.softmax(energy)
        proj_value = self.value_conv(x).view(batch_size, -1, width * height)  # [B, C, N]
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))  # [B, C, N]
        out = out.view(batch_size, C, width, height)
        out = self.gamma * out + x
        out = self.relu(out)
        return out

# 并行注意力模块
class ParallelAttentionBlock(nn.Module):
    def __init__(self, in_channels):
        super(ParallelAttentionBlock, self).__init__()
        self.edge_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.sigmoid = nn.Sigmoid()
        self.channel_conv1 = nn.Conv2d(in_channels, in_channels // 16, kernel_size=1)
        self.channel_conv2 = nn.Conv2d(in_channels // 16, in_channels, kernel_size=1)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        # 空间注意力
        edge = self.edge_conv(x)
        edge_attention = self.sigmoid(edge)
        edge_out = x * edge_attention + x

        # 通道注意力
        gap = F.adaptive_avg_pool2d(edge_out, (1, 1))
        channel_att = self.relu(self.channel_conv1(gap))
        channel_att = self.sigmoid(self.channel_conv2(channel_att))
        channel_out = edge_out * channel_att + edge_out

        return channel_out

# 主干网络
class MultiTaskNetwork(nn.Module):
    def __init__(self, descriptor_dim=128):
        super(MultiTaskNetwork, self).__init__()
        # 使用轻量级的ResNet18作为编码器
        resnet = resnet18(pretrained=True)
        self.encoder = nn.Sequential(
            resnet.conv1,   # [B, 64, H/2, W/2]
            resnet.bn1,
            resnet.relu,
            resnet.maxpool, # [B, 64, H/4, W/4]
            resnet.layer1,  # [B, 64, H/4, W/4]
            resnet.layer2,  # [B, 128, H/8, W/8]
            resnet.layer3,  # [B, 256, H/16, W/16]
            resnet.layer4   # [B, 512, H/32, W/32]
        )

        # 点线感知的多任务注意力模块（创新点一）
        self.point_aware_module = PointAwareModule(512)
        self.line_aware_module = LineAwareModule(512)
        self.parallel_attention_block = ParallelAttentionBlock(512)

        # 解码器，用于上采样到原始尺寸（256x256）
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  # [B, 256, H/16, W/16]
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # [B, 128, H/8, W/8]
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # [B, 64, H/4, W/4]
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # [B, 32, H/2, W/2]
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),    # [B, 16, H, W]
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
        )

        # 特征点检测头
        self.keypoint_head = nn.Sequential(
            nn.Conv2d(16, 1, kernel_size=1),
            nn.Sigmoid()
        )

        # 特征线检测头
        self.line_head = nn.Sequential(
            nn.Conv2d(16, 1, kernel_size=1),
            nn.Sigmoid()
        )

        # 特征点描述子头
        self.keypoint_descriptor_head = nn.Sequential(
            nn.Conv2d(16, descriptor_dim, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # [B, 128, H*2, W*2]
        )

        # 特征线描述子头
        self.line_descriptor_head = nn.Sequential(
            nn.Conv2d(16, descriptor_dim, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        )

    def forward(self, x):
        features = self.encoder(x)  # [B, 512, H/32, W/32]

        # 点线感知的多任务注意力模块
        point_features = self.point_aware_module(features)
        line_features = self.line_aware_module(features)
        combined_features = self.parallel_attention_block(point_features + line_features)

        decoded_features = self.decoder(combined_features)  # [B, 16, H, W]

        # 特征点检测
        keypoint_output = self.keypoint_head(decoded_features)  # [B, 1, H, W]
        # 特征线检测
        line_output = self.line_head(decoded_features)          # [B, 1, H, W]
        # 特征点描述子
        keypoint_descriptor = self.keypoint_descriptor_head(decoded_features)  # [B, 128, H*2, W*2]
        # 特征线描述子
        line_descriptor = self.line_descriptor_head(decoded_features)          # [B, 128, H*2, W*2]

        return keypoint_output, line_output, keypoint_descriptor, line_descriptor

# 定义知识蒸馏损失函数（创新点二）
class DistillationLoss(nn.Module):
    def __init__(self, temperature=4):
        super(DistillationLoss, self).__init__()
        self.temperature = temperature

    def forward(self, student_output, teacher_output):
        loss = F.kl_div(
            F.log_softmax(student_output / self.temperature, dim=1),
            F.softmax(teacher_output / self.temperature, dim=1),
            reduction='batchmean'
        ) * (self.temperature ** 2)
        return loss

# =======================================
# 训练和评估部分
# =======================================

def train_model():
    # 创建数据加载器
    train_loader, val_loader = create_dataloader(
        v1_1_path = "/content/drive/MyDrive/v1.1", # 替换为您的真实图像路径
        pointlines_path = "/content/drive/MyDrive/pointlines",   # 替换为实际路径
        batch_size=8,
        num_workers=2
    )

    # 初始化模型和损失函数
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 定义教师模型和学生模型
    teacher_model = MultiTaskNetwork().to(device)
    student_model = MultiTaskNetwork().to(device)

    # 加载教师模型的预训练权重（假设已经训练好）
    # teacher_model.load_state_dict(torch.load("teacher_model.pth"))

    # 定义优化器
    optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-4)
    num_epochs = 10

    # 定义损失函数
    mse_loss = nn.MSELoss()
    distillation_loss_fn = DistillationLoss()

    for epoch in range(num_epochs):
        student_model.train()
        teacher_model.eval()
        running_loss = 0.0

        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            images = batch['image'].to(device)
            keypoint_targets = batch['keypoint_heatmap'].to(device)
            line_targets = batch['line_heatmap'].to(device)

            optimizer.zero_grad()

            # 教师模型的输出
            with torch.no_grad():
                teacher_outputs = teacher_model(images)
                teacher_keypoint_output, teacher_line_output, _, _ = teacher_outputs

            # 学生模型的输出
            student_outputs = student_model(images)
            student_keypoint_output, student_line_output, _, _ = student_outputs

            # 计算损失
            loss_keypoint = mse_loss(student_keypoint_output, keypoint_targets)
            loss_line = mse_loss(student_line_output, line_targets)

            # 知识蒸馏损失
            loss_distillation_keypoint = distillation_loss_fn(student_keypoint_output, teacher_keypoint_output)
            loss_distillation_line = distillation_loss_fn(student_line_output, teacher_line_output)

            # 总损失
            loss = loss_keypoint + loss_line + loss_distillation_keypoint + loss_distillation_line
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        avg_loss = running_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

        # 每个epoch后保存模型
        torch.save(student_model.state_dict(), f"student_model_epoch_{epoch+1}.pth")

def calculate_sap(pred_points, gt_points, threshold=3.0):
    """计算结构感知性能(SAP)"""
    if len(pred_points) == 0 or len(gt_points) == 0:
        return 0.0

    distances = torch.cdist(pred_points.float(), gt_points.float())
    min_distances, _ = torch.min(distances, dim=1)
    correct_matches = (min_distances < threshold).float()

    precision = correct_matches.sum() / len(pred_points)
    recall = correct_matches.sum() / len(gt_points)
    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)

    return f1.item()

def calculate_lap(pred_lines, gt_lines, dist_threshold=5.0, angle_threshold=10.0):
    """计算线段感知性能(LAP)"""
    if len(pred_lines) == 0 or len(gt_lines) == 0:
        return 0.0

    def compute_line_similarity(line1, line2):
        dist1 = torch.sqrt(((line1[:2] - line2[:2])**2).sum())
        dist2 = torch.sqrt(((line1[2:] - line2[2:])**2).sum())

        vec1 = line1[2:] - line1[:2]
        vec2 = line2[2:] - line2[:2]
        cos_angle = (vec1 * vec2).sum() / (torch.norm(vec1) * torch.norm(vec2) + 1e-8)
        angle = torch.acos(torch.clamp(cos_angle, -1, 1)) * 180 / np.pi

        return (dist1 + dist2) / 2, angle

    matches = 0
    for pred_line in pred_lines:
        best_dist = float('inf')
        best_angle = float('inf')

        for gt_line in gt_lines:
            dist, angle = compute_line_similarity(pred_line, gt_line)
            if dist < best_dist:
                best_dist = dist
                best_angle = angle

        if best_dist < dist_threshold and best_angle < angle_threshold:
            matches += 1

    precision = matches / len(pred_lines) if len(pred_lines) > 0 else 0
    recall = matches / len(gt_lines) if len(gt_lines) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)

    return f1

def evaluate_model():
    """评估模型性能"""
    _, val_loader = create_dataloader(
        v1_1_path="/content/drive/MyDrive/v1.1",
        pointlines_path="/content/drive/MyDrive/pointlines",
        batch_size=1,
        num_workers=2
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = MultiTaskNetwork().to(device)

    try:
        model.load_state_dict(torch.load("student_model_epoch_10.pth", weights_only=True))
        print("成功加载模型权重")
    except Exception as e:
        print(f"加载模型出错: {str(e)}")
        return

    model.eval()

    # 评估指标
    sap_scores = []
    lap_scores = []
    fps_times = []

    print("开始评估...")

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(val_loader)):
            images = batch['image'].to(device)
            gt_points = batch['points'][0]
            gt_lines = batch['lines'][0]

            # 测量FPS
            start_time = time.time()
            outputs = model(images)
            inference_time = time.time() - start_time
            fps = 1.0 / inference_time
            fps_times.append(fps)

            keypoint_output, line_output, keypoint_desc, line_desc = outputs

            # 从热图中提取关键点
            keypoint_map = keypoint_output.cpu().squeeze().numpy()
            points_y, points_x = np.where(keypoint_map > 0.5)
            pred_points = torch.tensor(np.stack([points_x, points_y], axis=1))

            # 从热图中提取线段
            line_map = line_output.cpu().squeeze().numpy()
            line_map = (line_map > 0.5).astype(np.uint8) * 255
            lines = cv2.HoughLinesP(line_map, 1, np.pi/180,
                                  threshold=50,
                                  minLineLength=20,
                                  maxLineGap=10)

            if lines is not None:
                pred_lines = torch.tensor(lines[:, 0, :])
            else:
                pred_lines = torch.zeros((0, 4))

            # 计算SAP和LAP
            sap = calculate_sap(pred_points, gt_points)
            lap = calculate_lap(pred_lines, gt_lines)

            sap_scores.append(sap)
            lap_scores.append(lap)

            # 可视化部分评估结果
            if batch_idx % 50 == 0:
                image = images.cpu().squeeze().permute(1, 2, 0).numpy()
                keypoint_heatmap = keypoint_output.cpu().squeeze().numpy()
                line_heatmap = line_output.cpu().squeeze().numpy()

                plt.figure(figsize=(15, 5))

                plt.subplot(131)
                plt.imshow(image)
                plt.title('Input Image')
                plt.axis('off')

                plt.subplot(132)
                plt.imshow(keypoint_heatmap, cmap='jet')
                plt.plot(pred_points[:,0], pred_points[:,1], 'r.')
                plt.title(f'Keypoints (num={len(pred_points)})')
                plt.axis('off')

                plt.subplot(133)
                plt.imshow(line_heatmap, cmap='jet')
                for line in pred_lines:
                    x1,y1,x2,y2 = line
                    plt.plot([x1,x2], [y1,y2], 'r-', linewidth=1)
                plt.title(f'Lines (num={len(pred_lines)})')
                plt.axis('off')

                plt.tight_layout()
                plt.show()

    # 计算平均指标
    mean_sap = np.mean(sap_scores)
    mean_lap = np.mean(lap_scores)
    mean_fps = np.mean(fps_times)

    print("\n评估结果:")
    print(f"SAP (Structure-Aware Performance): {mean_sap:.4f}")
    print(f"LAP (Line-Aware Performance): {mean_lap:.4f}")
    print(f"Average FPS: {mean_fps:.2f}")

if __name__ == "__main__":
    print("开始训练...")
    train_model()
    print("\n开始评估...")
    evaluate_model()
    # 评估模型

"""# **线特征提取网络：**"""

import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision.models import mobilenet_v2
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2
import pickle
import matplotlib.pyplot as plt
from tqdm import tqdm
import time

# =======================================
# 数据预处理部分
# =======================================

class WireframeDataset(Dataset):
    def __init__(self, v1_1_path, pointlines_path, split='train', img_size=256):
        self.img_size = img_size
        self.v1_1_path = Path(v1_1_path)
        self.pointlines_path = Path(pointlines_path)
        self.split = split

        # 获取图像列表
        self.img_dir = self.v1_1_path / split
        self.img_paths = list(self.img_dir.glob('*.jpg'))

        # 定义数据增强
        self.transform = A.Compose([
            A.Resize(img_size, img_size),
            A.OneOf([
                A.RandomBrightnessContrast(),
                A.RandomGamma(),
                A.HueSaturationValue(),
            ], p=0.3),
            A.OneOf([
                A.GaussNoise(),
                A.GaussianBlur(),
                A.MotionBlur(),
            ], p=0.2),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            ),
            ToTensorV2()
        ])

        print(f"Found {len(self.img_paths)} images in {split} set")

    def generate_heatmap(self, points, shape):
        """生成高斯热图"""
        heatmap = np.zeros(shape, dtype=np.float32)
        if len(points) == 0:
            return heatmap

        for (x, y) in points:
            if not (0 <= x < shape[1] and 0 <= y < shape[0]):
                continue

            # 生成高斯核
            sigma = 2
            size = int(6 * sigma + 1)
            x0 = np.arange(0, size, 1, float)
            y0 = x0[:, np.newaxis]
            x0 = x0 - size // 2
            y0 = y0 - size // 2
            gaussian = np.exp(- (x0 ** 2 + y0 ** 2) / (2 * sigma ** 2))

            # 计算高斯核的范围
            x, y = int(x), int(y)
            x1, y1 = max(0, x - size // 2), max(0, y - size // 2)
            x2, y2 = min(shape[1], x + size // 2 + 1), min(shape[0], y + size // 2 + 1)
            gx1, gy1 = max(0, size // 2 - x), max(0, size // 2 - y)
            gx2, gy2 = gx1 + (x2 - x1), gy1 + (y2 - y1)

            # 更新热图
            heatmap[y1:y2, x1:x2] = np.maximum(
                heatmap[y1:y2, x1:x2],
                gaussian[gy1:gy2, gx1:gx2]
            )

        return heatmap

    def augment_with_sol(self, lines):
        augmented_lines = []
        for line in lines:
            x1, y1, x2, y2 = line
            # 随机决定是否对线段进行切分
            if np.random.rand() < 0.5:
                augmented_lines.append([x1, y1, x2, y2])  # 保持原线段
            else:
                # 切分线段
                num_segments = np.random.randint(2, 5)  # 随机切分成 2 到 4 段
                for i in range(num_segments):
                    ratio1 = i / num_segments
                    ratio2 = (i + 1) / num_segments
                    nx1 = x1 + (x2 - x1) * ratio1
                    ny1 = y1 + (y2 - y1) * ratio1
                    nx2 = x1 + (x2 - x1) * ratio2
                    ny2 = y1 + (y2 - y1) * ratio2
                    augmented_lines.append([nx1, ny1, nx2, ny2])
        return np.array(augmented_lines)

    def __getitem__(self, idx):
        # 读取图像
        img_path = str(self.img_paths[idx])
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w = image.shape[:2]

        # 获取对应的pkl文件
        img_name = self.img_paths[idx].stem
        pkl_path = self.pointlines_path / f"{img_name}.pkl"

        # 读取标注数据
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)

        # 获取点和线的坐标
        points = np.array(data['points'])  # [N, 2]
        lines = np.array(data['lines'])    # [M, 2]，表示点的索引

        # 将lines从点的索引转换为坐标
        if lines.size > 0:
            lines_coords = []
            for line in lines:
                idx1, idx2 = line.astype(int)
                x1, y1 = points[idx1]
                x2, y2 = points[idx2]
                lines_coords.append([x1, y1, x2, y2])
            lines = np.array(lines_coords)
        else:
            lines = np.zeros((0, 4))  # 没有线的情况

        # 调整坐标到目标尺寸
        scale_x = self.img_size / w
        scale_y = self.img_size / h
        if lines.size > 0:
            lines[:, [0, 2]] *= scale_x
            lines[:, [1, 3]] *= scale_y

        # SoL 数据增强
        lines = self.augment_with_sol(lines)

        # 生成中心点和位移向量
        center_points = (lines[:, :2] + lines[:, 2:]) / 2  # 线段中心点
        offsets = lines[:, 2:] - lines[:, :2]  # 位移向量

        # 生成中心点热图
        center_heatmap = self.generate_heatmap(center_points, (self.img_size, self.img_size))

        # 生成位移向量图
        offset_map = np.zeros((self.img_size, self.img_size, 2), dtype=np.float32)
        for idx, (cx, cy) in enumerate(center_points):
            x, y = int(cx), int(cy)
            if 0 <= x < self.img_size and 0 <= y < self.img_size:
                offset_map[y, x] = offsets[idx]

        # 应用数据增强
        augmented = self.transform(image=image)
        image = augmented['image']

        return {
            'image': image,
            'center_heatmap': torch.from_numpy(center_heatmap)[None],
            'offset_map': torch.from_numpy(offset_map).permute(2, 0, 1),  # [2, H, W]
            'lines': torch.from_numpy(lines.astype(np.float32)),
            'path': img_path
        }

    def __len__(self):
        return len(self.img_paths)

def custom_collate_fn(batch):
    images = torch.stack([item['image'] for item in batch])
    center_heatmaps = torch.stack([item['center_heatmap'] for item in batch])
    offset_maps = torch.stack([item['offset_map'] for item in batch])

    # 处理不同长度的lines
    max_lines = max([len(item['lines']) for item in batch])
    lines_padded = []
    for item in batch:
        lines = item['lines']
        lines_padding = torch.zeros((max_lines - len(lines), 4), dtype=lines.dtype)
        lines_padded.append(torch.cat([lines, lines_padding]))
    lines = torch.stack(lines_padded)

    paths = [item['path'] for item in batch]

    return {
        'image': images,
        'center_heatmap': center_heatmaps,
        'offset_map': offset_maps,
        'lines': lines,
        'path': paths
    }

def create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=2):
    """创建数据加载器"""
    train_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='train'
    )

    val_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='test'
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    return train_loader, val_loader

# =======================================
# 模型定义部分
# =======================================

# 编码器
class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        mobilenet = mobilenet_v2(pretrained=True)
        # 使用到 features[13]，输出通道数为 96
        self.features = nn.Sequential(*mobilenet.features[:14])

    def forward(self, x):
        x = self.features(x)
        return x  # 输出尺寸 [B, 96, H/16, W/16]

# Block Type A
class BlockA(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(BlockA, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1)
        self.relu2 = nn.ReLU(inplace=True)
        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.relu3 = nn.ReLU(inplace=True)
        self.conv4 = nn.Conv2d(out_channels, out_channels, kernel_size=1)
        self.relu4 = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.relu1(self.conv1(x))
        x = self.relu2(self.conv2(x))
        x = self.relu3(self.conv3(x))
        x = self.relu4(self.conv4(x))
        return x

# Block Type B
class BlockB(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(BlockB, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.relu(self.conv(x))
        return x

# Block Type C
class BlockC(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(BlockC, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.relu(self.conv(x))
        return x

# 解码器
class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()
        self.blockA1 = BlockA(96, 64)
        self.blockB1 = BlockB(64, 64)
        self.blockC1 = BlockC(64, 64)
        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)

        self.blockA2 = BlockA(64, 32)
        self.blockB2 = BlockB(32, 32)
        self.blockC2 = BlockC(32, 32)
        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)

        self.blockA3 = BlockA(32, 16)
        self.blockB3 = BlockB(16, 16)
        self.blockC3 = BlockC(16, 16)
        self.upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)

        self.blockA4 = BlockA(16, 16)
        self.blockB4 = BlockB(16, 16)
        self.blockC4 = BlockC(16, 16)
        self.upsample4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)

    def forward(self, x):
        x = self.blockA1(x)
        x = self.blockB1(x)
        x = self.blockC1(x)
        x = self.upsample1(x)

        x = self.blockA2(x)
        x = self.blockB2(x)
        x = self.blockC2(x)
        x = self.upsample2(x)

        x = self.blockA3(x)
        x = self.blockB3(x)
        x = self.blockC3(x)
        x = self.upsample3(x)

        x = self.blockA4(x)
        x = self.blockB4(x)
        x = self.blockC4(x)
        x = self.upsample4(x)

        return x  # 输出尺寸 [B, 16, H, W]

# 主干网络
class LineFeatureNetwork(nn.Module):
    def __init__(self):
        super(LineFeatureNetwork, self).__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()

        # 中心点检测头
        self.center_head = nn.Sequential(
            nn.Conv2d(16, 1, kernel_size=1),
            nn.Sigmoid()
        )

        # 位移向量预测头
        self.offset_head = nn.Conv2d(16, 2, kernel_size=1)

    def forward(self, x):
        features = self.encoder(x)
        decoded_features = self.decoder(features)
        center_output = self.center_head(decoded_features)
        offset_output = self.offset_head(decoded_features)
        return center_output, offset_output

# =======================================
# 训练和评估部分
# =======================================

def train_model():
    # 创建数据加载器
    train_loader, val_loader = create_dataloader(
        v1_1_path = "/content/drive/MyDrive/v1.1", # 真实图像路径
        pointlines_path = "/content/drive/MyDrive/pointlines",
        batch_size=8,
        num_workers=2
    )

    # 初始化模型和损失函数
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = LineFeatureNetwork().to(device)

    # 定义优化器
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    num_epochs = 30  # 训练30次

    # 定义损失函数
    mse_loss = nn.MSELoss()
    smooth_l1_loss = nn.SmoothL1Loss()

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            images = batch['image'].to(device)
            center_targets = batch['center_heatmap'].to(device)
            offset_targets = batch['offset_map'].to(device)

            optimizer.zero_grad()

            # 模型的输出
            center_output, offset_output = model(images)

            # 计算损失
            loss_center = mse_loss(center_output, center_targets)
            loss_offset = smooth_l1_loss(offset_output, offset_targets)

            loss = loss_center + loss_offset

            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        avg_loss = running_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

        # 每个epoch后保存模型
        torch.save(model.state_dict(), f"line_model_epoch_{epoch+1}.pth")

def calculate_sap(pred_lines, gt_lines, threshold=5.0):
    """计算结构感知性能(SAP)"""
    if len(pred_lines) == 0 or len(gt_lines) == 0:
        return 0.0

    matches = 0
    for pred_line in pred_lines:
        min_dist = float('inf')
        for gt_line in gt_lines:
            dist = torch.norm(pred_line - gt_line)
            if dist < min_dist:
                min_dist = dist
        if min_dist < threshold:
            matches += 1

    precision = matches / len(pred_lines) if len(pred_lines) > 0 else 0
    recall = matches / len(gt_lines) if len(gt_lines) > 0 else 0
    sap = 2 * (precision * recall) / (precision + recall + 1e-8)

    return sap

def calculate_lap(pred_lines, gt_lines, dist_threshold=5.0, angle_threshold=10.0):
    """计算线段感知性能(LAP)"""
    if len(pred_lines) == 0 or len(gt_lines) == 0:
        return 0.0

    def compute_line_similarity(line1, line2):
        # 计算线段的中点距离
        mid1 = (line1[:2] + line1[2:]) / 2
        mid2 = (line2[:2] + line2[2:]) / 2
        dist = torch.norm(mid1 - mid2)

        # 计算线段的角度差异
        vec1 = line1[2:] - line1[:2]
        vec2 = line2[2:] - line2[:2]
        cos_angle = (vec1 * vec2).sum() / (torch.norm(vec1) * torch.norm(vec2) + 1e-8)
        angle = torch.acos(torch.clamp(cos_angle, -1, 1)) * 180 / np.pi

        return dist, angle

    matches = 0
    for pred_line in pred_lines:
        best_dist = float('inf')
        best_angle = float('inf')

        for gt_line in gt_lines:
            dist, angle = compute_line_similarity(pred_line, gt_line)
            if dist < best_dist:
                best_dist = dist
                best_angle = angle

        if best_dist < dist_threshold and best_angle < angle_threshold:
            matches += 1

    precision = matches / len(pred_lines) if len(pred_lines) > 0 else 0
    recall = matches / len(gt_lines) if len(gt_lines) > 0 else 0
    lap = 2 * (precision * recall) / (precision + recall + 1e-8)

    return lap

def evaluate_model():
    """评估模型性能"""
    _, val_loader = create_dataloader(
        v1_1_path="/content/drive/MyDrive/v1.1",
        pointlines_path="/content/drive/MyDrive/pointlines",
        batch_size=1,
        num_workers=2
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = LineFeatureNetwork().to(device)

    try:
        model.load_state_dict(torch.load("line_model_epoch_30.pth"))
        print("成功加载模型权重")
    except Exception as e:
        print(f"加载模型出错: {str(e)}")
        return

    model.eval()

    # 评估指标
    sap_scores = []
    lap_scores = []
    fps_times = []

    print("开始评估...")

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(val_loader)):
            images = batch['image'].to(device)
            gt_lines = batch['lines'][0]

            # 测量FPS
            start_time = time.time()
            center_output, offset_output = model(images)
            inference_time = time.time() - start_time
            fps = 1.0 / inference_time
            fps_times.append(fps)

            # 从中心点热图中提取中心点
            center_map = center_output.cpu().squeeze().numpy()
            centers_y, centers_x = np.where(center_map > 0.5)
            pred_centers = np.stack([centers_x, centers_y], axis=1)

            # 获取对应的位移向量
            offset_map = offset_output.cpu().squeeze().numpy()
            if len(offset_map.shape) == 3:
                pred_offsets = offset_map[:, centers_y, centers_x].T  # [N, 2]
            else:
                pred_offsets = offset_map[:, centers_y, centers_x].reshape(-1, 2)  # [N, 2]

            # 计算预测的线段
            pred_lines = []
            for i in range(len(pred_centers)):
                cx, cy = pred_centers[i]
                dx, dy = pred_offsets[i]
                x1 = cx - dx / 2
                y1 = cy - dy / 2
                x2 = cx + dx / 2
                y2 = cy + dy / 2
                pred_lines.append([x1, y1, x2, y2])

            pred_lines = torch.tensor(pred_lines)

            # 计算SAP和LAP
            sap = calculate_sap(pred_lines, gt_lines)
            lap = calculate_lap(pred_lines, gt_lines)

            sap_scores.append(sap)
            lap_scores.append(lap)

            # 可视化部分评估结果
            if batch_idx % 50 == 0:
                image = images.cpu().squeeze().permute(1, 2, 0).numpy()
                center_heatmap = center_output.cpu().squeeze().numpy()

                plt.figure(figsize=(10, 5))

                plt.subplot(121)
                plt.imshow(image)
                plt.title('Input Image')
                plt.axis('off')

                plt.subplot(122)
                plt.imshow(center_heatmap, cmap='jet')
                for line in pred_lines:
                    x1, y1, x2, y2 = line
                    plt.plot([x1, x2], [y1, y2], 'r-', linewidth=1)
                plt.title(f'Predicted Lines (num={len(pred_lines)})')
                plt.axis('off')

                plt.tight_layout()
                plt.show()

        # 计算平均指标
        mean_sap = np.mean(sap_scores)
        mean_lap = np.mean(lap_scores)
        mean_fps = np.mean(fps_times)

        print("\n评估结果:")
        print(f"SAP (Structure-Aware Performance): {mean_sap:.4f}")
        print(f"LAP (Line-Aware Performance): {mean_lap:.4f}")
        print(f"Average FPS: {mean_fps:.2f}")

if __name__ == "__main__":
    print("开始训练...")
    train_model()
    print("\n开始评估...")
    evaluate_model()

import os
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2
import pickle
import matplotlib.pyplot as plt
import torch.nn as nn
from torchvision.models import mobilenet_v2
from tqdm import tqdm
import time

# =======================================
# 数据预处理部分
# =======================================

class WireframeDataset(Dataset):
    def __init__(self, v1_1_path, pointlines_path, split='train', img_size=256):
        self.img_size = img_size
        self.v1_1_path = Path(v1_1_path)
        self.pointlines_path = Path(pointlines_path)
        self.split = split

        # 获取图像列表
        self.img_dir = self.v1_1_path / split
        self.img_paths = list(self.img_dir.glob('*.jpg'))

        # 定义数据增强
        self.transform = A.Compose([
            A.Resize(img_size, img_size),
            A.OneOf([
                A.RandomBrightnessContrast(),
                A.RandomGamma(),
                A.HueSaturationValue(),
            ], p=0.3),
            A.OneOf([
                A.GaussNoise(),
                A.GaussianBlur(),
                A.MotionBlur(),
            ], p=0.2),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            ),
            ToTensorV2()
        ])

        print(f"Found {len(self.img_paths)} images in {split} set")

    def generate_heatmap(self, points, shape):
        """生成高斯热图"""
        heatmap = np.zeros(shape, dtype=np.float32)
        if len(points) == 0:
            return heatmap

        for (x, y) in points:
            if not (0 <= x < shape[1] and 0 <= y < shape[0]):
                continue

            # 生成高斯核
            sigma = 2
            size = int(6 * sigma + 1)
            x0 = np.arange(0, size, 1, float)
            y0 = x0[:, np.newaxis]
            x0 = x0 - size // 2
            y0 = y0 - size // 2
            gaussian = np.exp(- (x0 ** 2 + y0 ** 2) / (2 * sigma ** 2))

            # 计算高斯核的范围
            x, y = int(x), int(y)
            x1, y1 = max(0, x - size // 2), max(0, y - size // 2)
            x2, y2 = min(shape[1], x + size // 2 + 1), min(shape[0], y + size // 2 + 1)
            gx1, gy1 = max(0, size // 2 - x), max(0, size // 2 - y)
            gx2, gy2 = gx1 + (x2 - x1), gy1 + (y2 - y1)

            # 更新热图
            heatmap[y1:y2, x1:x2] = np.maximum(
                heatmap[y1:y2, x1:x2],
                gaussian[gy1:gy2, gx1:gx2]
            )

        return heatmap

    def augment_with_sol(self, lines):
        """Segments of Line segment 数据增强"""
        augmented_lines = []
        for line in lines:
            x1, y1, x2, y2 = line
            # 随机决定是否对线段进行切分
            if np.random.rand() < 0.5:
                augmented_lines.append([x1, y1, x2, y2])  # 保持原线段
            else:
                # 切分线段
                num_segments = np.random.randint(2, 5)  # 随机切分成 2 到 4 段
                for i in range(num_segments):
                    ratio1 = i / num_segments
                    ratio2 = (i + 1) / num_segments
                    nx1 = x1 + (x2 - x1) * ratio1
                    ny1 = y1 + (y2 - y1) * ratio1
                    nx2 = x1 + (x2 - x1) * ratio2
                    ny2 = y1 + (y2 - y1) * ratio2
                    augmented_lines.append([nx1, ny1, nx2, ny2])
        return np.array(augmented_lines)

    def __getitem__(self, idx):
        # 读取图像
        img_path = str(self.img_paths[idx])
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w = image.shape[:2]

        # 获取对应的pkl文件
        img_name = self.img_paths[idx].stem
        pkl_path = self.pointlines_path / f"{img_name}.pkl"

        # 读取标注数据
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)

        # 获取点和线的坐标
        points = np.array(data['points'])  # [N, 2]
        lines = np.array(data['lines'])    # [M, 2]，表示点的索引

        # 将lines从点的索引转换为坐标
        if lines.size > 0:
            lines_coords = []
            for line in lines:
                idx1, idx2 = line.astype(int)
                x1, y1 = points[idx1]
                x2, y2 = points[idx2]
                lines_coords.append([x1, y1, x2, y2])
            lines = np.array(lines_coords)
        else:
            lines = np.zeros((0, 4))  # 没有线的情况

        # 调整坐标到目标尺寸
        scale_x = self.img_size / w
        scale_y = self.img_size / h
        if lines.size > 0:
            lines[:, [0, 2]] *= scale_x
            lines[:, [1, 3]] *= scale_y

        # SoL 数据增强
        lines = self.augment_with_sol(lines)

        # 生成中心点和位移向量
        if len(lines) > 0:
            centers = (lines[:, :2] + lines[:, 2:]) / 2  # 线段中心点
            displacements = lines[:, 2:] - lines[:, :2]  # 位移向量
            lengths = np.linalg.norm(displacements, axis=1, keepdims=True)  # 线段长度
            angles = np.arctan2(displacements[:, 1], displacements[:, 0]).reshape(-1, 1)  # 线段角度
        else:
            centers = np.zeros((0, 2), dtype=np.float32)
            displacements = np.zeros((0, 2), dtype=np.float32)
            lengths = np.zeros((0, 1), dtype=np.float32)
            angles = np.zeros((0, 1), dtype=np.float32)

        # 生成中心点热图
        center_heatmap = self.generate_heatmap(centers, (self.img_size, self.img_size))

        # 初始化位移、长度、角度图
        offset_map = np.zeros((self.img_size, self.img_size, 2), dtype=np.float32)
        length_map = np.zeros((self.img_size, self.img_size, 1), dtype=np.float32)
        angle_map = np.zeros((self.img_size, self.img_size, 1), dtype=np.float32)

        # 确保至少有一个中心点
        if len(centers) > 0:
            for idx, (cx, cy) in enumerate(centers):
                x, y = int(cx), int(cy)
                if 0 <= x < self.img_size and 0 <= y < self.img_size:
                    offset_map[y, x] = displacements[idx]
                    length_map[y, x] = lengths[idx]
                    angle_map[y, x] = angles[idx]

        # 应用数据增强
        augmented = self.transform(image=image)
        image = augmented['image']

        return {
            'image': image,
            'center_heatmap': torch.from_numpy(center_heatmap)[None],
            'offset_map': torch.from_numpy(offset_map).permute(2, 0, 1),  # [2, H, W]
            'length_map': torch.from_numpy(length_map).permute(2, 0, 1),  # [1, H, W]
            'angle_map': torch.from_numpy(angle_map).permute(2, 0, 1),    # [1, H, W]
            'lines': torch.from_numpy(lines.astype(np.float32)),
            'path': img_path
        }

    def __len__(self):
        return len(self.img_paths)

def custom_collate_fn(batch):
    images = torch.stack([item['image'] for item in batch])
    center_heatmaps = torch.stack([item['center_heatmap'] for item in batch])
    offset_maps = torch.stack([item['offset_map'] for item in batch])
    length_maps = torch.stack([item['length_map'] for item in batch])
    angle_maps = torch.stack([item['angle_map'] for item in batch])

    # 处理不同长度的lines
    max_lines = max([len(item['lines']) for item in batch])
    lines_padded = []
    for item in batch:
        lines = item['lines']
        lines_padding = torch.zeros((max_lines - len(lines), 4), dtype=lines.dtype)
        lines_padded.append(torch.cat([lines, lines_padding]))
    lines = torch.stack(lines_padded)

    paths = [item['path'] for item in batch]

    return {
        'image': images,
        'center_heatmap': center_heatmaps,
        'offset_map': offset_maps,
        'length_map': length_maps,
        'angle_map': angle_maps,
        'lines': lines,
        'path': paths
    }

def create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=2):
    """创建数据加载器"""
    train_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='train'
    )

    val_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='test'
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    return train_loader, val_loader

# =======================================
# 模型定义部分
# =======================================

# LFEM 模块
class LFEM(nn.Module):
    def __init__(self, in_channels):
        super(LFEM, self).__init__()
        self.conv1x1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.conv3x3 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)
        self.conv_dilated = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=2, dilation=2)
        self.bn = nn.BatchNorm2d(in_channels)
        self.relu = nn.ReLU(inplace=True)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x1 = self.conv1x1(x)
        x2 = self.conv3x3(x)
        x3 = self.conv_dilated(x)
        x_combined = x1 + x2 + x3
        x_attention = self.sigmoid(self.bn(x_combined))
        out = x * x_attention
        out = self.relu(out)
        return out

# 编码器
class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        mobilenet = mobilenet_v2(pretrained=True)
        # 使用到 features[13]，输出通道数为 96
        self.features = nn.Sequential(*mobilenet.features[:14])
        self.lfem = LFEM(96)

    def forward(self, x):
        x = self.features(x)
        x = self.lfem(x)
        return x  # 输出尺寸 [B, 96, H/16, W/16]

# Block Type A
class BlockA(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(BlockA, self).__init__()
        self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.conv3x3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        identity = self.skip(x)
        out = self.relu(self.conv1x1(x))
        out = self.conv3x3(out)
        out += identity
        out = self.relu(out)
        return out

# Block Type B
class BlockB(nn.Module):
    def __init__(self, channels):
        super(BlockB, self).__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        identity = x
        out = self.relu(self.conv1(x))
        out = self.conv2(out)
        out += identity
        out = self.relu(out)
        return out

# Block Type C
class BlockC(nn.Module):
    def __init__(self, in_channels, out_channels, dilation):
        super(BlockC, self).__init__()
        self.conv_dilated = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilation, dilation=dilation)
        self.conv1x1 = nn.Conv2d(out_channels, out_channels, kernel_size=1)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        out = self.relu(self.conv_dilated(x))
        out = self.conv1x1(out)
        out = self.relu(out)
        return out

# 解码器
class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()
        self.blockA1 = BlockA(96, 64)
        self.blockB1 = BlockB(64)
        self.blockC1 = BlockC(64, 64, dilation=2)
        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)

        self.blockA2 = BlockA(64, 32)
        self.blockB2 = BlockB(32)
        self.blockC2 = BlockC(32, 32, dilation=2)
        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)

        self.blockA3 = BlockA(32, 16)
        self.blockB3 = BlockB(16)
        self.blockC3 = BlockC(16, 16, dilation=2)
        self.upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)

        self.blockA4 = BlockA(16, 16)
        self.blockB4 = BlockB(16)
        self.blockC4 = BlockC(16, 16, dilation=1)
        self.upsample4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)

    def forward(self, x):
        x = self.blockA1(x)
        x = self.blockB1(x)
        x = self.blockC1(x)
        x = self.upsample1(x)

        x = self.blockA2(x)
        x = self.blockB2(x)
        x = self.blockC2(x)
        x = self.upsample2(x)

        x = self.blockA3(x)
        x = self.blockB3(x)
        x = self.blockC3(x)
        x = self.upsample3(x)

        x = self.blockA4(x)
        x = self.blockB4(x)
        x = self.blockC4(x)
        x = self.upsample4(x)

        return x  # 输出尺寸 [B, 16, H, W]

# 主干网络
class LineFeatureNetwork(nn.Module):
    def __init__(self):
        super(LineFeatureNetwork, self).__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()

        # 中心点检测头
        self.center_head = nn.Sequential(
            nn.Conv2d(16, 1, kernel_size=1),
            nn.Sigmoid()
        )

        # 位移向量预测头
        self.offset_head = nn.Conv2d(16, 2, kernel_size=1)

        # 长度预测头
        self.length_head = nn.Conv2d(16, 1, kernel_size=1)

        # 角度预测头
        self.angle_head = nn.Conv2d(16, 1, kernel_size=1)

    def forward(self, x):
        features = self.encoder(x)
        decoded_features = self.decoder(features)
        center_output = self.center_head(decoded_features)
        offset_output = self.offset_head(decoded_features)
        length_output = self.length_head(decoded_features)
        angle_output = self.angle_head(decoded_features)
        return center_output, offset_output, length_output, angle_output

# =======================================
# 训练和评估部分
# =======================================

def train_model():
    # 创建数据加载器
    train_loader, val_loader = create_dataloader(
        v1_1_path = "/content/drive/MyDrive/v1.1", # 真实图像路径
        pointlines_path = "/content/drive/MyDrive/pointlines",
        batch_size=8,
        num_workers=2
    )

    # 初始化模型和损失函数
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = LineFeatureNetwork().to(device)

    # 定义优化器
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    num_epochs = 30  # 训练30次

    # 定义损失函数
    mse_loss = nn.MSELoss()
    smooth_l1_loss = nn.SmoothL1Loss()

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            images = batch['image'].to(device)
            center_targets = batch['center_heatmap'].to(device)
            offset_targets = batch['offset_map'].to(device)
            length_targets = batch['length_map'].to(device)
            angle_targets = batch['angle_map'].to(device)

            optimizer.zero_grad()

            # 模型的输出
            center_output, offset_output, length_output, angle_output = model(images)

            # 计算损失
            loss_center = mse_loss(center_output, center_targets)
            loss_offset = smooth_l1_loss(offset_output, offset_targets)
            loss_length = smooth_l1_loss(length_output, length_targets)
            loss_angle = smooth_l1_loss(angle_output, angle_targets)

            loss = loss_center + loss_offset + loss_length + loss_angle

            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        avg_loss = running_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

        # 每个epoch后保存模型
        torch.save(model.state_dict(), f"line_model_epoch_{epoch+1}.pth")

def calculate_sap(pred_lines, gt_lines, threshold=5.0):
    """计算结构感知性能(SAP)"""
    if len(pred_lines) == 0 or len(gt_lines) == 0:
        return 0.0

    matches = 0
    for pred_line in pred_lines:
        min_dist = float('inf')
        for gt_line in gt_lines:
            dist = torch.norm(pred_line - gt_line)
            if dist < min_dist:
                min_dist = dist
        if min_dist < threshold:
            matches += 1

    precision = matches / len(pred_lines) if len(pred_lines) > 0 else 0
    recall = matches / len(gt_lines) if len(gt_lines) > 0 else 0
    sap = 2 * (precision * recall) / (precision + recall + 1e-8)

    return sap

def calculate_lap(pred_lines, gt_lines, dist_threshold=5.0, angle_threshold=10.0):
    """计算线段感知性能(LAP)"""
    if len(pred_lines) == 0 or len(gt_lines) == 0:
        return 0.0

    def compute_line_similarity(line1, line2):
        # 计算线段的中点距离
        mid1 = (line1[:2] + line1[2:]) / 2
        mid2 = (line2[:2] + line2[2:]) / 2
        dist = torch.norm(mid1 - mid2)

        # 计算线段的角度差异
        vec1 = line1[2:] - line1[:2]
        vec2 = line2[2:] - line2[:2]
        cos_angle = (vec1 * vec2).sum() / (torch.norm(vec1) * torch.norm(vec2) + 1e-8)
        angle = torch.acos(torch.clamp(cos_angle, -1, 1)) * 180 / np.pi

        return dist, angle

    matches = 0
    for pred_line in pred_lines:
        best_dist = float('inf')
        best_angle = float('inf')

        for gt_line in gt_lines:
            dist, angle = compute_line_similarity(pred_line, gt_line)
            if dist < best_dist:
                best_dist = dist
                best_angle = angle

        if best_dist < dist_threshold and best_angle < angle_threshold:
            matches += 1

    precision = matches / len(pred_lines) if len(pred_lines) > 0 else 0
    recall = matches / len(gt_lines) if len(gt_lines) > 0 else 0
    lap = 2 * (precision * recall) / (precision + recall + 1e-8)

    return lap

def evaluate_model():
    """评估模型性能"""
    _, val_loader = create_dataloader(
        v1_1_path="/content/drive/MyDrive/v1.1",
        pointlines_path="/content/drive/MyDrive/pointlines",
        batch_size=1,
        num_workers=2
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = LineFeatureNetwork().to(device)

    try:
        model.load_state_dict(torch.load("line_model_epoch_30.pth"))
        print("成功加载模型权重")
    except Exception as e:
        print(f"加载模型出错: {str(e)}")
        return

    model.eval()

    # 评估指标
    sap_scores = []
    lap_scores = []
    fps_times = []

    print("开始评估...")

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(val_loader)):
            images = batch['image'].to(device)
            gt_lines = batch['lines'][0]

            # 测量FPS
            start_time = time.time()
            center_output, offset_output, length_output, angle_output = model(images)
            inference_time = time.time() - start_time
            fps = 1.0 / inference_time
            fps_times.append(fps)

            # 从中心点热图中提取中心点
            center_map = center_output.cpu().squeeze().numpy()
            centers_y, centers_x = np.where(center_map > 0.5)
            pred_centers = np.stack([centers_x, centers_y], axis=1)

            # 获取对应的位移向量、长度和角度
            offset_map = offset_output.cpu().squeeze().numpy()
            length_map = length_output.cpu().squeeze().numpy()
            angle_map = angle_output.cpu().squeeze().numpy()

            pred_lines = []
            for i in range(len(pred_centers)):
                cx, cy = pred_centers[i]
                x, y = int(cx), int(cy)
                if 0 <= x < offset_map.shape[2] and 0 <= y < offset_map.shape[1]:
                    dx, dy = offset_map[:, y, x]
                    length = length_map[y, x]
                    angle = angle_map[y, x]
                    # 使用长度和角度计算线段端点
                    x1 = cx - (length / 2) * np.cos(angle)
                    y1 = cy - (length / 2) * np.sin(angle)
                    x2 = cx + (length / 2) * np.cos(angle)
                    y2 = cy + (length / 2) * np.sin(angle)
                    pred_lines.append([x1, y1, x2, y2])

            pred_lines = torch.tensor(pred_lines)

            # 计算SAP和LAP
            sap = calculate_sap(pred_lines, gt_lines)
            lap = calculate_lap(pred_lines, gt_lines)

            sap_scores.append(sap)
            lap_scores.append(lap)

            # 可视化部分评估结果
            if batch_idx % 50 == 0:
                image = images.cpu().squeeze().permute(1, 2, 0).numpy()
                center_heatmap = center_output.cpu().squeeze().numpy()

                plt.figure(figsize=(10, 5))

                plt.subplot(121)
                plt.imshow(image)
                plt.title('Input Image')
                plt.axis('off')

                plt.subplot(122)
                plt.imshow(center_heatmap, cmap='jet')
                for line in pred_lines:
                    x1, y1, x2, y2 = line
                    plt.plot([x1, x2], [y1, y2], 'r-', linewidth=1)
                plt.title(f'Predicted Lines (num={len(pred_lines)})')
                plt.axis('off')

                plt.tight_layout()
                plt.show()

        # 计算平均指标
        mean_sap = np.mean(sap_scores)
        mean_lap = np.mean(lap_scores)
        mean_fps = np.mean(fps_times)

        print("\n评估结果:")
        print(f"SAP (Structure-Aware Performance): {mean_sap:.4f}")
        print(f"LAP (Line-Aware Performance): {mean_lap:.4f}")
        print(f"Average FPS: {mean_fps:.2f}")

if __name__ == "__main__":
    print("开始训练...")
    train_model()
    print("\n开始评估...")
    evaluate_model()









import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision.models import resnet18
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2
import pickle
import matplotlib.pyplot as plt
from tqdm import tqdm
import time

# =======================================
# 数据预处理部分
# =======================================

class WireframeDataset(Dataset):
    def __init__(self, v1_1_path, pointlines_path, split='train', img_size=256):
        self.img_size = img_size
        self.v1_1_path = Path(v1_1_path)
        self.pointlines_path = Path(pointlines_path)
        self.split = split

        # 获取图像列表
        self.img_dir = self.v1_1_path / split
        self.img_paths = list(self.img_dir.glob('*.jpg'))

        # 定义数据增强
        self.transform = A.Compose([
            A.Resize(img_size, img_size),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            ),
            ToTensorV2()
        ])

        print(f"Found {len(self.img_paths)} images in {split} set")

    def generate_heatmap(self, points, shape):
        """生成特征点的高斯热图"""
        heatmap = np.zeros(shape, dtype=np.float32)
        if len(points) == 0:
            return heatmap

        for (x, y) in points:
            if not (0 <= x < shape[1] and 0 <= y < shape[0]):
                continue

            # 生成高斯核
            sigma = 2
            size = int(6 * sigma + 1)
            x0 = np.arange(0, size, 1, float)
            y0 = x0[:, np.newaxis]
            x0 = x0 - size // 2
            y0 = y0 - size // 2
            gaussian = np.exp(- (x0 ** 2 + y0 ** 2) / (2 * sigma ** 2))

            # 计算高斯核的范围
            x, y = int(x), int(y)
            x1, y1 = max(0, x - size // 2), max(0, y - size // 2)
            x2, y2 = min(shape[1], x + size // 2 + 1), min(shape[0], y + size // 2 + 1)
            gx1, gy1 = max(0, size // 2 - x), max(0, size // 2 - y)
            gx2, gy2 = gx1 + (x2 - x1), gy1 + (y2 - y1)

            # 更新热图
            heatmap[y1:y2, x1:x2] = np.maximum(
                heatmap[y1:y2, x1:x2],
                gaussian[gy1:gy2, gx1:gx2]
            )

        return heatmap

    def generate_line_heatmap(self, lines, shape):
        """生成特征线的热图"""
        heatmap = np.zeros(shape, dtype=np.float32)
        if len(lines) == 0:
            return heatmap

        for (x1, y1, x2, y2) in lines:
            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])
            cv2.line(heatmap, (x1, y1), (x2, y2), 1, thickness=1)

        return heatmap

    def __getitem__(self, idx):
        # 读取图像
        img_path = str(self.img_paths[idx])
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w = image.shape[:2]

        # 获取对应的pkl文件
        img_name = self.img_paths[idx].stem
        pkl_path = self.pointlines_path / f"{img_name}.pkl"

        # 读取标注数据
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)

        # 获取点和线的坐标
        points = np.array(data['points'])  # [N, 2]
        lines = np.array(data['lines'])    # [M, 2]，表示点的索引

        # 将lines从点的索引转换为坐标
        if lines.size > 0:
            lines_coords = []
            for line in lines:
                idx1, idx2 = line.astype(int)
                x1, y1 = points[idx1]
                x2, y2 = points[idx2]
                lines_coords.append([x1, y1, x2, y2])
            lines = np.array(lines_coords)
        else:
            lines = np.zeros((0, 4))  # 没有线的情况

        # 调整坐标到目标尺寸
        scale_x = self.img_size / w
        scale_y = self.img_size / h
        points[:, 0] *= scale_x
        points[:, 1] *= scale_y
        if lines.size > 0:
            lines[:, [0, 2]] *= scale_x
            lines[:, [1, 3]] *= scale_y

        # 生成热图
        keypoint_heatmap = self.generate_heatmap(points, (self.img_size, self.img_size))
        line_heatmap = self.generate_line_heatmap(lines, (self.img_size, self.img_size))

        # 应用数据增强
        augmented = self.transform(image=image)
        image = augmented['image']

        return {
            'image': image,
            'keypoint_heatmap': torch.from_numpy(keypoint_heatmap)[None],
            'line_heatmap': torch.from_numpy(line_heatmap)[None],
            'points': torch.from_numpy(points.astype(np.float32)),
            'lines': torch.from_numpy(lines.astype(np.float32)),
            'path': img_path
        }

    def __len__(self):
        return len(self.img_paths)

def custom_collate_fn(batch):
    """自定义的collate函数，处理不同长度的points和lines"""
    images = torch.stack([item['image'] for item in batch])
    keypoint_heatmaps = torch.stack([item['keypoint_heatmap'] for item in batch])
    line_heatmaps = torch.stack([item['line_heatmap'] for item in batch])

    # 处理不同长度的points和lines
    max_points = max([len(item['points']) for item in batch])
    max_lines = max([len(item['lines']) for item in batch])
    points_padded = []
    lines_padded = []
    for item in batch:
        points = item['points']
        lines = item['lines']
        points_padding = torch.zeros((max_points - len(points), 2), dtype=points.dtype)
        lines_padding = torch.zeros((max_lines - len(lines), 4), dtype=lines.dtype)
        points_padded.append(torch.cat([points, points_padding]))
        lines_padded.append(torch.cat([lines, lines_padding]))
    points = torch.stack(points_padded)
    lines = torch.stack(lines_padded)

    paths = [item['path'] for item in batch]

    return {
        'image': images,
        'keypoint_heatmap': keypoint_heatmaps,
        'line_heatmap': line_heatmaps,
        'points': points,
        'lines': lines,
        'path': paths
    }

def create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=2):
    """创建数据加载器"""
    train_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='train'
    )

    val_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='test'
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    return train_loader, val_loader

# =======================================
# 网络定义部分（修改后的模型）
# =======================================

# 点感知模块
class PointAwareModule(nn.Module):
    def __init__(self, in_channels):
        super(PointAwareModule, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)
        self.bn = nn.BatchNorm2d(in_channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x1 = self.conv1(x)
        x2 = self.conv2(x)
        x3 = self.conv3(self.conv1(x))
        out = x + self.bn(x1) + self.bn(x2) + self.bn(x3)
        out = self.relu(out)
        return out

# 线感知模块
class LineAwareModule(nn.Module):
    def __init__(self, in_channels):
        super(LineAwareModule, self).__init__()
        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.softmax = nn.Softmax(dim=-1)
        self.gamma = nn.Parameter(torch.zeros(1))
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        batch_size, C, width, height = x.size()
        proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)
        proj_key = self.key_conv(x).view(batch_size, -1, width * height)
        energy = torch.bmm(proj_query, proj_key)
        attention = self.softmax(energy)
        proj_value = self.value_conv(x).view(batch_size, -1, width * height)
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(batch_size, C, width, height)
        out = self.gamma * out + x
        out = self.relu(out)
        return out

# 并行注意力模块
class ParallelAttentionBlock(nn.Module):
    def __init__(self, in_channels):
        super(ParallelAttentionBlock, self).__init__()
        self.edge_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.sigmoid = nn.Sigmoid()
        self.channel_conv1 = nn.Conv2d(in_channels, in_channels // 16, kernel_size=1)
        self.channel_conv2 = nn.Conv2d(in_channels // 16, in_channels, kernel_size=1)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        # 空间注意力
        edge = self.edge_conv(x)
        edge_attention = self.sigmoid(edge)
        edge_out = x * edge_attention + x

        # 通道注意力
        gap = F.adaptive_avg_pool2d(edge_out, (1, 1))
        channel_att = self.relu(self.channel_conv1(gap))
        channel_att = self.sigmoid(self.channel_conv2(channel_att))
        channel_out = edge_out * channel_att + edge_out

        return channel_out

# 主干网络
class MultiTaskNetwork(nn.Module):
    def __init__(self, descriptor_dim=128):
        super(MultiTaskNetwork, self).__init__()
        # 使用轻量级的ResNet18作为编码器
        resnet = resnet18(pretrained=True)
        self.encoder = nn.Sequential(
            resnet.conv1,
            resnet.bn1,
            resnet.relu,
            resnet.maxpool,
            resnet.layer1,
            resnet.layer2,
            resnet.layer3,
            resnet.layer4
        )

        # 点线感知的多任务注意力模块
        self.point_aware_module = PointAwareModule(512)
        self.line_aware_module = LineAwareModule(512)
        self.parallel_attention_block = ParallelAttentionBlock(512)

        # 解码器，用于上采样到原始尺寸（256x256）
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
        )

        # 特征点检测头
        self.keypoint_head = nn.Conv2d(16, 1, kernel_size=1)

        # 特征线检测头
        self.line_head = nn.Conv2d(16, 1, kernel_size=1)

        # 特征点描述子头
        self.keypoint_descriptor_head = nn.Sequential(
            nn.Conv2d(16, descriptor_dim, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        )

        # 特征线描述子头
        self.line_descriptor_head = nn.Sequential(
            nn.Conv2d(16, descriptor_dim, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        )

    def forward(self, x):
        features = self.encoder(x)

        # 点线感知的多任务注意力模块
        point_features = self.point_aware_module(features)
        line_features = self.line_aware_module(features)
        combined_features = self.parallel_attention_block(point_features + line_features)

        decoded_features = self.decoder(combined_features)

        # 特征点检测
        keypoint_output = self.keypoint_head(decoded_features)
        # 特征线检测
        line_output = self.line_head(decoded_features)
        # 特征点描述子
        keypoint_descriptor = self.keypoint_descriptor_head(decoded_features)
        # 特征线描述子
        line_descriptor = self.line_descriptor_head(decoded_features)

        return keypoint_output, line_output, keypoint_descriptor, line_descriptor

# =======================================
# 训练和评估部分
# =======================================

def train_model():
    # 创建数据加载器
    train_loader, val_loader = create_dataloader(
        v1_1_path="/content/drive/MyDrive/v1.1",
        pointlines_path="/content/drive/MyDrive/pointlines",
        batch_size=8,
        num_workers=2
    )

    # 初始化模型和损失函数
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 定义学生模型
    model = MultiTaskNetwork().to(device)

    # 定义优化器和学习率调度器
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)
    num_epochs = 1  # 增加训练轮数

    # 定义损失函数
    criterion = nn.BCEWithLogitsLoss()

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            images = batch['image'].to(device)
            keypoint_targets = batch['keypoint_heatmap'].to(device)
            line_targets = batch['line_heatmap'].to(device)

            optimizer.zero_grad()

            # 模型的输出
            outputs = model(images)
            keypoint_output, line_output, _, _ = outputs

            # 计算损失
            loss_keypoint = criterion(keypoint_output, keypoint_targets)
            loss_line = criterion(line_output, line_targets)

            # 总损失
            loss = loss_keypoint + loss_line
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        scheduler.step()  # 更新学习率
        avg_loss = running_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

        # 每个 epoch 后保存模型
        torch.save(model.state_dict(), f"model_epoch_{epoch+1}.pth")

def calculate_sap(pred_points, gt_points, threshold=3.0):
    """计算结构感知性能(SAP)"""
    if len(pred_points) == 0 or len(gt_points) == 0:
        return 0.0

    distances = torch.cdist(pred_points.float(), gt_points.float())
    min_distances, _ = torch.min(distances, dim=1)
    correct_matches = (min_distances < threshold).float()

    precision = correct_matches.sum() / len(pred_points)
    recall = correct_matches.sum() / len(gt_points)
    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)

    return f1.item()

def calculate_lap(pred_lines, gt_lines, dist_threshold=5.0, angle_threshold=10.0):
    """计算线段感知性能(LAP)"""
    if len(pred_lines) == 0 or len(gt_lines) == 0:
        return 0.0

    def compute_line_similarity(line1, line2):
        # 确保转换为浮点类型
        line1 = line1.float()
        line2 = line2.float()

        dist1 = torch.sqrt(((line1[:2] - line2[:2])**2).sum())
        dist2 = torch.sqrt(((line1[2:] - line2[2:])**2).sum())

        vec1 = line1[2:] - line1[:2]
        vec2 = line2[2:] - line2[:2]

        # 计算向量的范数
        norm1 = torch.sqrt((vec1 * vec1).sum())
        norm2 = torch.sqrt((vec2 * vec2).sum())

        # 计算余弦角
        cos_angle = (vec1 * vec2).sum() / (norm1 * norm2 + 1e-8)
        angle = torch.acos(torch.clamp(cos_angle, -1.0, 1.0)) * 180 / np.pi

        return dist1.item(), angle.item()

    matches = 0
    for pred_line in pred_lines:
        best_dist = float('inf')
        best_angle = float('inf')

        for gt_line in gt_lines:
            dist, angle = compute_line_similarity(pred_line, gt_line)
            if dist < best_dist:
                best_dist = dist
                best_angle = angle

        if best_dist < dist_threshold and best_angle < angle_threshold:
            matches += 1

    precision = matches / len(pred_lines) if len(pred_lines) > 0 else 0
    recall = matches / len(gt_lines) if len(gt_lines) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)

    return f1

    matches = 0
    for pred_line in pred_lines:
        best_dist = float('inf')
        best_angle = float('inf')

        for gt_line in gt_lines:
            dist, angle = compute_line_similarity(pred_line, gt_line)
            if dist < best_dist:
                best_dist = dist
                best_angle = angle

        if best_dist < dist_threshold and best_angle < angle_threshold:
            matches += 1

    precision = matches / len(pred_lines) if len(pred_lines) > 0 else 0
    recall = matches / len(gt_lines) if len(gt_lines) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)

    return f1

def evaluate_model():
    """评估模型性能"""
    _, val_loader = create_dataloader(
        v1_1_path="/content/drive/MyDrive/v1.1",
        pointlines_path="/content/drive/MyDrive/pointlines",
        batch_size=1,
        num_workers=2
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = MultiTaskNetwork().to(device)

    try:
        model.load_state_dict(torch.load("model_epoch_1.pth"))
        print("成功加载模型权重")
    except Exception as e:
        print(f"加载模型出错: {str(e)}")
        return

    model.eval()

    # 评估指标
    sap_scores = []
    lap_scores = []
    fps_times = []

    print("开始评估...")

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(val_loader)):
            images = batch['image'].to(device)
            gt_points = batch['points'][0]
            gt_lines = batch['lines'][0]

            # 测量FPS
            start_time = time.time()
            outputs = model(images)
            inference_time = time.time() - start_time
            fps = 1.0 / inference_time
            fps_times.append(fps)

            keypoint_output, line_output, keypoint_desc, line_desc = outputs

            # 应用 Sigmoid 激活函数
            keypoint_output = torch.sigmoid(keypoint_output)
            line_output = torch.sigmoid(line_output)

            # 从热图中提取关键点
            keypoint_map = keypoint_output.cpu().squeeze().numpy()
            points_y, points_x = np.where(keypoint_map > 0.5)
            pred_points = torch.tensor(np.stack([points_x, points_y], axis=1))

            # 从热图中提取线段
            line_map = line_output.cpu().squeeze().numpy()
            line_map = (line_map > 0.5).astype(np.uint8) * 255
            lines = cv2.HoughLinesP(line_map, 1, np.pi/180,
                                  threshold=50,
                                  minLineLength=20,
                                  maxLineGap=10)

            if lines is not None:
                pred_lines = torch.tensor(lines[:, 0, :])
            else:
                pred_lines = torch.zeros((0, 4))

            # 计算SAP和LAP
            sap = calculate_sap(pred_points, gt_points)
            lap = calculate_lap(pred_lines, gt_lines)

            sap_scores.append(sap)
            lap_scores.append(lap)

            # 可视化部分评估结果
            if batch_idx % 50 == 0:
                image = images.cpu().squeeze().permute(1, 2, 0).numpy()
                keypoint_heatmap = keypoint_output.cpu().squeeze().numpy()
                line_heatmap = line_output.cpu().squeeze().numpy()

                plt.figure(figsize=(15, 5))

                plt.subplot(131)
                plt.imshow(image)
                plt.title('Input Image')
                plt.axis('off')

                plt.subplot(132)
                plt.imshow(keypoint_heatmap, cmap='jet')
                plt.plot(pred_points[:,0], pred_points[:,1], 'r.')
                plt.title(f'Keypoints (num={len(pred_points)})')
                plt.axis('off')

                plt.subplot(133)
                plt.imshow(line_heatmap, cmap='jet')
                for line in pred_lines:
                    x1,y1,x2,y2 = line
                    plt.plot([x1,x2], [y1,y2], 'r-', linewidth=1)
                plt.title(f'Lines (num={len(pred_lines)})')
                plt.axis('off')

                plt.tight_layout()
                plt.show()

    # 计算平均指标
    mean_sap = np.mean(sap_scores)
    mean_lap = np.mean(lap_scores)
    mean_fps = np.mean(fps_times)

    print("\n评估结果:")
    print(f"SAP (Structure-Aware Performance): {mean_sap:.4f}")
    print(f"LAP (Line-Aware Performance): {mean_lap:.4f}")
    print(f"Average FPS: {mean_fps:.2f}")

if __name__ == "__main__":
    print("开始训练...")
    train_model()
    print("\n开始评估...")
    evaluate_model()









import time

def compute_sap(gt_points, pred_points, threshold=5):
    """
    计算 SAP (Structure-Aware Performance)
    Args:
        gt_points: Ground truth 特征点 (N, 2)
        pred_points: 预测特征点 (M, 2)
        threshold: 距离阈值
    Returns:
        precision, recall, f1_score
    """
    if len(pred_points) == 0 or len(gt_points) == 0:
        return 0.0, 0.0, 0.0

    gt_points = np.array(gt_points)
    pred_points = np.array(pred_points)

    matches = []
    for pred in pred_points:
        distances = np.linalg.norm(gt_points - pred, axis=1)
        if np.min(distances) < threshold:
            matches.append(pred)

    tp = len(matches)  # 正确预测
    precision = tp / len(pred_points) if len(pred_points) > 0 else 0.0
    recall = tp / len(gt_points) if len(gt_points) > 0 else 0.0
    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0

    return precision, recall, f1_score

def compute_lap(gt_lines, pred_lines, threshold=5):
    """
    计算 LAP (Line-Aware Performance)
    Args:
        gt_lines: Ground truth 特征线 (N, 4)
        pred_lines: 预测特征线 (M, 4)
        threshold: 距离阈值
    Returns:
        precision, recall, f1_score
    """
    if len(pred_lines) == 0 or len(gt_lines) == 0:
        return 0.0, 0.0, 0.0

    gt_lines = np.array(gt_lines)
    pred_lines = np.array(pred_lines)

    matches = []
    for pred_line in pred_lines:
        distances = []
        for gt_line in gt_lines:
            d1 = np.linalg.norm(gt_line[:2] - pred_line[:2])
            d2 = np.linalg.norm(gt_line[2:] - pred_line[2:])
            distances.append((d1 + d2) / 2)
        if np.min(distances) < threshold:
            matches.append(pred_line)

    tp = len(matches)  # 正确预测
    precision = tp / len(pred_lines) if len(pred_lines) > 0 else 0.0
    recall = tp / len(gt_lines) if len(gt_lines) > 0 else 0.0
    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0

    return precision, recall, f1_score

def evaluate_model_with_metrics():
    # 创建验证数据加载器
    _, val_loader = create_dataloader(
        v1_1_path="/content/drive/MyDrive/v1.1",
        pointlines_path="/content/drive/MyDrive/pointlines",
        batch_size=1,
        num_workers=2
    )

    # 加载模型
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = MultiTaskNetwork().to(device)
    model.load_state_dict(torch.load("student_model_epoch_50.pth"))  # 替换为实际的模型路径
    model.eval()

    total_sap = {"precision": 0.0, "recall": 0.0, "f1_score": 0.0}
    total_lap = {"precision": 0.0, "recall": 0.0, "f1_score": 0.0}
    total_samples = 0
    total_inference_time = 0.0

    with torch.no_grad():
        for batch in val_loader:
            images = batch['image'].to(device)
            gt_points = batch['points'][0].cpu().numpy()
            gt_lines = batch['lines'][0].cpu().numpy()

            # 计时推理时间
            start_time = time.time()
            outputs = model(images)
            keypoint_output, line_output, _, _ = outputs
            total_inference_time += time.time() - start_time

            # 解析预测结果
            pred_points = np.argwhere(keypoint_output.cpu().squeeze().numpy() > 0.5)
            pred_lines = np.argwhere(line_output.cpu().squeeze().numpy() > 0.5)

            # 计算 SAP 和 LAP
            sap = compute_sap(gt_points, pred_points)
            lap = compute_lap(gt_lines, pred_lines)

            # 累积
            total_samples += 1
            total_sap["precision"] += sap[0]
            total_sap["recall"] += sap[1]
            total_sap["f1_score"] += sap[2]
            total_lap["precision"] += lap[0]
            total_lap["recall"] += lap[1]
            total_lap["f1_score"] += lap[2]

    # 平均指标
    avg_sap = {k: v / total_samples for k, v in total_sap.items()}
    avg_lap = {k: v / total_samples for k, v in total_lap.items()}
    fps = total_samples / total_inference_time

    print(f"SAP (Structure-Aware Performance): Precision={avg_sap['precision']:.4f}, Recall={avg_sap['recall']:.4f}, F1={avg_sap['f1_score']:.4f}")
    print(f"LAP (Line-Aware Performance): Precision={avg_lap['precision']:.4f}, Recall={avg_lap['recall']:.4f}, F1={avg_lap['f1_score']:.4f}")
    print(f"FPS (Frames Per Second): {fps:.2f}")

if __name__ == "__main__":
    # 执行评估
    evaluate_model_with_metrics()





"""# **LSD**"""

import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision.models import resnet18
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2
import pickle
import matplotlib.pyplot as plt
from tqdm import tqdm
import time
import math
import warnings

warnings.filterwarnings("ignore")

# =======================================
# Data Preprocessing Utilities
# =======================================

def compute_gradients(image):
    """
    Compute gradient magnitude and angle using Sobel filters.
    """
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    gray = gray.astype(np.float32) / 255.0

    grad_x = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)
    grad_y = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)
    grad_mag = np.sqrt(grad_x**2 + grad_y**2)
    angle_map = np.arctan2(grad_y, grad_x) * 180 / np.pi
    angle_map = (angle_map + 180) % 180  # Normalize to [0, 180)

    return grad_x, grad_y, grad_mag, angle_map

def region_grow(grad_x, grad_y, angle_map, tolerance=22.5):
    """
    Perform region growing based on gradient orientations.
    """
    h, w = angle_map.shape
    status = np.zeros((h, w), dtype=np.uint8)  # 0: Not Used, 1: Used
    regions = []

    # Sort pixels by gradient magnitude (descending)
    grad_mag = np.sqrt(grad_x**2 + grad_y**2)
    sorted_indices = np.argsort(-grad_mag, axis=None)

    for idx in sorted_indices:
        y, x = divmod(idx, w)
        if status[y, x] != 0 or grad_mag[y, x] < 1e-3:
            continue
        current_angle = angle_map[y, x]
        region = [(x, y)]
        status[y, x] = 1
        queue = [(x, y)]

        while queue:
            cx, cy = queue.pop(0)
            neighbors = [
                (nx, ny)
                for nx in range(cx - 1, cx + 2)
                for ny in range(cy - 1, cy + 2)
                if 0 <= nx < w and 0 <= ny < h and (nx != cx or ny != cy)
            ]
            for nx, ny in neighbors:
                if status[ny, nx] == 0 and abs(angle_map[ny, nx] - current_angle) < tolerance:
                    status[ny, nx] = 1
                    region.append((nx, ny))
                    queue.append((nx, ny))

        regions.append(region)

    return regions

def approximate_rectangles(regions):
    """
    Approximate each region with a rectangle.
    """
    rectangles = []
    for region in regions:
        if len(region) < 2:
            continue
        pts = np.array(region, dtype=np.float32)
        rect = cv2.minAreaRect(pts)
        (center_x, center_y), (width, length), angle = rect

        # Ensure width >= length
        if width < length:
            width, length = length, width
            angle += 90

        rectangles.append({
            'center': (center_x, center_y),
            'angle': angle,
            'length': length,
            'width': width,
            'points': pts
        })

    return rectangles

def compute_nfa(rectangle, image_size, p=0.125, epsilon=1):
    """
    Compute the Number of False Alarms (NFA) for a rectangle using logarithms to prevent overflow.
    Returns log_nfa for comparison.
    """
    num_potential = image_size[0] ** 5

    n = int(rectangle['length'] * rectangle['width'])
    k = len(rectangle['points'])
    if n < k:
        return float('inf')  # Impossible case

    try:
        log_comb = math.lgamma(n + 1) - math.lgamma(k + 1) - math.lgamma(n - k + 1)
    except OverflowError:
        return float('inf')

    try:
        log_prob = k * math.log(p) + (n - k) * math.log(1 - p)
    except ValueError:
        return float('inf')

    # Compute log(NFA)
    log_nfa = math.log(num_potential) + log_comb + log_prob

    return log_nfa

def validate_rectangles(rectangles, image_size, epsilon=1):
    """
    Validate rectangles based on NFA.
    Returns a list of valid rectangles.
    """
    valid_rects = []
    log_epsilon = math.log(epsilon)
    for rect in rectangles:
        log_nfa = compute_nfa(rect, image_size, p=0.125, epsilon=epsilon)
        if log_nfa < log_epsilon:
            valid_rects.append(rect)
    return valid_rects

# =======================================
# Dataset Class
# =======================================

class WireframeDataset(Dataset):
    def __init__(self, v1_1_path, pointlines_path, split='train', img_size=256, nfa_threshold=1):
        self.img_size = img_size
        self.v1_1_path = Path(v1_1_path)
        self.pointlines_path = Path(pointlines_path)
        self.split = split
        self.nfa_threshold = nfa_threshold

        # Get image list
        self.img_dir = self.v1_1_path / split
        self.img_paths = list(self.img_dir.glob('*.jpg'))

        # Define data augmentation
        self.transform = A.Compose([
            A.Resize(img_size, img_size),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            ),
            ToTensorV2()
        ])

        print(f"Found {len(self.img_paths)} images in {split} set")

    def generate_heatmap(self, points, shape):
        """Generate Gaussian heatmap for keypoints."""
        heatmap = np.zeros(shape, dtype=np.float32)
        if len(points) == 0:
            return heatmap

        for (x, y) in points:
            if not (0 <= x < shape[1] and 0 <= y < shape[0]):
                continue

            sigma = 2
            size = int(6 * sigma + 1)
            x0 = np.arange(0, size, 1, float)
            y0 = x0[:, np.newaxis]
            x0 = x0 - size // 2
            y0 = y0 - size // 2
            gaussian = np.exp(- (x0 ** 2 + y0 ** 2) / (2 * sigma ** 2))

            x, y = int(x), int(y)
            x1, y1 = max(0, x - size // 2), max(0, y - size // 2)
            x2, y2 = min(shape[1], x + size // 2 + 1), min(shape[0], y + size // 2 + 1)
            gx1, gy1 = max(0, size // 2 - x), max(0, size // 2 - y)
            gx2, gy2 = gx1 + (x2 - x1), gy1 + (y2 - y1)

            heatmap[y1:y2, x1:x2] = np.maximum(
                heatmap[y1:y2, x1:x2],
                gaussian[gy1:gy2, gx1:gx2]
            )

        return heatmap

    def generate_line_heatmap(self, lines, shape):
        """Generate heatmap for lines."""
        heatmap = np.zeros(shape, dtype=np.float32)
        if len(lines) == 0:
            return heatmap

        for (x1, y1, x2, y2) in lines:
            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])
            cv2.line(heatmap, (x1, y1), (x2, y2), 1, thickness=1)

        return heatmap

    def __getitem__(self, idx):
        img_path = str(self.img_paths[idx])
        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image not found: {img_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w = image.shape[:2]

        img_name = self.img_paths[idx].stem
        pkl_path = self.pointlines_path / f"{img_name}.pkl"

        if not pkl_path.exists():
            raise FileNotFoundError(f"Pkl file not found: {pkl_path}")
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)

        points = np.array(data['points'])  # [N, 2]
        lines = np.array(data['lines'])    # [M, 2] (indices of points)

        # Convert lines from point indices to coordinates
        if lines.size > 0:
            lines_coords = []
            for line in lines:
                idx1, idx2 = line.astype(int)
                if idx1 >= len(points) or idx2 >= len(points):
                    continue
                x1, y1 = points[idx1]
                x2, y2 = points[idx2]
                lines_coords.append([x1, y1, x2, y2])
            lines = np.array(lines_coords)
        else:
            lines = np.zeros((0, 4))

        # Scale coordinates
        scale_x = self.img_size / w
        scale_y = self.img_size / h
        points[:, 0] *= scale_x
        points[:, 1] *= scale_y
        if lines.size > 0:
            lines[:, [0, 2]] *= scale_x
            lines[:, [1, 3]] *= scale_y

        keypoint_heatmap = self.generate_heatmap(points, (self.img_size, self.img_size))
        resized_image = cv2.resize(image, (self.img_size, self.img_size))
        grad_x, grad_y, grad_mag, angle_map = compute_gradients(resized_image)

        regions = region_grow(grad_x, grad_y, angle_map, tolerance=22.5)
        rectangles = approximate_rectangles(regions)
        valid_rects = validate_rectangles(rectangles, (self.img_size, self.img_size), epsilon=self.nfa_threshold)

        line_heatmap_lsd = np.zeros((self.img_size, self.img_size), dtype=np.float32)
        for rect in valid_rects:
            box = cv2.boxPoints((
                (rect['center'][0], rect['center'][1]),
                (rect['length'], rect['width']),
                rect['angle']
            ))
            box = np.int0(box)
            cv2.polylines(line_heatmap_lsd, [box], isClosed=True, color=1, thickness=1)

        augmented = self.transform(image=resized_image)
        image = augmented['image']
        angle_map = angle_map.astype(np.int64)

        return {
            'image': image,
            'keypoint_heatmap': torch.from_numpy(keypoint_heatmap)[None],
            'line_heatmap': torch.from_numpy(line_heatmap_lsd)[None],
            'points': torch.from_numpy(points.astype(np.float32)),
            'lines': torch.from_numpy(lines.astype(np.float32)),
            'angle_map': torch.from_numpy(angle_map),
            'path': img_path
        }

    def __len__(self):
        return len(self.img_paths)

def custom_collate_fn(batch):
    images = torch.stack([item['image'] for item in batch])
    keypoint_heatmaps = torch.stack([item['keypoint_heatmap'] for item in batch])
    line_heatmaps = torch.stack([item['line_heatmap'] for item in batch])

    max_points = max([len(item['points']) for item in batch])
    max_lines = max([len(item['lines']) for item in batch])
    points_padded = []
    lines_padded = []
    for item in batch:
        points = item['points']
        lines = item['lines']
        points_padding = torch.zeros((max_points - len(points), 2), dtype=points.dtype)
        lines_padding = torch.zeros((max_lines - len(lines), 4), dtype=lines.dtype)
        points_padded.append(torch.cat([points, points_padding]))
        lines_padded.append(torch.cat([lines, lines_padding]))
    points = torch.stack(points_padded)
    lines = torch.stack(lines_padded)

    angle_maps = torch.stack([item['angle_map'] for item in batch])
    paths = [item['path'] for item in batch]

    return {
        'image': images,
        'keypoint_heatmap': keypoint_heatmaps,
        'line_heatmap': line_heatmaps,
        'points': points,
        'lines': lines,
        'angle_map': angle_maps,
        'path': paths
    }

def create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=4):
    train_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='train',
        img_size=256,
        nfa_threshold=1
    )

    val_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='test',
        img_size=256,
        nfa_threshold=1
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    return train_loader, val_loader

# =======================================
# Global Context Module (Modified)
# =======================================
class GlobalContextModule(nn.Module):
    """
    简化的全局上下文模块。保持输入输出通道数相同。
    """
    def __init__(self, in_channels):
        super(GlobalContextModule, self).__init__()
        self.in_channels = in_channels
        self.inter_channels = in_channels // 2

        self.g = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)
        self.theta = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)
        self.phi = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)
        self.W_z = nn.Conv2d(self.inter_channels, in_channels, kernel_size=1)

        nn.init.constant_(self.W_z.weight, 0)
        nn.init.constant_(self.W_z.bias, 0)

        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        batch_size, C, H, W = x.size()

        g_x = self.g(x).view(batch_size, self.inter_channels, -1)  # B, C//2, HW
        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)  # B, C//2, HW
        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)    # B, C//2, HW

        theta_x = theta_x.permute(0, 2, 1)  # B, HW, C//2
        f = torch.matmul(theta_x, phi_x)  # B, HW, HW
        f_div_C = F.softmax(f, dim=-1)

        y = torch.matmul(f_div_C, g_x.permute(0, 2, 1))  # B, HW, C//2
        y = y.permute(0, 2, 1).view(batch_size, self.inter_channels, H, W)
        z = self.W_z(y)

        return x + self.gamma * z

# =======================================
# Multi-Task Network (Simplified)
# =======================================

class MultiTaskNetwork(nn.Module):
    def __init__(self, descriptor_dim=64):
        super(MultiTaskNetwork, self).__init__()
        # 使用ResNet18作为encoder
        resnet = resnet18(pretrained=True)
        self.encoder = nn.Sequential(
            resnet.conv1,
            resnet.bn1,
            resnet.relu,
            resnet.maxpool,
            resnet.layer1,   # [64, H/4, W/4]
            resnet.layer2,   # [128, H/8, W/8]
            resnet.layer3,   # [256, H/16, W/16]
            resnet.layer4    # [512, H/32, W/32]
        )

        # 全局上下文模块
        self.global_context_module = GlobalContextModule(512)

        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
        )

        # Detection Heads
        self.keypoint_head = nn.Conv2d(16, 1, kernel_size=1)
        self.line_head = nn.Conv2d(16, 1, kernel_size=1)

        # Descriptor Heads
        self.keypoint_descriptor_head = nn.Sequential(
            nn.Conv2d(16, descriptor_dim, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        )

        self.line_descriptor_head = nn.Sequential(
            nn.Conv2d(16, descriptor_dim, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        )

    def forward(self, x, angle_map):
        features = self.encoder(x)  # [B, 512, H/32, W/32]

        # 下采样angle_map以匹配特征图大小
        angle_map = F.interpolate(angle_map.unsqueeze(1).float(), size=features.shape[2:], mode='nearest').squeeze(1).long()

        # 全局上下文
        combined_features = self.global_context_module(features)  # [B, 512, H/32, W/32]

        # 解码
        decoded_features = self.decoder(combined_features)  # [B, 16, H, W]

        # 检测输出
        keypoint_output = self.keypoint_head(decoded_features)  # [B, 1, H, W]
        line_output = self.line_head(decoded_features)          # [B, 1, H, W]

        # 描述符输出
        keypoint_descriptor = self.keypoint_descriptor_head(decoded_features)  # [B, 64, 2H, 2W]
        line_descriptor = self.line_descriptor_head(decoded_features)          # [B, 64, 2H, 2W]

        return keypoint_output, line_output, keypoint_descriptor, line_descriptor


# =======================================
# Metrics
# =======================================

def calculate_sap(pred_points, gt_points, threshold=3.0):
    if len(pred_points) == 0 or len(gt_points) == 0:
        return 0.0
    distances = torch.cdist(pred_points.float(), gt_points.float())
    min_distances, _ = torch.min(distances, dim=1)
    correct_matches = (min_distances < threshold).float()

    precision = correct_matches.sum() / len(pred_points) if len(pred_points) > 0 else 0
    recall = correct_matches.sum() / len(gt_points) if len(gt_points) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)
    return f1.item()

def calculate_lap(pred_lines, gt_lines, dist_threshold=5.0, angle_threshold=10.0):
    if len(pred_lines) == 0 or len(gt_lines) == 0:
        return 0.0

    def compute_line_similarity(line1, line2):
        line1 = line1.float()
        line2 = line2.float()

        dist1 = torch.sqrt(((line1[:2] - line2[:2]) ** 2).sum())
        dist2 = torch.sqrt(((line1[2:] - line2[2:]) ** 2).sum())

        vec1 = line1[2:] - line1[:2]
        vec2 = line2[2:] - line2[:2]

        norm1 = torch.sqrt((vec1 * vec1).sum())
        norm2 = torch.sqrt((vec2 * vec2).sum())

        cos_angle = (vec1 * vec2).sum() / (norm1 * norm2 + 1e-8)
        angle = torch.acos(torch.clamp(cos_angle, -1.0, 1.0)) * 180 / np.pi
        return dist1.item(), angle.item()

    matches = 0
    matched_gt = set()
    for pred_line in pred_lines:
        best_dist = float('inf')
        best_angle = float('inf')
        best_gt = -1

        for idx, gt_line in enumerate(gt_lines):
            if idx in matched_gt:
                continue
            dist, angle = compute_line_similarity(pred_line, gt_line)
            if dist < best_dist:
                best_dist = dist
                best_angle = angle
                best_gt = idx

        if best_dist < dist_threshold and best_angle < angle_threshold:
            matches += 1
            matched_gt.add(best_gt)

    precision = matches / len(pred_lines) if len(pred_lines) > 0 else 0
    recall = matches / len(gt_lines) if len(gt_lines) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)
    return f1

# =======================================
# Training and Evaluation
# =======================================

def train_model(v1_1_path, pointlines_path, model_save_path="model", batch_size=8, num_workers=4, num_epochs=50):
    train_loader, val_loader = create_dataloader(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        batch_size=batch_size,
        num_workers=num_workers
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = MultiTaskNetwork().to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)
    criterion = nn.BCEWithLogitsLoss()

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            images = batch['image'].to(device)
            keypoint_targets = batch['keypoint_heatmap'].to(device)
            line_targets = batch['line_heatmap'].to(device)
            angle_maps = batch['angle_map'].to(device)

            optimizer.zero_grad()
            keypoint_output, line_output, _, _ = model(images, angle_maps)
            loss_keypoint = criterion(keypoint_output, keypoint_targets)
            loss_line = criterion(line_output, line_targets)
            loss = loss_keypoint + loss_line
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        scheduler.step()
        avg_loss = running_loss / len(train_loader)
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")
        torch.save(model.state_dict(), f"{model_save_path}_epoch_{epoch+1}.pth")

    print("Training completed.")

def evaluate_model(v1_1_path, pointlines_path, model_path="model_epoch_50.pth", batch_size=1, num_workers=4):
    _, val_loader = create_dataloader(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        batch_size=batch_size,
        num_workers=num_workers
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = MultiTaskNetwork().to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    sap_scores = []
    lap_scores = []
    fps_times = []

    print("Starting evaluation...")
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(val_loader, desc="Evaluating")):
            images = batch['image'].to(device)
            gt_points = batch['points'][0]
            gt_lines = batch['lines'][0]
            angle_maps = batch['angle_map'].to(device)

            start_time = time.time()
            keypoint_output, line_output, _, _ = model(images, angle_maps)
            inference_time = time.time() - start_time
            fps = 1.0 / inference_time if inference_time > 0 else 0.0
            fps_times.append(fps)

            keypoint_output = torch.sigmoid(keypoint_output)
            line_output = torch.sigmoid(line_output)

            keypoint_map = keypoint_output.cpu().squeeze().numpy()
            pred_points_y, pred_points_x = np.where(keypoint_map > 0.5)
            pred_points = torch.tensor(np.stack([pred_points_x, pred_points_y], axis=1))

            line_map = (line_output.cpu().squeeze().numpy() > 0.5).astype(np.uint8) * 255
            lines = cv2.HoughLinesP(line_map, 1, np.pi / 180, threshold=50, minLineLength=20, maxLineGap=10)

            if lines is not None:
                pred_lines = torch.tensor(lines[:, 0, :])
            else:
                pred_lines = torch.zeros((0, 4))

            sap = calculate_sap(pred_points, gt_points)
            lap = calculate_lap(pred_lines, gt_lines)
            sap_scores.append(sap)
            lap_scores.append(lap)

            if batch_idx % 50 == 0:
                image_np = images.cpu().squeeze().permute(1, 2, 0).numpy()
                image_np = (image_np * np.array([0.229, 0.224, 0.225]) +
                            np.array([0.485, 0.456, 0.406]))
                image_np = np.clip(image_np, 0, 1)

                plt.figure(figsize=(15, 5))

                plt.subplot(131)
                plt.imshow(image_np)
                plt.title('Input Image')
                plt.axis('off')

                plt.subplot(132)
                plt.imshow(keypoint_map, cmap='jet')
                plt.plot(pred_points[:, 0], pred_points[:, 1], 'r.', markersize=2)
                plt.title(f'Keypoints (num={len(pred_points)})')
                plt.axis('off')

                plt.subplot(133)
                plt.imshow(line_map, cmap='gray')
                if pred_lines.size(0) > 0:
                    for line in pred_lines:
                        x1, y1, x2, y2 = line
                        plt.plot([x1, x2], [y1, y2], 'r-', linewidth=1)
                plt.title(f'Lines (num={len(pred_lines)})')
                plt.axis('off')

                plt.tight_layout()
                plt.show()

    mean_sap = np.mean(sap_scores) if len(sap_scores) > 0 else 0.0
    mean_lap = np.mean(lap_scores) if len(lap_scores) > 0 else 0.0
    mean_fps = np.mean(fps_times) if len(fps_times) > 0 else 0.0

    print("\nEvaluation Results:")
    print(f"SAP (Structure-Aware Performance - F1 Score): {mean_sap:.4f}")
    print(f"LAP (Line-Aware Performance - F1 Score): {mean_lap:.4f}")
    print(f"Average FPS: {mean_fps:.2f}")

# =======================================
# Main Execution
# =======================================

if __name__ == "__main__":
    # 请根据实际数据集路径进行修改
    v1_1_path = "/content/drive/MyDrive/v1.1"
    pointlines_path = "/content/drive/MyDrive/pointlines"

    if not os.path.exists(v1_1_path):
        raise FileNotFoundError(f"v1.1_path not found: {v1_1_path}")
    if not os.path.exists(pointlines_path):
        raise FileNotFoundError(f"pointlines_path not found: {pointlines_path}")

    print("Starting Training...")
    train_model(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        model_save_path="model",
        batch_size=8,
        num_workers=4,
        num_epochs=50
    )

    print("\nStarting Evaluation...")
    evaluate_model(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        model_path="model_epoch_50.pth",
        batch_size=1,
        num_workers=4
    )

"""# **SUPERPOINT**"""

# %BANNER_BEGIN%
# ---------------------------------------------------------------------
# %COPYRIGHT_BEGIN%
#
#  Magic Leap, Inc. ("COMPANY") CONFIDENTIAL
#
#  Unpublished Copyright (c) 2020
#  Magic Leap, Inc., All Rights Reserved.
#
# NOTICE:  All information contained herein is, and remains the property
# of COMPANY. The intellectual and technical concepts contained herein
# are proprietary to COMPANY and may be covered by U.S. and Foreign
# Patents, patents in process, and are protected by trade secret or
# copyright law.  Dissemination of this information or reproduction of
# this material is strictly forbidden unless prior written permission is
# obtained from COMPANY.  Access to the source code contained herein is
# hereby forbidden to anyone except current COMPANY employees, managers
# or contractors who have executed Confidentiality and Non-disclosure
# agreements explicitly covering such access.
#
# The copyright notice above does not evidence any actual or intended
# publication or disclosure  of  this source code, which includes
# information that is confidential and/or proprietary, and is a trade
# secret, of  COMPANY.   ANY REPRODUCTION, MODIFICATION, DISTRIBUTION,
# PUBLIC  PERFORMANCE, OR PUBLIC DISPLAY OF OR THROUGH USE  OF THIS
# SOURCE CODE  WITHOUT THE EXPRESS WRITTEN CONSENT OF COMPANY IS
# STRICTLY PROHIBITED, AND IN VIOLATION OF APPLICABLE LAWS AND
# INTERNATIONAL TREATIES.  THE RECEIPT OR POSSESSION OF  THIS SOURCE
# CODE AND/OR RELATED INFORMATION DOES NOT CONVEY OR IMPLY ANY RIGHTS
# TO REPRODUCE, DISCLOSE OR DISTRIBUTE ITS CONTENTS, OR TO MANUFACTURE,
# USE, OR SELL ANYTHING THAT IT  MAY DESCRIBE, IN WHOLE OR IN PART.
#
# %COPYRIGHT_END%
# ----------------------------------------------------------------------
# %AUTHORS_BEGIN%
#
#  Originating Authors: Paul-Edouard Sarlin
#
# %AUTHORS_END%
# --------------------------------------------------------------------*/
# %BANNER_END%

import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2
import pickle
import matplotlib.pyplot as plt
from tqdm import tqdm
import time
import math
import warnings

warnings.filterwarnings("ignore")

# =======================================
# Data Preprocessing Utilities
# =======================================

def compute_gradients(image):
    """
    Compute gradient magnitude and angle using Sobel filters.
    """
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    gray = gray.astype(np.float32) / 255.0

    grad_x = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)
    grad_y = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)
    grad_mag = np.sqrt(grad_x**2 + grad_y**2)
    angle_map = np.arctan2(grad_y, grad_x) * 180 / np.pi
    angle_map = (angle_map + 180) % 180  # Normalize to [0, 180)

    return grad_x, grad_y, grad_mag, angle_map

def region_grow(grad_x, grad_y, angle_map, tolerance=22.5):
    """
    Perform region growing based on gradient orientations.
    """
    h, w = angle_map.shape
    status = np.zeros((h, w), dtype=np.uint8)  # 0: Not Used, 1: Used
    regions = []

    # Sort pixels by gradient magnitude (descending)
    grad_mag = np.sqrt(grad_x**2 + grad_y**2)
    sorted_indices = np.argsort(-grad_mag, axis=None)

    for idx in sorted_indices:
        y, x = divmod(idx, w)
        if status[y, x] != 0 or grad_mag[y, x] < 1e-3:
            continue
        current_angle = angle_map[y, x]
        region = [(x, y)]
        status[y, x] = 1
        queue = [(x, y)]

        while queue:
            cx, cy = queue.pop(0)
            neighbors = [
                (nx, ny)
                for nx in range(cx - 1, cx + 2)
                for ny in range(cy - 1, cy + 2)
                if 0 <= nx < w and 0 <= ny < h and (nx != cx or ny != cy)
            ]
            for nx, ny in neighbors:
                if status[ny, nx] == 0 and abs(angle_map[ny, nx] - current_angle) < tolerance:
                    status[ny, nx] = 1
                    region.append((nx, ny))
                    queue.append((nx, ny))

        regions.append(region)

    return regions

def approximate_rectangles(regions):
    """
    Approximate each region with a rectangle.
    """
    rectangles = []
    for region in regions:
        if len(region) < 2:
            continue
        pts = np.array(region, dtype=np.float32)
        rect = cv2.minAreaRect(pts)
        (center_x, center_y), (width, length), angle = rect

        # Ensure width >= length
        if width < length:
            width, length = length, width
            angle += 90

        rectangles.append({
            'center': (center_x, center_y),
            'angle': angle,
            'length': length,
            'width': width,
            'points': pts
        })

    return rectangles

def compute_nfa(rectangle, image_size, p=0.125, epsilon=1):
    """
    Compute the Number of False Alarms (NFA) for a rectangle using logarithms to prevent overflow.
    Returns log_nfa for comparison.
    """
    num_potential = image_size[0] ** 5

    n = int(rectangle['length'] * rectangle['width'])
    k = len(rectangle['points'])
    if n < k:
        return float('inf')  # Impossible case

    try:
        log_comb = math.lgamma(n + 1) - math.lgamma(k + 1) - math.lgamma(n - k + 1)
    except OverflowError:
        return float('inf')

    try:
        log_prob = k * math.log(p) + (n - k) * math.log(1 - p)
    except ValueError:
        return float('inf')

    # Compute log(NFA)
    log_nfa = math.log(num_potential) + log_comb + log_prob

    return log_nfa

def validate_rectangles(rectangles, image_size, epsilon=1):
    """
    Validate rectangles based on NFA.
    Returns a list of valid rectangles.
    """
    valid_rects = []
    log_epsilon = math.log(epsilon)
    for rect in rectangles:
        log_nfa = compute_nfa(rect, image_size, p=0.125, epsilon=epsilon)
        if log_nfa < log_epsilon:
            valid_rects.append(rect)
    return valid_rects

# =======================================
# Dataset Class
# =======================================

class WireframeDataset(Dataset):
    def __init__(self, v1_1_path, pointlines_path, split='train', img_size=256, nfa_threshold=1):
        self.img_size = img_size
        self.v1_1_path = Path(v1_1_path)
        self.pointlines_path = Path(pointlines_path)
        self.split = split
        self.nfa_threshold = nfa_threshold

        # Get image list
        self.img_dir = self.v1_1_path / split
        self.img_paths = list(self.img_dir.glob('*.jpg'))

        # Define data augmentation
        self.transform = A.Compose([
            A.Resize(img_size, img_size),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            ),
            ToTensorV2()
        ])

        print(f"Found {len(self.img_paths)} images in {split} set")

    def generate_heatmap(self, points, shape):
        """Generate Gaussian heatmap for keypoints."""
        heatmap = np.zeros(shape, dtype=np.float32)
        if len(points) == 0:
            return heatmap

        for (x, y) in points:
            if not (0 <= x < shape[1] and 0 <= y < shape[0]):
                continue

            sigma = 2
            size = int(6 * sigma + 1)
            x0 = np.arange(0, size, 1, float)
            y0 = x0[:, np.newaxis]
            x0 = x0 - size // 2
            y0 = y0 - size // 2
            gaussian = np.exp(- (x0 ** 2 + y0 ** 2) / (2 * sigma ** 2))

            x, y = int(x), int(y)
            x1, y1 = max(0, x - size // 2), max(0, y - size // 2)
            x2, y2 = min(shape[1], x + size // 2 + 1), min(shape[0], y + size // 2 + 1)
            gx1, gy1 = max(0, size // 2 - x), max(0, size // 2 - y)
            gx2, gy2 = gx1 + (x2 - x1), gy1 + (y2 - y1)

            heatmap[y1:y2, x1:x2] = np.maximum(
                heatmap[y1:y2, x1:x2],
                gaussian[gy1:gy2, gx1:gx2]
            )

        return heatmap

    def generate_line_heatmap(self, lines, shape):
        """Generate heatmap for lines."""
        heatmap = np.zeros(shape, dtype=np.float32)
        if len(lines) == 0:
            return heatmap

        for (x1, y1, x2, y2) in lines:
            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])
            cv2.line(heatmap, (x1, y1), (x2, y2), 1, thickness=1)

        return heatmap

    def __getitem__(self, idx):
        img_path = str(self.img_paths[idx])
        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image not found: {img_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w = image.shape[:2]

        img_name = self.img_paths[idx].stem
        pkl_path = self.pointlines_path / f"{img_name}.pkl"

        if not pkl_path.exists():
            raise FileNotFoundError(f"Pkl file not found: {pkl_path}")
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)

        points = np.array(data['points'])  # [N, 2]
        lines = np.array(data['lines'])    # [M, 2] (indices of points)

        # Convert lines from point indices to coordinates
        if lines.size > 0:
            lines_coords = []
            for line in lines:
                idx1, idx2 = line.astype(int)
                if idx1 >= len(points) or idx2 >= len(points):
                    continue
                x1, y1 = points[idx1]
                x2, y2 = points[idx2]
                lines_coords.append([x1, y1, x2, y2])
            lines = np.array(lines_coords)
        else:
            lines = np.zeros((0, 4))

        # Scale coordinates
        scale_x = self.img_size / w
        scale_y = self.img_size / h
        points[:, 0] *= scale_x
        points[:, 1] *= scale_y
        if lines.size > 0:
            lines[:, [0, 2]] *= scale_x
            lines[:, [1, 3]] *= scale_y

        keypoint_heatmap = self.generate_heatmap(points, (self.img_size, self.img_size))
        resized_image = cv2.resize(image, (self.img_size, self.img_size))
        grad_x, grad_y, grad_mag, angle_map = compute_gradients(resized_image)

        regions = region_grow(grad_x, grad_y, angle_map, tolerance=22.5)
        rectangles = approximate_rectangles(regions)
        valid_rects = validate_rectangles(rectangles, (self.img_size, self.img_size), epsilon=self.nfa_threshold)

        line_heatmap_lsd = np.zeros((self.img_size, self.img_size), dtype=np.float32)
        for rect in valid_rects:
            box = cv2.boxPoints((
                (rect['center'][0], rect['center'][1]),
                (rect['length'], rect['width']),
                rect['angle']
            ))
            box = np.int0(box)
            cv2.polylines(line_heatmap_lsd, [box], isClosed=True, color=1, thickness=1)

        # Convert image to grayscale for SuperPoint
        gray_image = cv2.cvtColor(resized_image, cv2.COLOR_RGB2GRAY)
        augmented = self.transform(image=resized_image)
        image_tensor = augmented['image']
        angle_map = angle_map.astype(np.int64)

        # Normalize grayscale image and convert to tensor
        gray_augmented = A.Compose([
            A.Normalize(
                mean=[0.485],
                std=[0.229]
            ),
            ToTensorV2()
        ])(image=gray_image)
        gray_tensor = gray_augmented['image']  # [1, H, W]

        return {
            'image_rgb': image_tensor,  # For potential other uses
            'image_gray': gray_tensor,  # For SuperPoint
            'keypoint_heatmap': torch.from_numpy(keypoint_heatmap)[None],
            'line_heatmap': torch.from_numpy(line_heatmap_lsd)[None],
            'points': torch.from_numpy(points.astype(np.float32)),
            'lines': torch.from_numpy(lines.astype(np.float32)),
            'angle_map': torch.from_numpy(angle_map),
            'path': img_path
        }

    def __len__(self):
        return len(self.img_paths)

def custom_collate_fn(batch):
    images_rgb = torch.stack([item['image_rgb'] for item in batch])
    images_gray = torch.stack([item['image_gray'] for item in batch])
    keypoint_heatmaps = torch.stack([item['keypoint_heatmap'] for item in batch])
    line_heatmaps = torch.stack([item['line_heatmap'] for item in batch])

    max_points = max([len(item['points']) for item in batch])
    max_lines = max([len(item['lines']) for item in batch])
    points_padded = []
    lines_padded = []
    for item in batch:
        points = item['points']
        lines = item['lines']
        points_padding = torch.zeros((max_points - len(points), 2), dtype=points.dtype)
        lines_padding = torch.zeros((max_lines - len(lines), 4), dtype=lines.dtype)
        points_padded.append(torch.cat([points, points_padding]))
        lines_padded.append(torch.cat([lines, lines_padding]))
    points = torch.stack(points_padded)
    lines = torch.stack(lines_padded)

    angle_maps = torch.stack([item['angle_map'] for item in batch])
    paths = [item['path'] for item in batch]

    return {
        'image_rgb': images_rgb,
        'image_gray': images_gray,
        'keypoint_heatmap': keypoint_heatmaps,
        'line_heatmap': line_heatmaps,
        'points': points,
        'lines': lines,
        'angle_map': angle_maps,
        'path': paths
    }

def create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=4):
    train_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='train',
        img_size=256,
        nfa_threshold=1
    )

    val_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='test',
        img_size=256,
        nfa_threshold=1
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )

    return train_loader, val_loader

# =======================================
# SuperPoint Network
# =======================================

class SuperPoint(nn.Module):
    """SuperPoint Convolutional Detector and Descriptor

    SuperPoint: Self-Supervised Interest Point Detection and
    Description. Daniel DeTone, Tomasz Malisiewicz, and Andrew
    Rabinovich. In CVPRW, 2019. https://arxiv.org/abs/1712.07629

    """
    default_config = {
        'descriptor_dim': 256,
        'nms_radius': 4,
        'keypoint_threshold': 0.005,
        'max_keypoints': -1,
        'remove_borders': 4,
    }

    def __init__(self, config={}):
        super().__init__()
        self.config = {**self.default_config, **config}

        self.relu = nn.ReLU(inplace=True)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        c1, c2, c3, c4, c5 = 64, 64, 128, 128, 256

        self.conv1a = nn.Conv2d(1, c1, kernel_size=3, stride=1, padding=1)
        self.conv1b = nn.Conv2d(c1, c1, kernel_size=3, stride=1, padding=1)
        self.conv2a = nn.Conv2d(c1, c2, kernel_size=3, stride=1, padding=1)
        self.conv2b = nn.Conv2d(c2, c2, kernel_size=3, stride=1, padding=1)
        self.conv3a = nn.Conv2d(c2, c3, kernel_size=3, stride=1, padding=1)
        self.conv3b = nn.Conv2d(c3, c3, kernel_size=3, stride=1, padding=1)
        self.conv4a = nn.Conv2d(c3, c4, kernel_size=3, stride=1, padding=1)
        self.conv4b = nn.Conv2d(c4, c4, kernel_size=3, stride=1, padding=1)

        # Change convPb to output 1 channel for binary keypoint detection
        self.convPa = nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)
        self.convPb = nn.Conv2d(c5, 1, kernel_size=1, stride=1, padding=0)

        self.convDa = nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)
        self.convDb = nn.Conv2d(
            c5, self.config['descriptor_dim'],
            kernel_size=1, stride=1, padding=0)

        # Initialize weights (assuming pretrained weights are not available)
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

        mk = self.config['max_keypoints']
        if mk == 0 or mk < -1:
            raise ValueError('\"max_keypoints\" must be positive or \"-1\"')

        print('Initialized SuperPoint model')

    def forward(self, data):
        """ Compute keypoints, scores, descriptors for image """
        # Shared Encoder
        x = self.relu(self.conv1a(data['image_gray']))
        x = self.relu(self.conv1b(x))
        x = self.pool(x)
        x = self.relu(self.conv2a(x))
        x = self.relu(self.conv2b(x))
        x = self.pool(x)
        x = self.relu(self.conv3a(x))
        x = self.relu(self.conv3b(x))
        x = self.pool(x)
        x = self.relu(self.conv4a(x))
        x = self.relu(self.conv4b(x))

        # Compute the dense keypoint scores (raw logits)
        cPa = self.relu(self.convPa(x))
        raw_scores = self.convPb(cPa)  # [B,1,H,W]

        # Compute keypoints for inference
        # Remove softmax and NMS for training
        scores = torch.sigmoid(raw_scores)  # Apply sigmoid for probability

        if not self.training:
            # NMS and keypoint extraction (only for inference)
            keypoints = []
            for i in range(scores.shape[0]):
                # Detach the tensor before moving to CPU and converting to NumPy
                score_map = scores[i, 0].detach().cpu().numpy()
                keypoints_i = np.argwhere(score_map > self.config['keypoint_threshold'])
                keypoints_i = keypoints_i[:, [1, 0]]  # (x, y)
                # Apply NMS if needed (can be implemented here)
                keypoints_i = torch.tensor(keypoints_i, dtype=torch.float32)
                keypoints.append(keypoints_i)
        else:
            keypoints = None

        # Compute the dense descriptors
        cDa = self.relu(self.convDa(x))
        descriptors = self.convDb(cDa)
        descriptors = torch.nn.functional.normalize(descriptors, p=2, dim=1)

        return {
            'keypoints': keypoints,          # List of keypoints per image in batch, None during training
            'scores': scores,                # [B,1,H,W]
            'raw_scores': raw_scores,        # [B,1,H,W] for loss
            'descriptors': descriptors,      # [B,D,H',W']
        }

# =======================================
# Helper Functions for SuperPoint
# =======================================

def simple_nms(scores, nms_radius: int):
    """ Fast Non-maximum suppression to remove nearby points """
    assert(nms_radius >= 0)

    def max_pool(x):
        return torch.nn.functional.max_pool2d(
            x, kernel_size=nms_radius*2+1, stride=1, padding=nms_radius)

    zeros = torch.zeros_like(scores)
    max_mask = scores == max_pool(scores)
    for _ in range(2):
        supp_mask = max_pool(max_mask.float()) > 0
        supp_scores = torch.where(supp_mask, zeros, scores)
        new_max_mask = supp_scores == max_pool(supp_scores)
        max_mask = max_mask | (new_max_mask & (~supp_mask))
    return torch.where(max_mask, scores, zeros)


def remove_borders(keypoints, scores, border: int, height: int, width: int):
    """ Removes keypoints too close to the border """
    mask_h = (keypoints[:, 0] >= border) & (keypoints[:, 0] < (height - border))
    mask_w = (keypoints[:, 1] >= border) & (keypoints[:, 1] < (width - border))
    mask = mask_h & mask_w
    return keypoints[mask], scores[mask]


def top_k_keypoints(keypoints, scores, k: int):
    if k >= len(keypoints):
        return keypoints, scores
    scores, indices = torch.topk(scores, k, dim=0)
    return keypoints[indices], scores


def sample_descriptors(keypoints, descriptors, s: int = 8):
    """ Interpolate descriptors at keypoint locations """
    b, c, h, w = descriptors.shape
    keypoints = keypoints - s / 2 + 0.5
    keypoints /= torch.tensor([(w*s - s/2 - 0.5), (h*s - s/2 - 0.5)],
                              ).to(keypoints)[None]
    keypoints = keypoints*2 - 1  # normalize to (-1, 1)
    args = {'align_corners': True} if torch.__version__ >= '1.3' else {}
    descriptors = torch.nn.functional.grid_sample(
        descriptors, keypoints.view(b, 1, -1, 2), mode='bilinear', **args)
    descriptors = torch.nn.functional.normalize(
        descriptors.reshape(b, c, -1), p=2, dim=1)
    return descriptors

# =======================================
# Metrics
# =======================================

def calculate_sap(pred_points, gt_points, threshold=3.0):
    if len(pred_points) == 0 or len(gt_points) == 0:
        return 0.0
    distances = torch.cdist(pred_points.float(), gt_points.float())
    min_distances, _ = torch.min(distances, dim=1)
    correct_matches = (min_distances < threshold).float()

    precision = correct_matches.sum() / len(pred_points) if len(pred_points) > 0 else 0
    recall = correct_matches.sum() / len(gt_points) if len(gt_points) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)
    return f1.item()

def calculate_lap(pred_lines, gt_lines, dist_threshold=5.0, angle_threshold=10.0):
    if len(pred_lines) == 0 or len(gt_lines) == 0:
        return 0.0

    def compute_line_similarity(line1, line2):
        line1 = line1.float()
        line2 = line2.float()

        dist1 = torch.sqrt(((line1[:2] - line2[:2]) ** 2).sum())
        dist2 = torch.sqrt(((line1[2:] - line2[:2]) ** 2).sum())

        vec1 = line1[2:] - line1[:2]
        vec2 = line2[2:] - line2[:2]

        norm1 = torch.sqrt((vec1 * vec1).sum())
        norm2 = torch.sqrt((vec2 * vec2).sum())

        cos_angle = (vec1 * vec2).sum() / (norm1 * norm2 + 1e-8)
        angle = torch.acos(torch.clamp(cos_angle, -1.0, 1.0)) * 180 / np.pi
        return dist1.item(), angle.item()

    matches = 0
    matched_gt = set()
    for pred_line in pred_lines:
        best_dist = float('inf')
        best_angle = float('inf')
        best_gt = -1

        for idx, gt_line in enumerate(gt_lines):
            if idx in matched_gt:
                continue
            dist, angle = compute_line_similarity(pred_line, gt_line)
            if dist < best_dist:
                best_dist = dist
                best_angle = angle
                best_gt = idx

        if best_dist < dist_threshold and best_angle < angle_threshold:
            matches += 1
            matched_gt.add(best_gt)

    precision = matches / len(pred_lines) if len(pred_lines) > 0 else 0
    recall = matches / len(gt_lines) if len(gt_lines) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)
    return f1

# =======================================
# Training and Evaluation
# =======================================

def train_model(v1_1_path, pointlines_path, model_save_path="superpoint_model", batch_size=8, num_workers=4, num_epochs=20):
    train_loader, val_loader = create_dataloader(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        batch_size=batch_size,
        num_workers=num_workers
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SuperPoint().to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
    criterion_keypoint = nn.BCEWithLogitsLoss()

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            images_gray = batch['image_gray'].to(device)
            keypoint_targets = batch['keypoint_heatmap'].to(device)  # [B,1,256,256]

            optimizer.zero_grad()
            outputs = model({'image_gray': images_gray})

            pred_keypoint_logits = outputs['raw_scores']  # [B,1,H_out,W_out], e.g. [B,1,32,32]

            # Downsample the target heatmap to match the prediction size
            # Use nearest or bilinear interpolation. Nearest is commonly used for heatmaps.
            keypoint_targets_resized = F.interpolate(keypoint_targets, size=pred_keypoint_logits.shape[-2:], mode='nearest')

            # Compute keypoint detection loss at the downsampled resolution
            loss_keypoint = criterion_keypoint(pred_keypoint_logits, keypoint_targets_resized)

            # Descriptor loss
            # Since we don't have ground truth descriptors, this part is skipped
            # Alternatively, implement a self-supervised descriptor loss if applicable
            loss_descriptor = torch.tensor(0.0).to(device)

            # Total loss
            loss = loss_keypoint + loss_descriptor
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        scheduler.step()
        avg_loss = running_loss / len(train_loader)
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")
        torch.save(model.state_dict(), f"{model_save_path}_epoch_{epoch+1}.pth")

    print("Training completed.")

def evaluate_model(v1_1_path, pointlines_path, model_path="superpoint_model_epoch_20.pth", batch_size=1, num_workers=4):
    _, val_loader = create_dataloader(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        batch_size=batch_size,
        num_workers=num_workers
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SuperPoint().to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    sap_scores = []
    lap_scores = []
    fps_times = []

    print("Starting evaluation...")
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(val_loader, desc="Evaluating")):
            images_gray = batch['image_gray'].to(device)
            gt_points = batch['points'][0]
            gt_lines = batch['lines'][0]
            angle_maps = batch['angle_map'].to(device)

            start_time = time.time()
            outputs = model({'image_gray': images_gray})
            inference_time = time.time() - start_time
            fps = 1.0 / inference_time if inference_time > 0 else 0.0
            fps_times.append(fps)

            # Keypoint detection
            keypoint_output = torch.sigmoid(outputs['raw_scores'])  # [B,1,H,W]
            keypoint_map = keypoint_output.cpu().squeeze().numpy()
            pred_points_y, pred_points_x = np.where(keypoint_map > 0.5)
            pred_points = torch.tensor(np.stack([pred_points_x, pred_points_y], axis=1))

            # Descriptor extraction (if needed)
            # pred_descriptors = outputs['descriptors']  # [B,D,H',W']

            sap = calculate_sap(pred_points, gt_points)
            lap = calculate_lap(torch.zeros((0, 4)), gt_lines)  # Line evaluation not applicable
            sap_scores.append(sap)
            lap_scores.append(lap)

            if batch_idx % 50 == 0:
                image_np = batch['image_rgb'].cpu().squeeze().permute(1, 2, 0).numpy()
                image_np = (image_np * np.array([0.229, 0.224, 0.225]) +
                            np.array([0.485, 0.456, 0.406]))
                image_np = np.clip(image_np, 0, 1)

                plt.figure(figsize=(15, 5))

                plt.subplot(131)
                plt.imshow(image_np)
                plt.title('Input Image')
                plt.axis('off')

                plt.subplot(132)
                plt.imshow(keypoint_map, cmap='jet')
                plt.plot(pred_points[:, 0], pred_points[:, 1], 'r.', markersize=2)
                plt.title(f'Keypoints (num={len(pred_points)})')
                plt.axis('off')

                plt.subplot(133)
                plt.imshow(batch['line_heatmap'].cpu().squeeze().numpy(), cmap='gray')
                plt.title(f'Lines (num={len(gt_lines)})')
                plt.axis('off')

                plt.tight_layout()
                plt.show()

    mean_sap = np.mean(sap_scores) if len(sap_scores) > 0 else 0.0
    mean_lap = np.mean(lap_scores) if len(lap_scores) > 0 else 0.0
    mean_fps = np.mean(fps_times) if len(fps_times) > 0 else 0.0

    print("\nEvaluation Results:")
    print(f"SAP (Structure-Aware Performance - F1 Score): {mean_sap:.4f}")
    print(f"LAP (Line-Aware Performance - F1 Score): {mean_lap:.4f}")
    print(f"Average FPS: {mean_fps:.2f}")

# =======================================
# Main Execution
# =======================================

if __name__ == "__main__":
    # Please modify the dataset paths accordingly
    v1_1_path = "/content/drive/MyDrive/v1.1"
    pointlines_path = "/content/drive/MyDrive/pointlines"

    if not os.path.exists(v1_1_path):
        raise FileNotFoundError(f"v1.1_path not found: {v1_1_path}")
    if not os.path.exists(pointlines_path):
        raise FileNotFoundError(f"pointlines_path not found: {pointlines_path}")

    print("Starting Training...")
    train_model(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        model_save_path="superpoint_model",
        batch_size=8,
        num_workers=4,
        num_epochs=20
    )

    print("\nStarting Evaluation...")
    evaluate_model(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        model_path="superpoint_model_epoch_20.pth",
        batch_size=1,
        num_workers=4
    )









import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2
import pickle
import matplotlib.pyplot as plt
from tqdm import tqdm

# =======================================
# 数据预处理部分
# =======================================

class WireframeDataset(Dataset):
    def __init__(self, v1_1_path, pointlines_path, split='train', img_size=128):
        self.img_size = img_size
        self.v1_1_path = Path(v1_1_path)
        self.pointlines_path = Path(pointlines_path)
        self.split = split

        self.img_dir = self.v1_1_path / split
        self.img_paths = list(self.img_dir.glob('*.jpg'))

        self.transform = A.Compose([
            A.Resize(img_size, img_size),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])

        print(f"Found {len(self.img_paths)} images in {split} set")

    def __getitem__(self, idx):
        img_path = str(self.img_paths[idx])
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        augmented = self.transform(image=image)
        image = augmented['image']

        return {'image': image, 'path': img_path}

    def __len__(self):
        return len(self.img_paths)


def create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=2, img_size=128):
    train_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='train',
        img_size=img_size
    )

    val_dataset = WireframeDataset(
        v1_1_path=v1_1_path,
        pointlines_path=pointlines_path,
        split='test',
        img_size=img_size
    )

    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers
    )

    val_loader = DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers
    )

    return train_loader, val_loader

# =======================================
# 网络定义部分
# =======================================

# SDF-Fusion模块：融合浅层和深层特征
# SDF-Fusion模块：融合浅层和深层特征
class SDFFusion(nn.Module):
    def __init__(self, low_channels, high_channels, out_channels):
        super(SDFFusion, self).__init__()
        self.low_conv = nn.Conv2d(low_channels, out_channels, kernel_size=1)
        self.high_conv = nn.Conv2d(high_channels, out_channels, kernel_size=1)
        self.fusion_conv = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.upsample = nn.Upsample(scale_factor=16, mode='bilinear', align_corners=False)

    def forward(self, low_features, high_features):
        low = self.low_conv(low_features)
        high = self.upsample(high_features)
        high = self.high_conv(high)
        fused = self.fusion_conv(low + high)
        return self.relu(self.bn(fused))



# 边界感知损失
class EdgeAwareLoss(nn.Module):
    def __init__(self):
        super(EdgeAwareLoss, self).__init__()

    def forward(self, predicted, target):
        predicted_grad_v = torch.abs(predicted[:, :, :-1, :] - predicted[:, :, 1:, :])
        predicted_grad_h = torch.abs(predicted[:, :, :, :-1] - predicted[:, :, :, 1:])
        target_grad_v = torch.abs(target[:, :, :-1, :] - target[:, :, 1:, :])
        target_grad_h = torch.abs(target[:, :, :, :-1] - target[:, :, :, 1:])

        return F.l1_loss(predicted_grad_v, target_grad_v) + F.l1_loss(predicted_grad_h, target_grad_h)


# 主干网络
class LightweightMultiTaskNetwork(nn.Module):
    def __init__(self):
        super(LightweightMultiTaskNetwork, self).__init__()
        mobilenet = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1)

        self.initial_conv = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False)
        self.initial_bn = nn.BatchNorm2d(16)
        self.initial_relu = nn.ReLU(inplace=True)

        self.features = nn.Sequential(*list(mobilenet.features.children())[1:])

        self.sdf_fusion = SDFFusion(low_channels=16, high_channels=960, out_channels=256)

        self.keypoint_head = nn.Conv2d(256, 1, kernel_size=1)
        self.line_head = nn.Conv2d(256, 1, kernel_size=1)

    def forward(self, x):
        x = self.initial_relu(self.initial_bn(self.initial_conv(x)))

        low_features = x
        high_features = self.features(x)

        fused_features = self.sdf_fusion(low_features, high_features)

        keypoint_output = self.keypoint_head(fused_features)
        line_output = self.line_head(fused_features)

        return keypoint_output, line_output


# =======================================
# 训练和评估部分
# =======================================

def train_model():
    img_size_low, img_size_high = 128, 256  # 渐进式学习：低分辨率到高分辨率
    train_loader, val_loader = create_dataloader(
        "/content/drive/MyDrive/v1.1", "/content/drive/MyDrive/pointlines", batch_size=8, img_size=img_size_low
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = LightweightMultiTaskNetwork().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    best_loss = float('inf')
    for epoch in range(50):
        model.train()
        running_loss = 0.0

        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/50"):
            images = batch['image'].to(device)

            optimizer.zero_grad()
            keypoint_output, line_output = model(images)

            # 假设 keypoint_output 和 line_output 的目标是随机生成的
            target_keypoints = torch.rand_like(keypoint_output).to(device)
            target_lines = torch.rand_like(line_output).to(device)

            loss_keypoint = F.mse_loss(keypoint_output, target_keypoints)
            loss_line = EdgeAwareLoss()(line_output, target_lines)
            loss = loss_keypoint + loss_line
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        avg_loss = running_loss / len(train_loader)
        print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")

        # 保存模型
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(model.state_dict(), "best_model.pth")
            print(f"Model saved at epoch {epoch+1} with loss {avg_loss:.4f}")

    # 在训练结束后保存最终模型
    torch.save(model.state_dict(), "final_model.pth")
    print("Training complete. Final model saved.")

def evaluate_model():
    _, val_loader = create_dataloader(
        "/content/drive/MyDrive/v1.1", "/content/drive/MyDrive/pointlines", batch_size=1, img_size=256
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = LightweightMultiTaskNetwork().to(device)
    model.load_state_dict(torch.load("best_model.pth"))
    model.eval()

    with torch.no_grad():
        for batch in val_loader:
            images = batch['image'].to(device)
            keypoint_output, line_output = model(images)

            # 可视化结果
            image = images.cpu().squeeze().permute(1, 2, 0).numpy()
            keypoint_heatmap = keypoint_output.cpu().squeeze().numpy()
            line_heatmap = line_output.cpu().squeeze().numpy()

            # 绘制原始图像
            plt.subplot(1, 3, 1)
            plt.imshow(image)
            plt.title('Original Image')

            # 绘制关键点热力图
            plt.subplot(1, 3, 2)
            plt.imshow(keypoint_heatmap, cmap='jet')
            plt.title('Keypoint Heatmap')

            # 绘制线段热力图
            plt.subplot(1, 3, 3)
            plt.imshow(line_heatmap, cmap='jet')
            plt.title('Line Heatmap')

            # 调整子图间距
            plt.tight_layout()
            plt.show()

if __name__ == "__main__":
    train_model()
    evaluate_model()

# Assuming `model` is an instance of LightweightMultiTaskNetwork
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"Total Parameters: {total_params}")
print(f"Trainable Parameters: {trainable_params}")

















"""# **轻量级网络**"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2
import numpy as np
from pathlib import Path
from tqdm import tqdm
import json

# Efficient Basic Layer using Depthwise Separable Convolution
class EfficientLayer(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.layer = nn.Sequential(
            # Depthwise convolution
            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True),
            # Pointwise convolution
            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.layer(x)

# Simplified Dynamic Feature Enhancement Module
class LightweightDynamicFeatureEnhancement(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.fc = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, channels // 4, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // 4, channels, kernel_size=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        weights = self.fc(x)
        return x * weights

# Simplified Multi-Scale Context Module
class LightweightMultiScaleContext(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        mid_channels = out_channels // 4
        self.branch1 = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=1, padding=0),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
        )
        self.branch2 = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, groups=mid_channels),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
        )
        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=5, padding=2, groups=mid_channels),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
        )
        self.branch4 = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, dilation=2, padding=2, groups=mid_channels),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
        )
        self.fusion = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=1, padding=0),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        b1 = self.branch1(x)
        b2 = self.branch2(x)
        b3 = self.branch3(x)
        b4 = self.branch4(x)
        out = torch.cat([b1, b2, b3, b4], dim=1)
        return self.fusion(out)

# Descriptor Output Head
class DescriptorHead(nn.Module):
    def __init__(self, in_channels, descriptor_dim=128):
        super().__init__()
        self.descriptor_head = nn.Sequential(
            nn.Conv2d(in_channels, descriptor_dim, kernel_size=1, stride=1),
            nn.BatchNorm2d(descriptor_dim),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        x = self.descriptor_head(x)
        return F.normalize(x, p=2, dim=1)  # L2 normalization

# Main Model
class LightweightXFeatModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.norm = nn.InstanceNorm2d(1)

        # Initial Convolution
        self.initial_conv = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
        )

        # Efficient Blocks
        self.block1 = EfficientLayer(16, 24, stride=2)
        self.block2 = EfficientLayer(24, 32, stride=2)
        self.block3 = EfficientLayer(32, 64, stride=2)

        # Lightweight Multi-Scale Context
        self.multi_scale_context = LightweightMultiScaleContext(64, 64)

        # Further Efficient Blocks
        self.block4 = EfficientLayer(64, 96, stride=2)
        self.block5 = EfficientLayer(96, 128, stride=2)

        # Fusion Layer
        self.block_fusion = nn.Sequential(
            nn.Conv2d(64 + 96 + 128, 64, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
        )

        # Lightweight Dynamic Feature Enhancement
        self.dynamic_feature_enhancement = LightweightDynamicFeatureEnhancement(64)

        # Output Heads
        self.heatmap_head = nn.Conv2d(64, 1, kernel_size=1)

        self.keypoint_head = nn.Conv2d(64, 65, kernel_size=1)

        self.descriptor_head = DescriptorHead(64, descriptor_dim=128)

    def forward(self, x):
        # Ensure input is grayscale
        if x.shape[1] != 1:
            x = x.mean(dim=1, keepdim=True)
        x = self.norm(x)

        x = self.initial_conv(x)
        x1 = self.block1(x)
        x2 = self.block2(x1)
        x3 = self.block3(x2)

        x3 = self.multi_scale_context(x3)

        x4 = self.block4(x3)
        x5 = self.block5(x4)

        # Upsample and concatenate features
        x4_up = F.interpolate(x4, size=x3.shape[2:], mode='bilinear', align_corners=False)
        x5_up = F.interpolate(x5, size=x3.shape[2:], mode='bilinear', align_corners=False)
        fusion_input = torch.cat([x3, x4_up, x5_up], dim=1)
        feats = self.block_fusion(fusion_input)

        feats = self.dynamic_feature_enhancement(feats)

        # Outputs
        heatmap = self.heatmap_head(feats)  # No activation, logits output
        keypoints = self.keypoint_head(feats)
        descriptors = self.descriptor_head(feats)
        return feats, keypoints, heatmap, descriptors

# Descriptor Contrastive Loss
class DescriptorContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0, img_size=512, output_size=64):
        super().__init__()
        self.margin = margin
        self.img_size = img_size
        self.output_size = output_size

    def forward(self, descriptors, keypoints_batch):
        # descriptors: [B, C, H, W]
        # keypoints_batch: list of keypoint coordinates for each image in the batch

        batch_size, C, H, W = descriptors.size()
        device = descriptors.device
        loss = torch.tensor(0.0, requires_grad=True).to(device)
        count = 0

        for i in range(batch_size):
            desc = descriptors[i]  # [C, H, W]
            kp = keypoints_batch[i]  # List of (x, y) tuples

            # Get descriptor vectors at keypoint locations
            pos_descriptors = []
            for x, y in kp:
                x = int(x * self.output_size / self.img_size)
                y = int(y * self.output_size / self.img_size)
                if 0 <= x < W and 0 <= y < H:
                    pos_descriptors.append(desc[:, y, x])

            if len(pos_descriptors) < 2:
                continue  # Not enough keypoints to compute loss

            pos_descriptors = torch.stack(pos_descriptors)  # [N, C]

            # Compute pairwise distances between descriptors
            pdist = torch.pdist(pos_descriptors, p=2)

            # Positive pairs (should be close)
            pos_loss = pdist.mean()

            # Negative pairs (should be far)
            # Sample random descriptors from other locations
            neg_descriptors = desc.view(C, -1).t()  # [H*W, C]
            neg_indices = torch.randperm(neg_descriptors.size(0))[:len(pos_descriptors)].to(device)
            neg_descriptors = neg_descriptors[neg_indices]

            # Compute distances between positive and negative descriptors
            neg_dists = torch.cdist(pos_descriptors, neg_descriptors, p=2)
            neg_loss = F.relu(self.margin - neg_dists).mean()

            loss += pos_loss + neg_loss
            count += 1

        if count > 0:
            loss = loss / count
        else:
            loss = torch.tensor(0.0, requires_grad=True).to(device)

        return loss

# Wireframe Dataset
class WireframeDataset(Dataset):
    def __init__(self, v1_1_path, pointlines_path, split='train', img_size=512, output_size=64):
        self.img_size = img_size
        self.output_size = output_size
        self.v1_1_path = Path(v1_1_path)
        self.pointlines_path = Path(pointlines_path)
        self.split = split

        self.img_dir = self.v1_1_path / split
        self.img_paths = list(self.img_dir.glob('*.jpg'))

        # Load keypoint annotations
        self.annotations = self.load_annotations()

        self.transform = A.Compose([
            A.Resize(img_size, img_size),
            A.Normalize(mean=[0.485], std=[0.229]),
            ToTensorV2()
        ])

        print(f"Found {len(self.img_paths)} images in {split} set")

    def load_annotations(self):
        # Implement logic to load annotations from files
        annotations = {}
        annotation_files = list((self.pointlines_path / self.split).glob('*.json'))
        for anno_file in annotation_files:
            with open(anno_file, 'r') as f:
                data = json.load(f)
                image_id = data['image_id']
                keypoints = data['keypoints']  # List of (x, y) tuples
                annotations[image_id] = keypoints
        return annotations

    def generate_heatmap(self, keypoints):
        heatmap = np.zeros((self.output_size, self.output_size), dtype=np.float32)
        for x, y in keypoints:
            x = int(x * self.output_size / self.img_size)
            y = int(y * self.output_size / self.img_size)
            if 0 <= x < self.output_size and 0 <= y < self.output_size:
                heatmap[y, x] = 1.0  # Set keypoint location to 1
        # Apply Gaussian blur
        heatmap = cv2.GaussianBlur(heatmap, (7, 7), 0)
        # Normalize heatmap
        heatmap = heatmap / heatmap.max() if heatmap.max() != 0 else heatmap
        return heatmap

    def __getitem__(self, idx):
        img_path = str(self.img_paths[idx])
        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        height, width = image.shape

        # Get image ID to retrieve annotations
        image_id = Path(img_path).stem
        keypoints = self.annotations.get(image_id, [])

        # Transform image
        transformed = self.transform(image=image)
        image = transformed["image"]

        # Generate heatmap
        heatmap = self.generate_heatmap(keypoints)
        heatmap = torch.tensor(heatmap, dtype=torch.float32).unsqueeze(0)

        # No need to scale keypoints here since scaling is handled in the loss function

        return {
            'image': image,
            'heatmap': heatmap,
            'keypoints': keypoints,  # Use original keypoints in loss function
            'path': img_path
        }

    def __len__(self):
        return len(self.img_paths)

# 自定义 collate_fn 函数
def custom_collate_fn(batch):
    images = torch.stack([item['image'] for item in batch])
    heatmaps = torch.stack([item['heatmap'] for item in batch])
    keypoints = [item['keypoints'] for item in batch]  # 保持为列表的列表
    paths = [item['path'] for item in batch]
    return {'image': images, 'heatmap': heatmaps, 'keypoints': keypoints, 'path': paths}

def create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=2):
    train_dataset = WireframeDataset(v1_1_path, pointlines_path, split='train')
    val_dataset = WireframeDataset(v1_1_path, pointlines_path, split='test')

    # 使用自定义的 collate_fn
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=custom_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate_fn)
    return train_loader, val_loader

# Main function
def main():
    v1_1_path = "/content/drive/MyDrive/v1.1"
    pointlines_path = "/content/drive/MyDrive/pointlines"

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    train_loader, val_loader = create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=2)

    model = LightweightXFeatModel().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    # Define loss functions
    heatmap_criterion = nn.BCEWithLogitsLoss()
    descriptor_criterion = DescriptorContrastiveLoss(img_size=512, output_size=64)

    num_epochs = 10
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            images = batch['image'].to(device)
            heatmaps_gt = batch['heatmap'].to(device)
            keypoints_gt = batch['keypoints']

            optimizer.zero_grad()
            feats, keypoints_pred, heatmap_pred, descriptors = model(images)

            # Heatmap loss
            heatmap_loss = heatmap_criterion(heatmap_pred, heatmaps_gt)

            # Descriptor loss
            descriptor_loss = descriptor_criterion(descriptors, keypoints_gt)

            # Total loss
            loss = heatmap_loss + descriptor_loss

            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")

        # 可选：在验证集上进行评估
        # evaluate(model, val_loader)

if __name__ == "__main__":
    main()

import matplotlib.pyplot as plt
import torch
import numpy as np

# 可视化热力图函数
def visualize_heatmap_from_model(model, val_loader, device):
    model.eval()  # 切换模型到评估模式

    with torch.no_grad():
        # 从验证集取出一张图片
        for batch in val_loader:
            images = batch['image'].to(device)  # 输入图片
            heatmaps_gt = batch['heatmap']  # 真实热力图
            paths = batch['path']  # 图片路径

            # 取第一张图片进行预测
            image = images[0].unsqueeze(0)
            heatmap_gt = heatmaps_gt[0].cpu().numpy().squeeze()
            image_path = paths[0]

            # 模型预测
            _, _, heatmap_pred, _ = model(image)

            # 提取预测的热力图
            heatmap_pred = heatmap_pred.sigmoid().cpu().numpy().squeeze()

            # 可视化
            fig, ax = plt.subplots(1, 3, figsize=(15, 5))
            ax[0].imshow(image.cpu().squeeze().numpy(), cmap='gray')
            ax[0].set_title("输入图像")
            ax[1].imshow(heatmap_gt, cmap='hot')
            ax[1].set_title("真实热力图")
            ax[2].imshow(heatmap_pred, cmap='hot')
            ax[2].set_title("预测热力图")
            plt.show()

            break  # 仅处理一张图片

# 加载验证集
v1_1_path = "/content/drive/MyDrive/v1.1"
pointlines_path = "/content/drive/MyDrive/pointlines"
batch_size = 8
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

_, val_loader = create_dataloader(v1_1_path, pointlines_path, batch_size=batch_size, num_workers=2)

# 使用当前模型实例进行热力图可视化
visualize_heatmap_from_model(model, val_loader, device)





"""# **包含评估和保存模型的代码**"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2
import numpy as np
from pathlib import Path
from tqdm import tqdm
import json
import os

# Efficient Basic Layer using Depthwise Separable Convolution
class EfficientLayer(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.layer = nn.Sequential(
            # Depthwise convolution
            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True),
            # Pointwise convolution
            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.layer(x)

# Simplified Dynamic Feature Enhancement Module
class LightweightDynamicFeatureEnhancement(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.fc = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, channels // 4, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // 4, channels, kernel_size=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        weights = self.fc(x)
        return x * weights

# Simplified Multi-Scale Context Module
class LightweightMultiScaleContext(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        mid_channels = out_channels // 4
        self.branch1 = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=1, padding=0),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
        )
        self.branch2 = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, groups=mid_channels),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
        )
        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=5, padding=2, groups=mid_channels),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
        )
        self.branch4 = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, dilation=2, padding=2, groups=mid_channels),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
        )
        self.fusion = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=1, padding=0),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        b1 = self.branch1(x)
        b2 = self.branch2(x)
        b3 = self.branch3(x)
        b4 = self.branch4(x)
        out = torch.cat([b1, b2, b3, b4], dim=1)
        return self.fusion(out)

# Descriptor Output Head
class DescriptorHead(nn.Module):
    def __init__(self, in_channels, descriptor_dim=128):
        super().__init__()
        self.descriptor_head = nn.Sequential(
            nn.Conv2d(in_channels, descriptor_dim, kernel_size=1, stride=1),
            nn.BatchNorm2d(descriptor_dim),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        x = self.descriptor_head(x)
        return F.normalize(x, p=2, dim=1)  # L2 normalization

# Main Model
class LightweightXFeatModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.norm = nn.InstanceNorm2d(1)

        # Initial Convolution
        self.initial_conv = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
        )

        # Efficient Blocks
        self.block1 = EfficientLayer(16, 24, stride=2)
        self.block2 = EfficientLayer(24, 32, stride=2)
        self.block3 = EfficientLayer(32, 64, stride=2)

        # Lightweight Multi-Scale Context
        self.multi_scale_context = LightweightMultiScaleContext(64, 64)

        # Further Efficient Blocks
        self.block4 = EfficientLayer(64, 96, stride=2)
        self.block5 = EfficientLayer(96, 128, stride=2)

        # Fusion Layer
        self.block_fusion = nn.Sequential(
            nn.Conv2d(64 + 96 + 128, 64, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
        )

        # Lightweight Dynamic Feature Enhancement
        self.dynamic_feature_enhancement = LightweightDynamicFeatureEnhancement(64)

        # Output Heads
        self.heatmap_head = nn.Conv2d(64, 1, kernel_size=1)

        self.keypoint_head = nn.Conv2d(64, 65, kernel_size=1)

        self.descriptor_head = DescriptorHead(64, descriptor_dim=128)

    def forward(self, x):
        # Ensure input is grayscale
        if x.shape[1] != 1:
            x = x.mean(dim=1, keepdim=True)
        x = self.norm(x)

        x = self.initial_conv(x)
        x1 = self.block1(x)
        x2 = self.block2(x1)
        x3 = self.block3(x2)

        x3 = self.multi_scale_context(x3)

        x4 = self.block4(x3)
        x5 = self.block5(x4)

        # Upsample and concatenate features
        x4_up = F.interpolate(x4, size=x3.shape[2:], mode='bilinear', align_corners=False)
        x5_up = F.interpolate(x5, size=x3.shape[2:], mode='bilinear', align_corners=False)
        fusion_input = torch.cat([x3, x4_up, x5_up], dim=1)
        feats = self.block_fusion(fusion_input)

        feats = self.dynamic_feature_enhancement(feats)

        # Outputs
        heatmap = self.heatmap_head(feats)  # No activation, logits output
        keypoints = self.keypoint_head(feats)
        descriptors = self.descriptor_head(feats)
        return feats, keypoints, heatmap, descriptors

# Descriptor Contrastive Loss with Improvements
class DescriptorContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0, img_size=512, output_size=64):
        super().__init__()
        self.margin = margin
        self.img_size = img_size
        self.output_size = output_size

    def forward(self, descriptors, keypoints_batch):
        # descriptors: [B, C, H, W]
        # keypoints_batch: list of keypoint coordinates for each image in the batch

        batch_size, C, H, W = descriptors.size()
        device = descriptors.device
        loss = torch.tensor(0.0, requires_grad=True).to(device)
        count = 0

        for i in range(batch_size):
            desc = descriptors[i]  # [C, H, W]
            kp = keypoints_batch[i]  # List of (x, y) tuples

            # Get descriptor vectors at keypoint locations
            pos_descriptors = []
            for x, y in kp:
                x = int(x * self.output_size / self.img_size)
                y = int(y * self.output_size / self.img_size)
                if 0 <= x < W and 0 <= y < H:
                    pos_descriptors.append(desc[:, y, x])

            if len(pos_descriptors) < 2:
                continue  # Not enough keypoints to compute loss

            pos_descriptors = torch.stack(pos_descriptors)  # [N, C]

            # Compute pairwise distances between descriptors
            pdist = torch.pdist(pos_descriptors, p=2)

            # Positive pairs (should be close)
            pos_loss = pdist.mean()

            # Negative pairs (should be far)
            # Sample random descriptors from other locations
            neg_descriptors = desc.view(C, -1).t()  # [H*W, C]
            neg_indices = torch.randperm(neg_descriptors.size(0))[:len(pos_descriptors)].to(device)
            neg_descriptors = neg_descriptors[neg_indices]

            # Compute distances between positive and negative descriptors
            neg_dists = torch.cdist(pos_descriptors, neg_descriptors, p=2)
            neg_loss = F.relu(self.margin - neg_dists).mean()

            loss += pos_loss + neg_loss
            count += 1

        if count > 0:
            loss = loss / count
        else:
            loss = torch.tensor(0.0, requires_grad=True).to(device)

        return loss

# Wireframe Dataset with Data Augmentation
# Wireframe Dataset with Data Augmentation
class WireframeDataset(Dataset):
    def __init__(self, v1_1_path, pointlines_path, split='train', img_size=512, output_size=64):
        self.img_size = img_size
        self.output_size = output_size
        self.v1_1_path = Path(v1_1_path)
        self.pointlines_path = Path(pointlines_path)
        self.split = split

        self.img_dir = self.v1_1_path / split
        self.img_paths = list(self.img_dir.glob('*.jpg'))

        # Load keypoint annotations
        self.annotations = self.load_annotations()

        # Data augmentation
        if self.split == 'train':
            self.transform = A.Compose([
                A.Resize(img_size, img_size),
                A.RandomRotate90(),
                A.HorizontalFlip(),
                A.VerticalFlip(),
                A.Normalize(mean=[0.485], std=[0.229]),
                ToTensorV2()
            ], keypoint_params=A.KeypointParams(format='xy'))
        else:
            self.transform = A.Compose([
                A.Resize(img_size, img_size),
                A.Normalize(mean=[0.485], std=[0.229]),
                ToTensorV2()
            ], keypoint_params=A.KeypointParams(format='xy'))

        print(f"Found {len(self.img_paths)} images in {split} set")

    def load_annotations(self):
        # Implement logic to load annotations from files
        annotations = {}
        annotation_files = list((self.pointlines_path / self.split).glob('*.json'))
        for anno_file in annotation_files:
            with open(anno_file, 'r') as f:
                data = json.load(f)
                image_id = data['image_id']
                keypoints = data['keypoints']  # List of (x, y) tuples
                annotations[image_id] = keypoints
        return annotations

    def generate_heatmap(self, keypoints):
        heatmap = np.zeros((self.output_size, self.output_size), dtype=np.float32)
        for x, y in keypoints:
            x = int(x * self.output_size / self.img_size)
            y = int(y * self.output_size / self.img_size)
            if 0 <= x < self.output_size and 0 <= y < self.output_size:
                heatmap[y, x] = 1.0  # Set keypoint location to 1
        # Apply Gaussian blur
        heatmap = cv2.GaussianBlur(heatmap, (7, 7), 0)
        # Normalize heatmap
        heatmap = heatmap / heatmap.max() if heatmap.max() != 0 else heatmap
        return heatmap

    def __getitem__(self, idx):
        img_path = str(self.img_paths[idx])
        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        height, width = image.shape

        # Get image ID to retrieve annotations
        image_id = Path(img_path).stem
        keypoints = self.annotations.get(image_id, [])

        # Prepare keypoints for augmentation
        keypoints_aug = keypoints  # Already in (x, y) format

        # Transform image and keypoints
        transformed = self.transform(image=image, keypoints=keypoints_aug)
        image = transformed["image"]
        keypoints_transformed = transformed["keypoints"]  # List of (x, y) tuples

        # Generate heatmap
        heatmap = self.generate_heatmap(keypoints_transformed)
        heatmap = torch.tensor(heatmap, dtype=torch.float32).unsqueeze(0)

        return {
            'image': image,
            'heatmap': heatmap,
            'keypoints': keypoints_transformed,  # Use transformed keypoints
            'path': img_path
        }

    def __len__(self):
        return len(self.img_paths)


# 自定义 collate_fn 函数
def custom_collate_fn(batch):
    images = torch.stack([item['image'] for item in batch])
    heatmaps = torch.stack([item['heatmap'] for item in batch])
    keypoints = [item['keypoints'] for item in batch]  # 保持为列表的列表
    paths = [item['path'] for item in batch]
    return {'image': images, 'heatmap': heatmaps, 'keypoints': keypoints, 'path': paths}

def create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=2):
    train_dataset = WireframeDataset(v1_1_path, pointlines_path, split='train')
    val_dataset = WireframeDataset(v1_1_path, pointlines_path, split='test')

    # 使用自定义的 collate_fn
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=custom_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate_fn)
    return train_loader, val_loader

# 评估函数
def evaluate(model, val_loader, device, heatmap_criterion, descriptor_criterion):
    model.eval()
    total_loss = 0.0
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validation"):
            images = batch['image'].to(device)
            heatmaps_gt = batch['heatmap'].to(device)
            keypoints_gt = batch['keypoints']

            feats, keypoints_pred, heatmap_pred, descriptors = model(images)

            # 计算热图损失
            heatmap_loss = heatmap_criterion(heatmap_pred, heatmaps_gt)

            # 计算描述符损失
            descriptor_loss = descriptor_criterion(descriptors, keypoints_gt)

            # 总损失
            loss = heatmap_loss + descriptor_loss

            total_loss += loss.item()

    avg_loss = total_loss / len(val_loader)
    print(f"Validation Loss: {avg_loss:.4f}")

# 主函数
def main():
    v1_1_path = "/content/drive/MyDrive/v1.1"
    pointlines_path = "/content/drive/MyDrive/pointlines"

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    train_loader, val_loader = create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=2)

    model = LightweightXFeatModel().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    # 学习率调度器
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

    # 定义损失函数
    heatmap_criterion = nn.BCEWithLogitsLoss()
    descriptor_criterion = DescriptorContrastiveLoss(img_size=512, output_size=64)

    num_epochs = 20  # 增加训练周期
    best_val_loss = float('inf')  # 用于保存最佳模型

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            images = batch['image'].to(device)
            heatmaps_gt = batch['heatmap'].to(device)
            keypoints_gt = batch['keypoints']

            optimizer.zero_grad()
            feats, keypoints_pred, heatmap_pred, descriptors = model(images)

            # 计算热图损失
            heatmap_loss = heatmap_criterion(heatmap_pred, heatmaps_gt)

            # 计算描述符损失
            descriptor_loss = descriptor_criterion(descriptors, keypoints_gt)

            # 总损失
            loss = heatmap_loss + descriptor_loss

            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}")

        # 在验证集上评估模型
        evaluate(model, val_loader, device, heatmap_criterion, descriptor_criterion)

        # 更新学习率
        scheduler.step()

        # 保存模型检查点
        val_loss = avg_loss  # 这里可以使用验证集的损失，如果在 evaluate 中返回
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            model_save_path = f"best_model_epoch_{epoch+1}.pth"
            torch.save(model.state_dict(), model_save_path)
            print(f"Saved Best Model at Epoch {epoch+1} to {model_save_path}")

if __name__ == "__main__":
    main()

import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2
import numpy as np
import albumentations as A
from albumentations.pytorch import ToTensorV2

# EfficientLayer
class EfficientLayer(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

# LightweightMultiScaleContext
class LightweightMultiScaleContext(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.branch1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.branch2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)
        self.branch3 = nn.Conv2d(in_channels, out_channels, kernel_size=5, padding=2, bias=False)
        self.bn = nn.BatchNorm2d(3 * out_channels)

    def forward(self, x):
        out1 = self.branch1(x)
        out2 = self.branch2(x)
        out3 = self.branch3(x)
        out = torch.cat([out1, out2, out3], dim=1)
        out = self.bn(out)
        return F.relu(out)

# LightweightDynamicFeatureEnhancement
class LightweightDynamicFeatureEnhancement(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=1, bias=False)
        self.bn2 = nn.BatchNorm2d(channels)

    def forward(self, x):
        attention = torch.sigmoid(self.conv2(F.relu(self.bn1(self.conv1(x)))))
        return x * attention

# DescriptorHead
class DescriptorHead(nn.Module):
    def __init__(self, in_channels, descriptor_dim):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, descriptor_dim, kernel_size=1, bias=False)
        self.bn = nn.BatchNorm2d(descriptor_dim)

    def forward(self, x):
        return F.normalize(self.bn(self.conv(x)), p=2, dim=1)

# Main Model Class
class LightweightXFeatModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.norm = nn.InstanceNorm2d(1)

        # Initial Convolution
        self.initial_conv = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
        )

        # Efficient Blocks
        self.block1 = EfficientLayer(16, 24, stride=2)
        self.block2 = EfficientLayer(24, 32, stride=2)
        self.block3 = EfficientLayer(32, 64, stride=2)

        # Lightweight Multi-Scale Context
        self.multi_scale_context = LightweightMultiScaleContext(64, 64)

        # Further Efficient Blocks
        self.block4 = EfficientLayer(64, 96, stride=2)
        self.block5 = EfficientLayer(96, 128, stride=2)

        # Fusion Layer
        self.block_fusion = nn.Sequential(
            nn.Conv2d(64 + 96 + 128, 64, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
        )

        # Lightweight Dynamic Feature Enhancement
        self.dynamic_feature_enhancement = LightweightDynamicFeatureEnhancement(64)

        # Output Heads
        self.heatmap_head = nn.Conv2d(64, 1, kernel_size=1)
        self.keypoint_head = nn.Conv2d(64, 65, kernel_size=1)
        self.descriptor_head = DescriptorHead(64, descriptor_dim=128)

    def forward(self, x):
        if x.shape[1] != 1:
            x = x.mean(dim=1, keepdim=True)
        x = self.norm(x)

        x = self.initial_conv(x)
        x1 = self.block1(x)
        x2 = self.block2(x1)
        x3 = self.block3(x2)

        x3 = self.multi_scale_context(x3)

        x4 = self.block4(x3)
        x5 = self.block5(x4)

        x4_up = F.interpolate(x4, size=x3.shape[2:], mode='bilinear', align_corners=False)
        x5_up = F.interpolate(x5, size=x3.shape[2:], mode='bilinear', align_corners=False)
        fusion_input = torch.cat([x3, x4_up, x5_up], dim=1)
        feats = self.block_fusion(fusion_input)

        feats = self.dynamic_feature_enhancement(feats)

        heatmap = self.heatmap_head(feats)
        keypoints = self.keypoint_head(feats)
        descriptors = self.descriptor_head(feats)
        return feats, keypoints, heatmap, descriptors



import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2
import numpy as np
from pathlib import Path
from tqdm import tqdm

# 基础网络层
class BasicLayer(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, bias=False):
        super().__init__()
        self.layer = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, stride=stride, dilation=dilation, bias=bias),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        return self.layer(x)

# 动态特征增强模块
class DynamicFeatureEnhancement(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(2 * channels, channels),
            nn.ReLU(inplace=True),
            nn.Linear(channels, channels),
            nn.Sigmoid()
        )

    def forward(self, x):
        mean = torch.mean(x, dim=(2, 3), keepdim=True)
        std = torch.std(x, dim=(2, 3), keepdim=True)
        stats = torch.cat([mean, std], dim=1).squeeze(-1).squeeze(-1)
        weights = self.fc(stats)
        weights = weights.view(x.size(0), x.size(1), 1, 1)
        return x * weights

# 多尺度上下文嵌入模块
class MultiScaleLocalContextEmbedding(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.branch1 = nn.Conv2d(in_channels, out_channels // 4, kernel_size=1, padding=0)
        self.branch2 = nn.Conv2d(in_channels, out_channels // 4, kernel_size=3, padding=1)
        self.branch3 = nn.Conv2d(in_channels, out_channels // 4, kernel_size=5, padding=2)
        self.branch4 = nn.Conv2d(in_channels, out_channels // 4, kernel_size=3, dilation=2, padding=2)
        self.fusion = nn.Conv2d(out_channels, out_channels, kernel_size=1, padding=0)

    def forward(self, x):
        b1 = self.branch1(x)
        b2 = self.branch2(x)
        b3 = self.branch3(x)
        b4 = self.branch4(x)
        out = torch.cat([b1, b2, b3, b4], dim=1)
        return self.fusion(out)

# 描述子输出头
class DescriptorHead(nn.Module):
    def __init__(self, in_channels, descriptor_dim=256):
        super().__init__()
        self.descriptor_head = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, stride=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, descriptor_dim, kernel_size=1, stride=1)
        )

    def forward(self, x):
        return F.normalize(self.descriptor_head(x), p=2, dim=1)  # L2归一化

# 主模型
class XFeatModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.norm = nn.InstanceNorm2d(1)

        self.skip1 = nn.Sequential(
            nn.AvgPool2d(4, stride=4),
            nn.Conv2d(1, 24, 1, stride=1, padding=0)
        )

        self.block1 = nn.Sequential(
            BasicLayer(1, 4, stride=1),
            BasicLayer(4, 8, stride=2),
            BasicLayer(8, 8, stride=1),
            BasicLayer(8, 24, stride=2),
        )

        self.block2 = nn.Sequential(
            BasicLayer(24, 24, stride=1),
            BasicLayer(24, 24, stride=1),
        )

        self.block3 = nn.Sequential(
            BasicLayer(24, 64, stride=2),
            BasicLayer(64, 64, stride=1),
            BasicLayer(64, 64, 1, padding=0),
        )

        self.multi_scale_context = MultiScaleLocalContextEmbedding(64, 64)

        self.block4 = nn.Sequential(
            BasicLayer(64, 64, stride=2),
            BasicLayer(64, 64, stride=1),
            BasicLayer(64, 64, stride=1),
        )

        self.block5 = nn.Sequential(
            BasicLayer(64, 128, stride=2),
            BasicLayer(128, 128, stride=1),
            BasicLayer(128, 128, stride=1),
            BasicLayer(128, 64, 1, padding=0),
        )

        self.block_fusion = nn.Sequential(
            BasicLayer(64, 64, stride=1),
            BasicLayer(64, 64, stride=1),
            nn.Conv2d(64, 64, 1, padding=0)
        )

        self.dynamic_feature_enhancement = DynamicFeatureEnhancement(64)

        self.heatmap_head = nn.Sequential(
            BasicLayer(64, 64, 1, padding=0),
            BasicLayer(64, 64, 1, padding=0),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()
        )

        self.keypoint_head = nn.Sequential(
            BasicLayer(64, 64, 1, padding=0),
            BasicLayer(64, 64, 1, padding=0),
            nn.Conv2d(64, 65, 1),
        )

        self.descriptor_head = DescriptorHead(64, descriptor_dim=256)

    def forward(self, x):
        # 确保输入为灰度图
        with torch.no_grad():
            if x.shape[1] != 1:
                x = x.mean(dim=1, keepdim=True)
            x = self.norm(x)

        x1 = self.block1(x)
        skip1_out = self.skip1(x)

        # 调整 skip1_out 和 x1 的通道数
        skip1_out = F.interpolate(skip1_out, size=x1.shape[2:], mode='bilinear', align_corners=False)
        if skip1_out.shape[1] < x1.shape[1]:
            skip1_out = skip1_out.expand(-1, x1.shape[1], -1, -1)

        x2 = self.block2(x1 + skip1_out)
        x3 = self.block3(x2)
        x3 = self.multi_scale_context(x3)
        x4 = self.block4(x3)
        x5 = self.block5(x4)

        # 融合特征
        x4 = F.interpolate(x4, (x3.shape[-2], x3.shape[-1]), mode='bilinear')
        x5 = F.interpolate(x5, (x3.shape[-2], x3.shape[-1]), mode='bilinear')
        feats = self.block_fusion(x3 + x4 + x5)
        feats = self.dynamic_feature_enhancement(feats)

        # 输出
        heatmap = self.heatmap_head(feats)
        keypoints = self.keypoint_head(feats)
        descriptors = self.descriptor_head(feats)
        return feats, keypoints, heatmap, descriptors

# 数据集部分保持不变
class WireframeDataset(Dataset):
    def __init__(self, v1_1_path, pointlines_path, split='train', img_size=512):
        self.img_size = img_size
        self.v1_1_path = Path(v1_1_path)
        self.pointlines_path = Path(pointlines_path)
        self.split = split

        self.img_dir = self.v1_1_path / split
        self.img_paths = list(self.img_dir.glob('*.jpg'))

        self.transform = A.Compose([
            A.Resize(img_size, img_size),
            A.Normalize(mean=[0.485], std=[0.229]),
            ToTensorV2()
        ])

        print(f"Found {len(self.img_paths)} images in {split} set")

    def __getitem__(self, idx):
        img_path = str(self.img_paths[idx])
        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        image = self.transform(image=image)["image"]
        return {'image': image, 'path': img_path}

    def __len__(self):
        return len(self.img_paths)

def create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=2):
    train_dataset = WireframeDataset(v1_1_path, pointlines_path, split='train')
    val_dataset = WireframeDataset(v1_1_path, pointlines_path, split='test')

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    return train_loader, val_loader

# 主函数
def main():
    v1_1_path = "/content/drive/MyDrive/v1.1"
    pointlines_path = "/content/drive/MyDrive/pointlines"

    train_loader, val_loader = create_dataloader(v1_1_path, pointlines_path, batch_size=8, num_workers=2)

    model = XFeatModel()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    for epoch in range(10):
        model.train()
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/10"):
            images = batch['image']
            optimizer.zero_grad()
            feats, keypoints, heatmap, descriptors = model(images)
            loss = F.mse_loss(heatmap, torch.zeros_like(heatmap))  # 示例损失
            loss.backward()
            optimizer.step()

if __name__ == "__main__":
    main()

"""# **修改过损失函数的版本：**"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class ResBlock(nn.Module):
    """残差块防止模型退化"""
    def __init__(self, channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(channels)

        # 如果stride不为1，添加1x1卷积的shortcut
        self.shortcut = nn.Identity()
        if stride != 1:
            self.shortcut = nn.Sequential(
                nn.Conv2d(channels, channels, 1, stride=stride, bias=False),
                nn.BatchNorm2d(channels)
            )

    def forward(self, x):
        identity = self.shortcut(x)

        out = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        out += identity
        out = F.relu(out)

        return out

class OptimizedXFeatModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.norm = nn.InstanceNorm2d(1)

        # 初始特征提取
        self.init_conv = nn.Sequential(
            nn.Conv2d(1, 32, 7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2, padding=1)
        )

        # 编码器
        self.encoder1 = self._make_layer(32, 64, stride=2)
        self.encoder2 = self._make_layer(64, 128, stride=2)
        self.encoder3 = self._make_layer(128, 256, stride=2)

        # 特征融合
        self.fusion = nn.Sequential(
            nn.Conv2d(448, 256, 1, bias=False),  # 448 = 64 + 128 + 256
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            ResBlock(256)
        )

        # 输出头
        self.heatmap_head = nn.Sequential(
            nn.Conv2d(256, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()
        )

        self.keypoint_head = nn.Sequential(
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 65, 1)
        )

        self.descriptor_head = nn.Sequential(
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, 1)
        )

        # 初始化权重
        self._initialize_weights()

    def _make_layer(self, in_channels, out_channels, stride):
        layers = []
        layers.append(nn.Conv2d(in_channels, out_channels, 3,
                              stride=stride, padding=1, bias=False))
        layers.append(nn.BatchNorm2d(out_channels))
        layers.append(nn.ReLU(inplace=True))
        layers.append(ResBlock(out_channels))
        return nn.Sequential(*layers)

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        # 归一化
        with torch.no_grad():
            if x.shape[1] != 1:
                x = x.mean(dim=1, keepdim=True)
            x = self.norm(x)

        # 特征提取
        x0 = self.init_conv(x)
        x1 = self.encoder1(x0)  # 1/4
        x2 = self.encoder2(x1)  # 1/8
        x3 = self.encoder3(x2)  # 1/16

        # 特征融合 (上采样并拼接)
        x2_up = F.interpolate(x2, size=x1.shape[2:], mode='bilinear', align_corners=False)
        x3_up = F.interpolate(x3, size=x1.shape[2:], mode='bilinear', align_corners=False)
        fusion = torch.cat([x1, x2_up, x3_up], dim=1)
        feats = self.fusion(fusion)

        # 输出
        heatmap = self.heatmap_head(feats)
        keypoints = self.keypoint_head(feats)
        descriptors = F.normalize(self.descriptor_head(feats), p=2, dim=1)

        return feats, keypoints, heatmap, descriptors









import matplotlib.pyplot as plt

# 可视化单张图片的关键点
keypoints_np = keypoints[0].cpu().numpy()  # 提取第一张图的关键点
image_np = images[0].squeeze(0).cpu().numpy()  # 提取第一张图像

plt.imshow(image_np, cmap="gray")  # 显示原图
plt.scatter(keypoints_np[:, 0], keypoints_np[:, 1], color="lime", s=10)  # 绘制关键点
plt.title("Predicted Keypoints")
plt.show()

# 检查 keypoints 的形状和值
print("Keypoints shape:", keypoints[0].shape)  # 打印 keypoints 的形状
print("Keypoints data (example):", keypoints[0][0].cpu().numpy())  # 查看特征图第一个通道的值

# 如果是特征图，打印其最小值和最大值
if len(keypoints_np.shape) == 3:
    print("Feature map max value:", keypoints_np.max())
    print("Feature map min value:", keypoints_np.min())

import numpy as np
from scipy.ndimage import maximum_filter

keypoints_list = []
threshold = threshold.item()  # 将 torch.Tensor 转换为标量

for i in range(5):  # 遍历前 5 个通道
    feature_map = keypoints[i].cpu().numpy()  # 提取通道特征图
    local_max = maximum_filter(feature_map, size=3)  # 寻找局部最大值
    y_coords, x_coords = np.where((feature_map == local_max) & (feature_map > threshold))
    channel_keypoints = np.stack((x_coords, y_coords), axis=1)  # 转换为 (N, 2)
    keypoints_list.append(channel_keypoints)





# 从模型的某一层提取特征
with torch.no_grad():
    x1 = model.block1(images)
    feature_map = x1[0, 0].cpu().numpy()  # 提取第一张图片的第一层特征图

plt.imshow(feature_map, cmap="viridis")
plt.title("Intermediate Feature Map")
plt.colorbar()
plt.show()

print("Descriptors shape:", descriptors.shape)

# 假设我们想提取 batch 中的第 0 张图片的描述子
descriptors_np = descriptors[0].cpu().numpy()  # [256, 64, 64]
print("Single image descriptors shape:", descriptors_np.shape)

# 展平描述子
C, H, W = descriptors_np.shape  # 通道数、特征图高度和宽度
descriptors_flat = descriptors_np.reshape(C, H * W).T  # 转为 [N, D]（[4096, 256]）
print("Flattened descriptors shape:", descriptors_flat.shape)

# 假设我们选择 batch 中的第 1 张图片
descriptors2_np = descriptors[1].cpu().numpy()  # [256, 64, 64]
descriptors2_flat = descriptors2_np.reshape(C, H * W).T  # 转为 [N, D]
print("Second image descriptors shape:", descriptors2_flat.shape)

import cv2

# 创建 BFMatcher
bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)

# 进行匹配
matches = bf.match(descriptors_flat, descriptors2_flat)

# 按距离排序匹配点（越小越好）
matches = sorted(matches, key=lambda x: x.distance)

print(f"Found {len(matches)} matches")

# 提取匹配点的距离
distances = [m.distance for m in matches]

# 绘制距离分布直方图
plt.hist(distances, bins=50, color="blue", alpha=0.7)
plt.title("Distribution of Match Distances")
plt.xlabel("Distance")
plt.ylabel("Frequency")
plt.show()

print("Keypoints shape:", keypoints1_np.shape)
print("Keypoints dtype:", keypoints1_np.dtype)

import matplotlib.pyplot as plt

plt.hist(keypoints_np.flatten(), bins=50, color="blue", alpha=0.7)
plt.title("Keypoint Map Value Distribution")
plt.xlabel("Value")
plt.ylabel("Frequency")
plt.show()







import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import cv2
import numpy as np
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2
from tqdm import tqdm

class AdaptiveFeatureAggregation(nn.Module):
    """自适应特征聚合模块"""
    def __init__(self, channels):
        super().__init__()
        self.attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels * 3, channels, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, 3, 1),
            nn.Softmax(dim=1)
        )

    def forward(self, x1, x2, x3):
        concat = torch.cat([
            F.adaptive_avg_pool2d(x1, 1),
            F.adaptive_avg_pool2d(x2, 1),
            F.adaptive_avg_pool2d(x3, 1)
        ], dim=1)
        weights = self.attention(concat)

        x2 = F.interpolate(x2, size=x1.shape[2:], mode='bilinear')
        x3 = F.interpolate(x3, size=x1.shape[2:], mode='bilinear')

        out = x1 * weights[:,0:1] + x2 * weights[:,1:2] + x3 * weights[:,2:3]
        return out

class LightweightDescriptor(nn.Module):
    """轻量级描述子生成模块"""
    def __init__(self, in_channels, desc_dim=128):
        super().__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels)
        self.pointwise = nn.Conv2d(in_channels, desc_dim, 1)
        self.channel_shuffle = lambda x: torch.cat(x.chunk(4, dim=1), dim=1)
        self.norm = nn.BatchNorm2d(desc_dim)

    def forward(self, x):
        x = self.depthwise(x)
        x = self.channel_shuffle(x)
        x = self.pointwise(x)
        x = self.norm(x)
        return F.normalize(x, p=2, dim=1)

class LightBlock(nn.Module):
    """轻量级特征提取块"""
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv = nn.Sequential(
            # 深度可分离卷积
            nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels),
            nn.Conv2d(in_channels, out_channels, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

        # 残差连接
        self.shortcut = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 1, stride=stride),
            nn.BatchNorm2d(out_channels)
        ) if in_channels != out_channels or stride != 1 else nn.Identity()

    def forward(self, x):
        return self.conv(x) + self.shortcut(x)

class EnhancedXFeatModel(nn.Module):
    def __init__(self, desc_dim=128):
        super().__init__()
        # 输入处理
        self.input_norm = nn.InstanceNorm2d(1)

        # 基础特征提取
        self.stem = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2)
        )

        # 特征提取块
        self.stage1 = self._make_stage(32, 64, blocks=2)
        self.stage2 = self._make_stage(64, 64, blocks=2)
        self.stage3 = self._make_stage(64, 64, blocks=2)

        # 创新模块
        self.feature_aggregation = AdaptiveFeatureAggregation(64)
        self.descriptor = LightweightDescriptor(64, desc_dim)

        # 检测头
        self.detect_head = nn.Sequential(
            nn.Conv2d(64, 32, 1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, 1),
            nn.Sigmoid()
        )

    def _make_stage(self, in_channels, out_channels, blocks):
        layers = []
        layers.append(LightBlock(in_channels, out_channels, stride=2))
        for _ in range(1, blocks):
            layers.append(LightBlock(out_channels, out_channels, stride=1))
        return nn.Sequential(*layers)

    def forward(self, x):
        # 确保输入为灰度图
        if x.shape[1] != 1:
            x = x.mean(dim=1, keepdim=True)
        x = self.input_norm(x)

        # 特征提取
        x = self.stem(x)
        f1 = self.stage1(x)      # 1/4
        f2 = self.stage2(f1)     # 1/8
        f3 = self.stage3(f2)     # 1/16

        # 特征聚合
        features = self.feature_aggregation(f1, f2, f3)

        # 生成描述子和检测scores
        descriptors = self.descriptor(features)
        scores = self.detect_head(features)

        return {
            'descriptors': descriptors,
            'scores': scores,
            'features': features
        }

    @torch.no_grad()
    def get_keypoints(self, scores, top_k=1000, nms_radius=4):
        """提取关键点"""
        B, _, H, W = scores.shape
        keypoints = []
        scores_out = []

        for b in range(B):
            score = scores[b, 0]
            nms = F.max_pool2d(score.unsqueeze(0), nms_radius, stride=1, padding=nms_radius//2)
            mask = (score == nms.squeeze(0)) & (score > 0.015)

            y, x = mask.nonzero(as_tuple=True)
            s = score[y, x]

            if len(s) > top_k:
                idx = torch.argsort(s, descending=True)[:top_k]
                y, x, s = y[idx], x[idx], s[idx]

            keypoints.append(torch.stack([x, y], dim=1))
            scores_out.append(s)

        return keypoints, scores_out

class FeatureMatchingLoss(nn.Module):
    """特征匹配损失函数"""
    def __init__(self, pos_margin=1, neg_margin=0.2):
        super().__init__()
        self.pos_margin = pos_margin
        self.neg_margin = neg_margin

    def forward(self, descriptors1, descriptors2, matches):
        """
        Args:
            descriptors1, descriptors2: [B, C, H, W] 特征描述子
            matches: List of [N, 4] tensors containing matching point coordinates (x1,y1,x2,y2)
        """
        loss = 0
        batch_size = descriptors1.shape[0]

        for b in range(batch_size):
            if len(matches[b]) == 0:
                continue

            # 提取匹配点对的描述子
            x1, y1, x2, y2 = matches[b].long().t()
            desc1 = descriptors1[b, :, y1, x1].t()  # [N, C]
            desc2 = descriptors2[b, :, y2, x2].t()  # [N, C]

            # 计算正样本损失
            pos_dist = F.pairwise_distance(desc1, desc2)
            pos_loss = F.relu(pos_dist - self.pos_margin)

            # 计算负样本损失(使用随机负样本)
            N = len(desc1)
            idx = torch.randperm(N)
            neg_desc2 = desc2[idx]
            neg_dist = F.pairwise_distance(desc1, neg_desc2)
            neg_loss = F.relu(self.neg_margin - neg_dist)

            loss += (pos_loss.mean() + neg_loss.mean()) / 2

        return loss / batch_size

class DetectionLoss(nn.Module):
    """关键点检测损失函数"""
    def __init__(self):
        super().__init__()

    def forward(self, pred_scores, gt_keypoints, img_size):
        """
        Args:
            pred_scores: [B, 1, H, W] 预测的检测分数
            gt_keypoints: List of [N, 2] tensors containing keypoint coordinates
            img_size: tuple of (H, W)
        """
        loss = 0
        batch_size = pred_scores.shape[0]

        for b in range(batch_size):
            # 生成真值热力图
            gt_map = torch.zeros_like(pred_scores[b,0])
            kpts = gt_keypoints[b]
            if len(kpts) > 0:
                x, y = kpts.long().t()
                gt_map[y, x] = 1

                # 高斯模糊使热力图更平滑
                gt_map = torch.from_numpy(
                    cv2.GaussianBlur(gt_map.cpu().numpy(), (3,3), 1)
                ).to(pred_scores.device)

            # 使用Focal Loss
            pred = pred_scores[b,0]
            pos_weight = (gt_map > 0.1).float()
            neg_weight = 1 - pos_weight

            pos_loss = -(pos_weight * (1-pred)**2 * torch.log(pred + 1e-6))
            neg_loss = -(neg_weight * pred**2 * torch.log(1-pred + 1e-6))

            loss += (pos_loss.mean() + neg_loss.mean()) / 2

        return loss / batch_size

def train_step(model, batch, feature_loss, detect_loss):
    """单步训练"""
    optimizer.zero_grad()

    # 前向传播
    outputs1 = model(batch['image1'])
    outputs2 = model(batch['image2'])

    # 计算损失
    matching_loss = feature_loss(
        outputs1['descriptors'],
        outputs2['descriptors'],
        batch['matches']
    )

    detection_loss = detect_loss(
        outputs1['scores'],
        batch['keypoints1'],
        batch['image1'].shape[-2:]
    ) + detect_loss(
        outputs2['scores'],
        batch['keypoints2'],
        batch['image2'].shape[-2:]
    )

    total_loss = matching_loss + detection_loss

    # 反向传播
    total_loss.backward()
    optimizer.step()

    return {
        'total_loss': total_loss.item(),
        'matching_loss': matching_loss.item(),
        'detection_loss': detection_loss.item()
    }

def train_model(model, train_loader, val_loader, num_epochs=50):
    """模型训练主循环"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    # 初始化损失函数和优化器
    feature_loss = FeatureMatchingLoss()
    detect_loss = DetectionLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)

    best_val_loss = float('inf')

    for epoch in range(num_epochs):
        # 训练阶段
        model.train()
        train_losses = []

        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):
            batch = {k: v.to(device) if torch.is_tensor(v) else v
                    for k, v in batch.items()}

            losses = train_step(model, batch, feature_loss, detect_loss)
            train_losses.append(losses['total_loss'])

        avg_train_loss = sum(train_losses) / len(train_losses)

        # 验证阶段
        model.eval()
        val_losses = []

        with torch.no_grad():
            for batch in val_loader:
                batch = {k: v.to(device) if torch.is_tensor(v) else v
                        for k, v in batch.items()}

                outputs1 = model(batch['image1'])
                outputs2 = model(batch['image2'])

                matching_loss = feature_loss(
                    outputs1['descriptors'],
                    outputs2['descriptors'],
                    batch['matches']
                )

                detection_loss = detect_loss(
                    outputs1['scores'],
                    batch['keypoints1'],
                    batch['image1'].shape[-2:]
                ) + detect_loss(
                    outputs2['scores'],
                    batch['keypoints2'],
                    batch['image2'].shape[-2:]
                )

                val_losses.append((matching_loss + detection_loss).item())

        avg_val_loss = sum(val_losses) / len(val_losses)
        scheduler.step(avg_val_loss)

        # 保存最佳模型
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), 'best_model.pth')

        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'Train Loss: {avg_train_loss:.4f}')
        print(f'Val Loss: {avg_val_loss:.4f}')







"""# **包括创新点的 XFeatModel架构**"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class BasicLayer(nn.Module):
    """
    Basic Convolutional Layer: Conv2d -> BatchNorm -> ReLU
    """
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, bias=False):
        super().__init__()
        self.layer = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, stride=stride, dilation=dilation, bias=bias),
            nn.BatchNorm2d(out_channels, affine=False),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.layer(x)

class XFeatModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.norm = nn.InstanceNorm2d(1)

        # CNN Backbone & Heads
        self.skip1 = nn.Sequential(
            nn.AvgPool2d(4, stride=4),
            nn.Conv2d(1, 24, 1, stride=1, padding=0)
        )

        # 创新点 1: 注意力机制
        self.attention_module = nn.Sequential(
            nn.Conv2d(24, 12, kernel_size=1, stride=1, padding=0),
            nn.ReLU(),
            nn.Conv2d(12, 24, kernel_size=1, stride=1, padding=0),
            nn.Sigmoid()
        )

        self.block1 = nn.Sequential(
            BasicLayer(1, 4, stride=1),
            BasicLayer(4, 8, stride=2),
            BasicLayer(8, 8, stride=1),
            BasicLayer(8, 24, stride=2)
        )

        self.block2 = nn.Sequential(
            BasicLayer(24, 24, stride=1),
            BasicLayer(24, 24, stride=1)
        )

        # 创新点 2: 多尺度特征融合
        self.block3 = nn.Sequential(
            BasicLayer(24, 64, stride=2),
            BasicLayer(64, 64, stride=1),
            BasicLayer(64, 64, 1, padding=0)
        )
        self.block4 = nn.Sequential(
            BasicLayer(64, 64, stride=2),
            BasicLayer(64, 64, stride=1),
            BasicLayer(64, 64, stride=1)
        )
        self.block5 = nn.Sequential(
            BasicLayer(64, 128, stride=2),
            BasicLayer(128, 128, stride=1),
            BasicLayer(128, 128, stride=1),
            BasicLayer(128, 64, 1, padding=0)
        )
        self.block_fusion = nn.Sequential(
            BasicLayer(64 + 64 + 64, 64, stride=1),
            BasicLayer(64, 64, stride=1),
            nn.Conv2d(64, 64, 1, padding=0)
        )

        self.heatmap_head = nn.Sequential(
            BasicLayer(64, 64, 1, padding=0),
            BasicLayer(64, 64, 1, padding=0),
            nn.Conv2d(64, 1, 1),
            nn.Sigmoid()
        )

        self.keypoint_head = nn.Sequential(
            BasicLayer(64, 64, 1, padding=0),
            BasicLayer(64, 64, 1, padding=0),
            BasicLayer(64, 64, 1, padding=0),
            nn.Conv2d(64, 65, 1)
        )

        self.fine_matcher = nn.Sequential(
            nn.Linear(128, 512),
            nn.BatchNorm1d(512, affine=False),
            nn.ReLU(inplace=True),
            nn.Linear(512, 512),
            nn.BatchNorm1d(512, affine=False),
            nn.ReLU(inplace=True),
            nn.Linear(512, 512),
            nn.BatchNorm1d(512, affine=False),
            nn.ReLU(inplace=True),
            nn.Linear(512, 512),
            nn.BatchNorm1d(512, affine=False),
            nn.ReLU(inplace=True),
            nn.Linear(512, 64)
        )

    def _unfold2d(self, x, ws=2):
        B, C, H, W = x.shape
        x = x.unfold(2, ws, ws).unfold(3, ws, ws).reshape(B, C, H//ws, W//ws, ws**2)
        return x.permute(0, 1, 4, 2, 3).reshape(B, -1, H//ws, W//ws)

    def forward(self, x):
        with torch.no_grad():
            x = x.mean(dim=1, keepdim=True)
            x = self.norm(x)

        x1 = self.block1(x)
        x2 = self.block2(x1 + self.skip1(x))

        # 创新点 1: 注意力机制
        attention_map = self.attention_module(x2)
        x2 = x2 * attention_map

        x3 = self.block3(x2)
        x4 = self.block4(x3)
        x5 = self.block5(x4)

        # 创新点 2: 多尺度特征融合
        x4 = F.interpolate(x4, (x3.shape[-2], x3.shape[-1]), mode='bilinear')
        x5 = F.interpolate(x5, (x3.shape[-2], x3.shape[-1]), mode='bilinear')
        feats = self.block_fusion(torch.cat([x3, x4, x5], dim=1))

        heatmap = self.heatmap_head(feats)
        keypoints = self.keypoint_head(self._unfold2d(x, ws=8))

        return feats, keypoints, heatmap



























# 导入必要的库
import torch
from torch.utils.data import Dataset, DataLoader
import cv2
import numpy as np
from pathlib import Path
import scipy.io as sio
import pickle
import matplotlib.pyplot as plt

# 定义展平函数，递归展开所有嵌套结构
def flatten_nested_list(nested_list):
    flat_list = []
    for item in nested_list:
        if isinstance(item, (list, tuple)):
            flat_list.extend(flatten_nested_list(item))
        else:
            flat_list.append(item)
    return flat_list

# 定义数据集类
class MultiTaskDataset(Dataset):
    def __init__(self, base_dir, pointlines_dir, line_mat_dir, split='train', transform=None):
        self.image_dir = Path(base_dir) / split  # 指定 train 或 test 文件夹
        self.pointlines_dir = Path(pointlines_dir)  # .pkl 文件目录
        self.line_mat_dir = Path(line_mat_dir)  # .mat 文件目录
        self.transform = transform

        # 根据 split 加载相应的 txt 文件中的文件列表
        self.image_files = []
        with open(Path(base_dir) / f"{split}.txt", 'r') as f:
            for line in f:
                self.image_files.append(line.strip())  # 假设每行是图像文件名

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        # 构造图像路径
        img_path = self.image_dir / self.image_files[idx]
        image = cv2.imread(str(img_path))
        if image is None:
            print(f"警告: 无法读取图像文件 '{img_path}'，跳过该样本。")
            return self.__getitem__((idx + 1) % len(self))  # 递归获取下一个样本
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0  # 归一化到 [0, 1]

        # 构造点线标注数据路径并读取 .pkl 文件
        pointlines_file = self.pointlines_dir / f"{img_path.stem}.pkl"
        if not pointlines_file.exists():
            print(f"警告: 点线文件 '{pointlines_file}' 不存在，跳过该样本。")
            return self.__getitem__((idx + 1) % len(self))  # 递归获取下一个样本

        with open(pointlines_file, 'rb') as f:
            data = pickle.load(f)

        # 从数据中提取信息
        points = data.get('points', [])
        pointlines = data.get('pointlines', [])
        lines = data.get('lines', [])

        # 将数据转换为 NumPy 数组并确保维度一致
        points = np.array(points, dtype=np.float32)

        # 使用递归展平 `pointlines` 列表，确保每个元素是 [x, y]
        pointlines_flat = []
        for pl in pointlines:
            if isinstance(pl, (list, tuple)) and len(pl) >= 2:
                # 检查每个元素是否为长度为2的坐标对
                pl = flatten_nested_list(pl)
                if len(pl) == 2:
                    pointlines_flat.append(pl)

        # 转换为二维数组
        pointlines = np.array(pointlines_flat, dtype=np.float32).reshape(-1, 2)

        lines = np.array(lines, dtype=np.float32)

        # 构造线段Mat文件路径并读取
        line_mat_file = self.line_mat_dir / f"{img_path.stem}_line.mat"
        if not line_mat_file.exists():
            print(f"警告: 线段文件 '{line_mat_file}' 不存在，跳过该样本。")
            return self.__getitem__((idx + 1) % len(self))  # 递归获取下一个样本
        line_data = sio.loadmat(str(line_mat_file))

        # 从 .mat 文件中提取线段数据
        mat_lines = line_data.get('lines', np.empty((0, 4)))  # 若 'lines' 不存在，使用空数组
        line_desc = line_data.get('lines_desc', np.empty((0,)))  # 若 'lines_desc' 不存在，使用空数组

        # 数据增强
        if self.transform:
            augmented = self.transform(image=image, keypoints=points, lines=lines)
            image = augmented['image']
            points = augmented['keypoints']
            lines = augmented['lines']

        # 转换为tensor并确保维度一致
        image = torch.FloatTensor(image).permute(2, 0, 1)  # [C, H, W]
        points = torch.FloatTensor(points)
        pointlines = torch.FloatTensor(pointlines)
        lines = torch.FloatTensor(lines)
        mat_lines = torch.FloatTensor(mat_lines)
        line_desc = torch.FloatTensor(line_desc)

        # 组织样本输出
        sample = {
            'image': image,
            'points': points,
            'pointlines': pointlines,
            'lines': lines,
            'mat_lines': mat_lines,
            'line_desc': line_desc
        }

        return sample

# 定义数据路径
base_dir = '/content/drive/MyDrive/v1.1'  # 包含 train 和 test 文件夹的主目录
pointlines_dir = '/content/drive/MyDrive/pointlines/'  # .pkl 文件目录
line_mat_dir = '/content/drive/MyDrive/line_mat/'  # .mat 文件目录

# 创建数据集和DataLoader（以训练集为例）
multi_task_dataset = MultiTaskDataset(base_dir, pointlines_dir, line_mat_dir, split='train')
data_loader = DataLoader(multi_task_dataset, batch_size=1, shuffle=True)

# 可视化和打印数据
def visualize_sample(sample):
    # 去除批次维度，变为 [C, H, W] 格式
    image = sample['image'].squeeze(0).permute(1, 2, 0).numpy()  # 转换为 [H, W, C]
    points = sample['points'].squeeze(0).numpy()
    mat_lines = sample['mat_lines'].squeeze(0).numpy()  # 使用 mat_lines 绘制线段

    plt.figure(figsize=(10, 10))
    plt.imshow(image)

    # 绘制关键点
    plt.scatter(points[:, 0], points[:, 1], c='red', s=10, label="Keypoints")

    # 绘制线段
    for line in mat_lines:
        if len(line) == 4:
            plt.plot([line[0], line[2]], [line[1], line[3]], 'b-', linewidth=1, label="Lines" if line is mat_lines[0] else "")

    plt.legend()
    plt.show()

# 从数据集中随机选取一个样本并可视化
sample = next(iter(data_loader))
if sample:  # 检查样本是否存在
    print("样本信息:")
    print("Image Tensor Shape:", sample['image'].shape)
    print("Points Shape:", sample['points'].shape)
    print("Pointlines Shape:", sample['pointlines'].shape)
    print("Lines Shape:", sample['lines'].shape)
    print("Mat Lines Shape:", sample['mat_lines'].shape)
    print("Line Descriptors Shape:", sample['line_desc'].shape)

    # 可视化该样本
    visualize_sample(sample)
else:
    print("没有找到可用的样本。")

class ComplexityEstimator(nn.Module):
    """场景复杂度估计器"""
    def __init__(self, in_channels):
        super().__init__()
        self.pools = nn.ModuleList([
            nn.AdaptiveAvgPool2d((size, size))
            for size in [1, 2, 4]  # 多尺度池化
        ])

        reduction = 8
        self.fc = nn.Sequential(
            nn.Linear(in_channels * (1 + 4 + 16), in_channels // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels),
            nn.Sigmoid()
        )

    def forward(self, x):
        # 多尺度特征聚合
        pool_outs = []
        for pool in self.pools:
            out = pool(x)
            pool_outs.append(out.flatten(2))

        # 拼接多尺度特征
        out = torch.cat([o.mean(2) for o in pool_outs], dim=1)

        # 生成复杂度得分
        complexity = self.fc(out)
        return complexity.unsqueeze(2).unsqueeze(3)

class AFEM(nn.Module):
    """自适应特征增强模块"""
    def __init__(self, in_channels):
        super().__init__()
        self.complexity_estimator = ComplexityEstimator(in_channels)

        # 特征增强分支
        self.enhance = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, in_channels, 1)
        )

        # 特征重校准
        self.recalibration = nn.Sequential(
            nn.Conv2d(in_channels*2, in_channels, 1),
            nn.BatchNorm2d(in_channels),
            nn.Sigmoid()
        )

    def forward(self, x):
        # 估计场景复杂度
        complexity = self.complexity_estimator(x)

        # 增强特征
        enhanced = self.enhance(x)

        # 根据复杂度自适应融合
        combined = torch.cat([x, enhanced], dim=1)
        weights = self.recalibration(combined)

        out = x + weights * enhanced
        return out, complexity

class SuperPointLineAFEM(nn.Module):
    """加入AFEM的点线特征检测网络"""
    def __init__(self, descriptor_dim=256):
        super().__init__()
        self.encoder = VGGStyleEncoder()

        # 在encoder后添加AFEM
        self.afem = AFEM(128)  # 128是encoder输出通道数

        self.point_head = PointDetector()
        self.line_head = LineDetector()
        self.descriptor = DescriptorExtractor(descriptor_dim=descriptor_dim)

    def forward(self, x):
        # 特征提取
        features = self.encoder(x)

        # 自适应特征增强
        features, complexity = self.afem(features)

        # 检测点和线
        point_pred = self.point_head(features)
        line_pred = self.line_head(features)

        # 提取描述子
        descriptors = self.descriptor(features)

        # 上采样
        point_pred = F.interpolate(point_pred, size=x.shape[2:], mode='bilinear', align_corners=False)
        line_pred = F.interpolate(line_pred, size=x.shape[2:], mode='bilinear', align_corners=False)

        return {
            'points': point_pred,
            'lines': line_pred,
            'descriptors': descriptors,
            'complexity': complexity
        }

    def compute_loss(self, predictions, targets, weights={'point': 1.0, 'line': 1.0, 'desc': 1.0}):
        """计算损失函数"""
        # 基础损失计算
        point_loss = F.binary_cross_entropy(predictions['points'], targets['points'])
        line_loss = F.mse_loss(predictions['lines'], targets['lines'])
        desc_loss = self.descriptor_loss(predictions['descriptors'], targets['descriptors'])

        # 根据场景复杂度调整损失权重
        complexity = predictions['complexity']

        # 复杂场景下增加检测损失权重
        point_loss = point_loss * (1 + complexity.mean())
        line_loss = line_loss * (1 + complexity.mean())

        total_loss = (weights['point'] * point_loss +
                     weights['line'] * line_loss +
                     weights['desc'] * desc_loss)

        return total_loss, {
            'point_loss': point_loss.item(),
            'line_loss': line_loss.item(),
            'desc_loss': desc_loss.item(),
            'complexity': complexity.mean().item()
        }

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import cv2
from tqdm import tqdm
import pickle
import os

# 1. 模型定义部分
class VGGStyleEncoder(nn.Module):
    """VGG风格特征提取主干网络"""
    def __init__(self):
        super().__init__()
        # 使用简单的VGG风格卷积块
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.conv3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        return x

class ComplexityEstimator(nn.Module):
    """场景复杂度估计器"""
    def __init__(self, in_channels):
        super().__init__()
        self.pools = nn.ModuleList([
            nn.AdaptiveAvgPool2d((size, size))
            for size in [1, 2, 4]
        ])

        reduction = 8
        self.fc = nn.Sequential(
            nn.Linear(in_channels * (1 + 4 + 16), in_channels // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels),
            nn.Sigmoid()
        )

    def forward(self, x):
        pool_outs = []
        for pool in self.pools:
            out = pool(x)
            pool_outs.append(out.flatten(2))

        out = torch.cat([o.mean(2) for o in pool_outs], dim=1)
        complexity = self.fc(out)
        return complexity.unsqueeze(2).unsqueeze(3)

class AFEM(nn.Module):
    """自适应特征增强模块"""
    def __init__(self, in_channels):
        super().__init__()
        self.complexity_estimator = ComplexityEstimator(in_channels)

        self.enhance = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, in_channels, 1)
        )

        self.recalibration = nn.Sequential(
            nn.Conv2d(in_channels*2, in_channels, 1),
            nn.BatchNorm2d(in_channels),
            nn.Sigmoid()
        )

    def forward(self, x):
        complexity = self.complexity_estimator(x)
        enhanced = self.enhance(x)
        combined = torch.cat([x, enhanced], dim=1)
        weights = self.recalibration(combined)
        out = x + weights * enhanced
        return out, complexity

class PointDetector(nn.Module):
    """点特征检测头"""
    def __init__(self, input_channel=128):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(input_channel, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 1, kernel_size=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.conv(x)

class LineDetector(nn.Module):
    """线特征检测头"""
    def __init__(self, input_channel=128):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(input_channel, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 2, kernel_size=1)
        )

    def forward(self, x):
        return self.conv(x)

class DescriptorExtractor(nn.Module):
    """特征描述子提取器"""
    def __init__(self, input_channel=128, descriptor_dim=256):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(input_channel, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, descriptor_dim, kernel_size=1)
        )

    def forward(self, x):
        x = self.conv(x)
        x = F.normalize(x, p=2, dim=1)
        return x

class SuperPointLineAFEM(nn.Module):
    """完整的点线特征检测网络"""
    def __init__(self, descriptor_dim=256):
        super().__init__()
        self.encoder = VGGStyleEncoder()
        self.afem = AFEM(128)
        self.point_head = PointDetector()
        self.line_head = LineDetector()
        self.descriptor = DescriptorExtractor(descriptor_dim=descriptor_dim)

    def forward(self, x):
        features = self.encoder(x)
        features, complexity = self.afem(features)

        point_pred = self.point_head(features)
        line_pred = self.line_head(features)
        descriptors = self.descriptor(features)

        point_pred = F.interpolate(point_pred, size=x.shape[2:], mode='bilinear', align_corners=False)
        line_pred = F.interpolate(line_pred, size=x.shape[2:], mode='bilinear', align_corners=False)

        return {
            'points': point_pred,
            'lines': line_pred,
            'descriptors': descriptors,
            'complexity': complexity
        }

    def compute_loss(self, predictions, targets, weights={'point': 1.0, 'line': 1.0, 'desc': 1.0}):
        point_loss = F.binary_cross_entropy(predictions['points'], targets['points'])
        line_loss = F.mse_loss(predictions['lines'], targets['lines'])
        desc_loss = self.descriptor_loss(predictions['descriptors'], targets.get('descriptors', None))

        complexity = predictions['complexity']
        point_loss = point_loss * (1 + complexity.mean())
        line_loss = line_loss * (1 + complexity.mean())

        total_loss = (weights['point'] * point_loss +
                     weights['line'] * line_loss +
                     weights['desc'] * desc_loss)

        return total_loss, {
            'point_loss': point_loss.item(),
            'line_loss': line_loss.item(),
            'desc_loss': desc_loss.item(),
            'complexity': complexity.mean().item()
        }

    def descriptor_loss(self, pred_desc, gt_desc):
        if gt_desc is None:
            return torch.tensor(0.0, device=pred_desc.device)
        distances = 1.0 - torch.matmul(pred_desc, gt_desc.t())
        positive_dist = torch.diag(distances)
        loss = torch.mean(torch.clamp(positive_dist - 0.1, min=0.0))
        return loss

import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import cv2
from pathlib import Path
import pickle

# 2. 数据集定义
class PointLineDataset(Dataset):
    """修复后的数据集类"""
    def __init__(self, pointlines_dir):
        self.pointlines_dir = Path(pointlines_dir)
        self.file_list = list(self.pointlines_dir.glob('*.pkl'))

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        pkl_path = self.file_list[idx]

        # 加载pkl文件
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)

        # 从数据中提取信息
        points = np.array(data.get('points', []), dtype=np.float32)
        lines = np.array(data.get('lines', []), dtype=np.float32)

        # 确保points和lines是2D数组
        if len(points) == 0:
            points = np.zeros((0, 2), dtype=np.float32)
        elif len(points.shape) == 1:
            points = points.reshape(-1, 2)

        if len(lines) == 0:
            lines = np.zeros((0, 4), dtype=np.float32)
        elif len(lines.shape) == 1:
            lines = lines.reshape(-1, 4)

        # 创建点和线的热力图
        H, W = 480, 640  # 假设图像大小，根据实际情况调整
        point_heatmap = self.create_point_heatmap(points, H, W)
        line_heatmap = self.create_line_heatmap(lines, H, W)

        # 创建一个全零的输入图像
        input_image = torch.zeros((1, H, W))

        sample = {
            'image': input_image,
            'points': torch.FloatTensor(points),
            'lines': torch.FloatTensor(lines),
            'point_heatmap': point_heatmap,
            'line_heatmap': line_heatmap
        }

        return sample







